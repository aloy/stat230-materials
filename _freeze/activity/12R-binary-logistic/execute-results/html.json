{
  "hash": "b8830c55d515637f86983e438b917920",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Binary Logistic Regression in R\"\nwebr:\n  packages:\n    - Sleuth3\n    - broom\nformat: \n  live-html:\n    toc: true\neditor: source\neditor_options: \n  chunk_output_type: console\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n## Fitting a Binary Logistic Regression Model\n\nFitting a binary logistic regression model in R is similar to fitting a linear regression model. To guide you through this process, let's revisit the Framingham Heart Study data set that we discussed in class.\n\nTo load the data, run the following code chunk:\n\n\n::: {.cell}\n```{webr}\nframingham <- read.csv(\"https://aloy.github.io/stat230-materials/data/framingham.csv\")\n```\n:::\n\n\nBinary logistic regression requires a binary response variable. In this case, we will use the `TenYearCHD` variable, which indicates whether an individual developed coronary heart disease within ten years (1 = yes, 0 = no). Be sure to always check that your response variable is coded correctly before modeling.\n\n\nNext, we will fit a binary logistic regression model using the `glm()` function in R. The syntax is similar to that of the `lm()` function, but we need to specify the `family` argument as `binomial` to indicate that we are performing logistic regression. Here, we will model the probability of developing coronary heart disease based on age.\n\n\n::: {.cell}\n```{webr}\nlogistic_model <- glm(TenYearCHD ~ age, data = framingham, family = binomial)\n```\n:::\n\n\nYou can still use the `summary()` function to view the results of the model fit:\n\n\n::: {.cell}\n```{webr}\nsummary(logistic_model)\n```\n:::\n\n\nMuch of the regression output looks the same as in linear regression, but there are some key differences. The main differences are:\n\n- The test statistic is no labeled `z value` instead of `t value`.\n- A `Null deviance` and `Residual deviance` are reported instead of `Multiple R-squared` and `Adjusted R-squared`.\n\n\n## Making predictions\n\nWe can also use R to calculate the predicted probabilities from our fitted logistic regression model using the `predict()` function. Below is an example where we are calculating the predicted probability that a 50-year old develops CHD in the next ten years:\n\n\n::: {.cell}\n```{webr}\npredict(logistic_model, newdata = data.frame(age = 50), type = \"response\")\n```\n:::\n\n\nNotice that we are specifying the data (x values) use for prediction using the same argument as in linear regression: we create a data frame with columns named identically to the actual data set used to fit the mode. We also must specify `type = \"response\"` to get probabilities. If you omit this argument, you will get a prediction on the log-odds scale, which isn't a probability!\n\n\n## Inference for single regression coefficients\n\nYou have already seen that we can obtain Wald-based tests via the `summary()` command. To extract only that coefficient table, you can use the `tidy()` command from the broom package:\n\n\n::: {.cell}\n```{webr}\nlibrary(broom)\ntidy(logistic_model)\n```\n:::\n\n\nTo calculate Wald-based confidence intervals for each regression coefficient, we can use the `confint()` command, specifying the `level` argument:\n\n\n::: {.cell}\n```{webr}\ntidy(logistic_model, conf.int = TRUE, conf.level = 0.9)\n```\n:::\n\n\n\n## Drop-in deviance tests\n\nWe could perform drop-in deviance tests \"by hand\" by fitting two models and comparing their reisduals deviances. However, a more convenient way to perform these tests is to use the `anova()` function with the `test = \"Chisq\"` argument. As an example, let's compare the \n`logistic_model` above to a model that includes age, smoking status, total cholesterol, and BMI:\n\n\n::: {.cell}\n```{webr}\nframingham_complete <- tidyr::drop_na(framingham, TenYearCHD, age, currentSmoker, totChol, BMI)\n\nfull_model <- glm(TenYearCHD ~ age + currentSmoker + totChol + BMI,\n                  data = framingham_complete, family = binomial)\nreduced_model <- glm(TenYearCHD ~ age, data = framingham_complete, family = binomial)\n```\n:::\n\n\nNow we can use the `anova()` function to compare the two models:\n\n\n::: {.cell}\n```{webr}\nanova(reduced_model, full_model, test = \"Chisq\")\n```\n:::\n\n\n:::{.callout-note}\n## Why did we use `drop_na()`?\n\nWhen fitting models in R, any rows with missing values in the variables used in the model are automatically excluded. However, when comparing two models using `anova()`, it's crucial that both models are fitted on the same subset of data. If one model has missing values in additional variables not present in the other model, it could lead to discrepancies in the number of observations used for each model.\n\nTo ensure that both models are fitted on the same data, we use `tidyr::drop_na()` to remove any rows with missing values in any of the variables used in either model. This guarantees that both the reduced and full models are based on the same set of observations, allowing for a valid comparison using the drop-in deviance test.\n:::\n\n\n***\n\n## Function quick reference\n\nThe following table summarizes the functions discussed above:\n\n| Function | Purpose |\n|----|-----------|\n| `glm(formula, data, family = binomial)` | Fit a binary logistic regression model |\n| `summary(model)` | View model summary and Wald tests |\n| `predict(model, newdata, type = \"response\")` | Make predictions on the probability scale |\n| `tidy(model, conf.int = TRUE, conf.level)` | Extract coefficient table with confidence intervals |\n| `anova(reduced_model, full_model, test = \"Chisq\")` | Perform drop-in deviance test between two nested models |\n| `tidyr::drop_na(data, vars...)` | Remove rows with missing values in specified variables |\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}