{
  "hash": "7432baaca7bab5d2c384996feed25133",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression Cautions: Multicollinearity and R<sup>2</sup>\"\nformat: html\neditor: source\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n## Overview\n\nIn this activity you will learn how to calculate diagnostic measures and create diagnostic plots for multiple linear regression models in R. \n\n\n## Example 1: Uncorrelated Predictors\n\nAs a first example, let's consider a multiple linear regression model where the two predictor variables are uncorrelated.\n\n**Task 1.** Inspect the below correlation matrix and verify that the correlation between the two predictor variables (`x1` and `x2`) is 0.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n          x1        x2         y\nx1 1.0000000 0.0000000 0.7419309\nx2 0.0000000 1.0000000 0.6384057\ny  0.7419309 0.6384057 1.0000000\n```\n\n\n:::\n:::\n\n\n**Task 2.** The below code chunk fits a simple linear regression model with `y` as the response variable and `x1` as the predictor variables. The coefficients table was extract using the `broom::tidy()`. (Feel free to use `broom::tidy()` it in your work!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(y ~ x1, data = ex1)\nbroom::tidy(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    23.5      10.1       2.32  0.0591\n2 x1              5.37      1.98      2.71  0.0351\n```\n\n\n:::\n:::\n\n\nSimilarly, here is the output for the simple linear regression model with `y` as the response variable and `x2` as the predictor variable.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    23.5      10.1       2.32  0.0591\n2 x1              5.37      1.98      2.71  0.0351\n```\n\n\n:::\n:::\n\n\nAnd finally, here is the output for the multiple linear regression model with `y` as the response variable and both `x1` and `x2` as the predictor variables.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    0.375     4.74     0.0791 0.940   \n2 x1             5.37      0.664    8.10   0.000466\n3 x2             9.25      1.33     6.97   0.000937\n```\n\n\n:::\n:::\n\n\nCompare the model summaries from the three models. What do you observe about the coefficients when both predictors are included in the model? What do you observe about the standard errors? The results of the hypothesis tests?\n\n\n## Example 2: Prefectly Correlated Predictors\n\nIn many examples the predictor variables will be correlated. As an extreme example, let's consider a situation where the predictors are perfectly correlated. The data set for this small example is printed below.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  x1 x2   y\n1  2  6  23\n2  8  9  83\n3  6  8  63\n4 10 10 103\n```\n\n\n:::\n:::\n\n\n\n**Task 3.** Inspect the below correlation matrix and verify that the correlation between the two predictor variables (`x1` and `x2`) is 1.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   x1 x2 y\nx1  1  1 1\nx2  1  1 1\ny   1  1 1\n```\n\n\n:::\n:::\n\n\n**Task 4.** From the previous task, you should have seen that the correlation between `x1` and `x2` is 1, as is the correlation between both predictors and `y`. This sounds like a great situation, right? We'll explore two models to unpack this.\n\na. Consider the fitted regression equation: $\\widehat{y} = -87 + x_1 + 18x_2$. Verify that the predicted values are exactly equal to the observed values.\n\n\n\nb. Consider the fitted regression equation: $\\widehat{y} = -7 + 9x_1 + 2x_2$. Verify that the predicted values are exactly equal to the observed values.\n\n\nc. Discuss the following questions with your group:\n\n    - Would you be willing to use either model to make predictions for new observations? Why or why not?\n    - Would you be willing to interpret the coefficients in either model? Why or why not?\n    - Do you think the standard error of the slopes is reasonably small or large? Why?\n\n## Example 3: A more realistic example\n\nLet's consider a more realistic example where the predictor variables are correlated, but not perfectly correlated. The data set `bodyfat` contains measurements on 20 healthy adults between 25 and 34 years of age. We're interested in predicting body fat percentage using three body measurements: triceps skinfold thickness, thigh circumference, and midarm circumference. \n\n**Task 5.** Below is the correlation matrix for the data set. What do you observe about the correlations between the predictor variables?\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n                     triceps_skinfold thigh_circumference midarm_circumference\ntriceps_skinfold            1.0000000           0.9238425            0.4577772\nthigh_circumference         0.9238425           1.0000000            0.0846675\nmidarm_circumference        0.4577772           0.0846675            1.0000000\nbody_fat                    0.8432654           0.8780896            0.1424440\n                      body_fat\ntriceps_skinfold     0.8432654\nthigh_circumference  0.8780896\nmidarm_circumference 0.1424440\nbody_fat             1.0000000\n```\n\n\n:::\n:::\n\n\n**Task 6.** There are numerous models that we could consider. Below are summary tables from several models. Compare the model summaries from the fitted models. What do you observe about the coefficients when different predictors are included in the model? What do you observe about the standard errors? The results of the hypothesis tests?\n\n\nOnly triceps skinfold as a predictor:\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term             estimate std.error statistic    p.value\n  <chr>               <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)        -1.50      3.32     -0.451 0.658     \n2 triceps_skinfold    0.857     0.129     6.66  0.00000302\n```\n\n\n:::\n:::\n\n\nOnly thigh circumference as a predictor:\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term                estimate std.error statistic     p.value\n  <chr>                  <dbl>     <dbl>     <dbl>       <dbl>\n1 (Intercept)          -23.6       5.66      -4.18 0.000566   \n2 thigh_circumference    0.857     0.110      7.79 0.000000360\n```\n\n\n:::\n:::\n\n\n\nOnly midarm circumference as a predictor:\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term                 estimate std.error statistic p.value\n  <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)            14.7       9.10      1.61    0.124\n2 midarm_circumference    0.199     0.327     0.611   0.549\n```\n\n\n:::\n:::\n\n\n\nTriceps skinfold and thigh circumference as predictors:\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term                estimate std.error statistic p.value\n  <chr>                  <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)          -19.2       8.36     -2.29   0.0348\n2 triceps_skinfold       0.222     0.303     0.733  0.474 \n3 thigh_circumference    0.659     0.291     2.26   0.0369\n```\n\n\n:::\n:::\n\n\nTriceps skinfold and midarm circumference as predictors:\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term                 estimate std.error statistic     p.value\n  <chr>                   <dbl>     <dbl>     <dbl>       <dbl>\n1 (Intercept)          -26.0        7.00     -3.72  0.00172    \n2 midarm_circumference   0.0960     0.161     0.595 0.560      \n3 thigh_circumference    0.851      0.112     7.57  0.000000772\n```\n\n\n:::\n:::\n\n\nThigh circumference and midarm circumference as predictors:\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term                 estimate std.error statistic p.value\n  <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)            117.       99.8       1.17   0.258\n2 triceps_skinfold         4.33      3.02      1.44   0.170\n3 thigh_circumference     -2.86      2.58     -1.11   0.285\n4 midarm_circumference    -2.19      1.60     -1.37   0.190\n```\n\n\n:::\n:::\n\n\n\n<i class=\"bi bi-sign-stop-fill\"></i> Let me know when you have reached this spot. We'll regroup as a class to discuss your observations.\n\n\n## What about predictions?\n\nNow that we've discussed the issues revolving around interpreting coefficients and drawing inference on them in the presence of multicollinearity, let's consider how multicollinearity impacts predictions. \n\nRecall that the mean squared error (MSE) of a model measures the variability in the error terms (residuals). In other words, it is measuring the variability unexplained by the model.\n\n**Task 7.** Below are the MSE values for three of the models we considered in Task 6. What do you observe about the MSE values when different predictors are included in the model? \n\n| Predictors | MSE |\n|------------|-----|\n| triceps_skinfold |  7.95 |\n| triceps_skinfold, thigh_circumference |  6.47 |\n| triceps_skinfold, thigh_circumference, midarm_circumference |  6.15 |\n\n\n**Task 8.** Based on your observations, do you think we can use any of the models to make predictions for new observations? Why or why not?\n\n\n<i class=\"bi bi-sign-stop-fill\"></i> Let me know when you have reached this spot. We'll regroup as a class to discuss your observations.\n\n\n## Be Wary of R<sup>2</sup> when comparing models\n\nWe all love R<sup>2</sup> right? It is \"easy\" to interpret and is a popular metric of model fit to report in many fields. While I agree that  R<sup>2</sup> is a nice metric for describing a **single** model, it has an issue when it's used to compare models. In this example, you'll explore this issue.\n\nCase Study 10.1.1 in *The Statistical Sleuth* describes Galileo's experiment that led him to discover that the trajectory of a body falling with horizontal velocity is a parabola (see page 272). The `case1011` data set in the `Sleuth3` R package contains the results of Galileo's experiment. The data set contains seven observations on the following two variables: the horizontal distance traveled from the edge of the table to its landing spot, and the initial height of the object. Both measurements are in punti (1 punti = or 169/180 mm or about 0.037 in).\n\n**Task 9.** Fit each of the following models and report the R<sup>2</sup> value for each. Fill in the table below.\n\n\n| Model | R<sup>2</sup> |\n|-------|---------|\n| $\\text{distance} \\sim \\text{height}$ |  |\n| $\\text{distance} \\sim \\text{height} + \\text{height}^2$ |  |\n| $\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3$ |  | \n| $\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3 + \\text{height}^4$|  |\n| $\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3 + \\text{height}^4 + \\text{height}^5$ |  |\n| $\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3 + \\text{height}^4 + \\text{height}^5+ \\text{height}^6$ |  |\n\nWhat do you observe about the R<sup>2</sup> values as you add more polynomial terms to the model? ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}