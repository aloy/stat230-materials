{
  "hash": "f988ae9ce17916060f9a49f9a4c34298",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Quasibinomial Logistic Regression\"\nsubtitle: \"Logistic regression -- Stat 230\"\nformat: \n  revealjs:\n    chalkboard: \n      buttons: false\n    scrollable: true\neditor: source\nfilters:\n - flourish\n---\n\n\n\n## Support for railroad referenda in 1870s Alabama {.smaller}\n\n**Research question:** \n\nWas voting on railroad referenda during the Reconstruction Era related to distance from the proposed railroad line and the racial composition of a community?\n\n<br>\n\n**Hypotheses:**\n\n- Positive votes were inversely proportional to the distance a voter is from the proposed railroad\n\n- racial composition of a community is hypothesized to be associated with voting behavior\n\n::: footer\nData source: *Broadening Your Statistical Horizons*, Legler & Roback\n:::\n\n\n\n## Data {.smaller}\n\nMichael Fitzgerald obtained data from the 1870 U.S. Census from communities in Hale County, Alabama\n\n- `YesVotes` = the number of “Yes” votes in favor of the proposed railroad line (primary response variable)\n\n- `NumVotes` = total number of votes cast in the election\n\n- `pctBlack` = racial composition (% black)\n\n- `distance` = the distance from the proposed railroad (in miles)\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> community </th>\n   <th style=\"text-align:right;\"> pctBlack </th>\n   <th style=\"text-align:right;\"> distance </th>\n   <th style=\"text-align:right;\"> YesVotes </th>\n   <th style=\"text-align:right;\"> NumVotes </th>\n   <th style=\"text-align:right;\"> propYes </th>\n   <th style=\"text-align:left;\"> InFavor </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Carthage </td>\n   <td style=\"text-align:right;\"> 58.40 </td>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:right;\"> 61 </td>\n   <td style=\"text-align:right;\"> 110 </td>\n   <td style=\"text-align:right;\"> 0.555 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Cederville </td>\n   <td style=\"text-align:right;\"> 92.40 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 0.000 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Five Mile Creek </td>\n   <td style=\"text-align:right;\"> 18.28 </td>\n   <td style=\"text-align:right;\"> 15 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 42 </td>\n   <td style=\"text-align:right;\"> 0.095 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Greensboro </td>\n   <td style=\"text-align:right;\"> 59.38 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1790 </td>\n   <td style=\"text-align:right;\"> 1804 </td>\n   <td style=\"text-align:right;\"> 0.992 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n\n## EDA\n\nWas voting on railroad referenda during the Reconstruction Era related to distance from the proposed railroad line and the racial composition of a community?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-quasibinomial_files/figure-revealjs/unnamed-chunk-2-1.png){width=1008}\n:::\n:::\n\n\n\n\n## Is there evidence of lack of fit?\n\n\n\n```{eval=FALSE}\nglm(formula = YesVotes/NumVotes ~ distance * pctBlack, \n    family = binomial, data = rrdata, weights = NumVotes)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        7.5509017  0.6383697  11.828  < 2e-16\ndistance          -0.6140052  0.0573808 -10.701  < 2e-16\npctBlack          -0.0647308  0.0091723  -7.057 1.70e-12\ndistance:pctBlack  0.0053665  0.0008984   5.974 2.32e-09\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 988.45  on 10  degrees of freedom\nResidual deviance: 274.23  on  7  degrees of freedom\nAIC: 320.53\n```\n\n\n\n\n## Possible causes of lack of fit \n\n:::{style=\"font-size: 0.9em\"}\n**Outliers**\n\n- Deviance in logistic regression is analogous to SSE in linear regression\n\n- Outliers can inflate the deviance\n:::\n\n. . .\n\n:::{style=\"font-size: 0.9em\"}\n**Detection**\n\n- Deviance residual plots\n:::\n\n. . .\n\n\n:::{style=\"font-size: 0.9em\"}\n**Why do we care?**\n\n\n- Influential outliers result in biased estimates of the $\\widehat{\\beta}_i$s\n:::\n\n\n## Possible causes of lack of fit {.smaller}\n\n**Incorrect logit (mean) function**\n\n- We fit a line to a curve\n\n- We omitted important predictor variables\n\n. . .\n\n**Detection**\n\n- Empirical logit plots\n\n- Deviance residual plots \n\n- GOF test\n\n. . .\n\n**Why do we care?**\n\n- Biased estimates of the $\\widehat{\\beta}_i$s\n\n\n\n## Possible causes of lack of fit {.smaller}\n\n**Binomial model for $Y$ is wrong**\n\n- Trials are not independent\n\n- Probability of success is not the same across trials\n\n- Important predictors might be omitted\n\n. . . \n\n**Detection**\n\n- Think\n\n- GOF test\n\n- Deviance residuals\n\n. . .\n\n**Why do we care?**\n\n- Variance is greater than $n_i \\pi (X_i) (1 - \\pi (X_i))$\n\n- SEs are likely too small $\\Longrightarrow$ p-values too small and  CIs too narrow\n\n\n\n## Exploring lack of fit\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-quasibinomial_files/figure-revealjs/unnamed-chunk-4-1.png){width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n- Since we have lack of fit, don't treat residuals as normal\n\n- Greensboro not really an outlier\n\n- Possible nonlinearity\n:::\n::::\n\n\n\n\n## Quadratic model\n\n\n::: {style=\"font-size: 0.8em\"}\n\\begin{aligned}\n{\\rm logit}(\\pi) = \\beta_0 + \\beta_1 {\\tt distance} + \\beta_2 {\\tt pctBlack} + \\beta_3 {\\tt distance^2}\\\\ + \\beta_4 {\\tt distance \\times pctBlack}\n\\end{aligned}\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        8.365538   0.919710   9.096  < 2e-16 ***\ndistance          -1.592867   0.131070 -12.153  < 2e-16 ***\npctBlack          -0.062498   0.012845  -4.866 1.14e-06 ***\nI(distance^2)      0.044576   0.003388  13.156  < 2e-16 ***\ndistance:pctBlack  0.009830   0.001514   6.493 8.43e-11 ***\n  \n    Null deviance: 988.450  on 10  degrees of freedom\nResidual deviance:  72.018  on  6  degrees of freedom\n```\n:::\n\n\n:::{style=\"font-size: 0.8em\"}\nIs there still evidence of lack of fit?\n:::\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pchisq(72.018, df = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.575406e-13\n```\n\n\n:::\n:::\n\n\n\n\n\n## Exploring lack of fit\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-quasibinomial_files/figure-revealjs/unnamed-chunk-7-1.png){width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::::: {style=\"padding: 175px 0;\"}\nDo you see anything concerning here?\n:::::\n\n:::\n::::\n\n\n\n# Overdispersion\n (extra-binomial variation)\n\n\n\n## Model assumptions\n\n:::{style=\"font-size: 0.95em\"}\nWe **assume** that $Y_i$ is binomial\n\n$$Y | X_i \\sim {\\rm Binomial}(n_i, \\pi_i)$$\n\nThis implies that within the same subpopulation<br> (combo of $x_i$s)\n\n- trials are independent\n\n- trials have the same probability of success\n\n- ${\\rm E}(Y|X_i) = n_i \\pi_i$\n\n- ${\\rm Var}(Y|X_i) = n_i \\pi_i(1- \\pi_i)$\n:::\n\n\n\n## Overdispersion\n\nIf the binomial assumptions are not met, then the variance of the $Y_i$ will usually be larger than what is expected for a binomial distribution:\n\n$${\\rm Var}(Y|X_i) > n_i \\pi_i (1 - \\pi_i)$$\n\n## Overdispersion\n\n:::{style=\"font-size: 0.9em\"}\nLet $Z_1, \\ldots, Z_m$ be iid Bernoulli (S/F) trials with probability of success $\\pi$.\n:::\n\n::: {style=\"font-size: 0.8em\"}\n\\begin{aligned}\nY &= Z_1 + \\ldots + Z_m\\\\\n{\\rm Var} (Y) &= {\\rm Var}(Z_1 + \\ldots + Z_m)\\\\\n  &= {\\rm Var}(Z_1) + \\ldots + {\\rm Var}(Z_m) + 2 \\sum_{i<j} {\\rm Cov}(Z_i, Z_j)\\\\\n  &= \\psi m \\pi (1 - \\pi)\n\\end{aligned}\n:::\n\n:::{style=\"font-size: 0.9em\"}\n**So what?**\n\n- p-values too small  \n\n- CIs too narrow\n:::\n\n\n## Ad hoc test of overdispersion\n\n:::{style=\"font-size: 0.9em\"}\nRecall:\n\n$D^2 = 2 \\sum \\left[ Y_i \\log \\left( \\dfrac{Y_i}{n_i\\widehat{\\pi}_i} \\right)  + (n_i - Y_i)  \\log \\left( \\dfrac{n_i - Y_i}{n_i - n_i \\widehat{\\pi}_i} \\right) \\right]$\n\n<br>\n\n**If the model is correct and all *n<sub>i</sub>* are large enough**, $D^2 \\overset{\\cdot}{\\sim} \\chi^2$ with $\\text{df}= n-(p+1)$ \n\n\n::: r-stack\n:::{style=\"background: #B7CFDC; border-radius: 10px;\"}\n:::{style=\"margin: 30px;\"}\n$E\\left( D^2 \\right) = {\\rm df} \\Longrightarrow \\dfrac{D^2}{{\\rm df}} \\approx 1$\n\nRed flag if $D^2/{\\rm df} >\\!\\!> 1$\n:::\n:::\n:::\n\n:::\n\n\n## Model checking\n\nIf over-dispersion exists, check whether the assumptions are met\n\n\n:::{.incremental}\n\n1. Do we have independence?\n    <br>\n\n1. If assumptions met, then check for outliers.\n    <br>\n\n1. If assumptions met, do we need to include an interaction term, some other term(s)?\n    <br>\n\n1. If assumptions met, then maybe an incorrect model (binomial model not appropriate)?\n\n:::\n\n## Quasi-likelihood approach\n\n:::{style=\"font-size: 0.9em\"}\n:::{.incremental}\n\n1. Estimate $\\beta_i$s using ML estimation, as before\n\n2. Estimate $\\psi$ using $\\widehat{\\psi} = \\dfrac{\\text{deviance}}{{\\rm df}}$\n\n3. Use ${\\rm SE_{quasibinomial}}(\\widehat{\\beta}_i) = \\sqrt{\\widehat{\\psi}} \\cdot {\\rm SE_{binomial}}(\\widehat{\\beta}_i)$ and use the $t$-distribution with $n-(p+1)$ degrees of freedom\n\n4. You can do a \"drop-in-deviance\" test using an F test:\n\n    $${\\rm F} = \\dfrac{\\left[ \\text{deviance(reduced)} - \\text{deviance(full)} \\right] / d}{\\widehat{\\psi}}$$\n\n    where ${\\rm F} \\sim F_{d,\\, n-(p+1)}$\n    \n:::\n:::\n\n\n\n## Ad-hoc adjustment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        8.365538   0.919710   9.096  < 2e-16 ***\ndistance          -1.592867   0.131070 -12.153  < 2e-16 ***\npctBlack          -0.062498   0.012845  -4.866 1.14e-06 ***\nI(distance^2)      0.044576   0.003388  13.156  < 2e-16 ***\ndistance:pctBlack  0.009830   0.001514   6.493 8.43e-11 ***\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 988.45  on 10  degrees of freedom\nResidual deviance: 72.018  on  6  degrees of freedom\n```\n:::\n\n\nLet's correct the test for whether $\\beta_{\\rm distance} = 0$\n\n\n\n## Quasi-likelihood in R (\"proactive\" approach)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(YesVotes/NumVotes ~ distance * pctBlack + I(distance^2), \n    data = rrdata, weights = NumVotes, family = quasibinomial)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)   \n(Intercept)        8.365538   3.038714   2.753  0.03316 * \ndistance          -1.592867   0.433054  -3.678  0.01035 * \npctBlack          -0.062498   0.042440  -1.473  0.19127   \nI(distance^2)      0.044576   0.011195   3.982  0.00727 **\ndistance:pctBlack  0.009830   0.005003   1.965  0.09701 . \n\n(Dispersion parameter for quasibinomial family taken to be 10.91635)\n\n    Null deviance: 988.450  on 10  degrees of freedom\nResidual deviance:  72.018  on  6  degrees of freedom\n```\n:::\n\n\n\n::: {.aside}\nR estimates the dispersion parameter differently than I showed you, that's why the results are a bit different.\n:::\n\n\n## Comparing models in R\n\nYou have to tell R to use an F-test to conduct the quasi-binomial drop-in-deviance  test\n\n\n::: {.cell flourish='[{\"target\":\"test = \\'F\\'\"}]'}\n\n```{.r .cell-code}\nfull <- glm(YesVotes/NumVotes ~ distance * pctBlack + I(distance^2), \n    data = rrdata, weights = NumVotes, family = quasibinomial)\n\nreduced <- update(full, . ~ . - distance:pctBlack)\n\nanova(reduced, full, test = 'F')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: YesVotes/NumVotes ~ distance + pctBlack + I(distance^2)\nModel 2: YesVotes/NumVotes ~ distance * pctBlack + I(distance^2)\n  Resid. Df Resid. Dev Df Deviance      F  Pr(>F)  \n1         7    118.302                             \n2         6     72.018  1   46.284 4.2399 0.08516 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n",
    "supporting": [
      "19-quasibinomial_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}