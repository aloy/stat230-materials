---
title: "Inference for a two-sample model"
subtitle: "Making connections"
author: "Stat 230"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
library(mosaic)
library(tidyverse)
library(resampledata)
library(ggthemes)
library(kableExtra)
library(fontawesome)
library(patchwork)
data("BookPrices")

book_means <- round(mean(Price ~ Area, data = BookPrices), 2)
```

## Textbook prices revisited

Is the average textbook price different between STEM and the social sciences?

::: columns
::: {.column width="80%"}
```{r fig.height = 3, fig.width = 5, out.width="100%"}


ggplot(data = BookPrices, aes(x = Area, y = Price, fill = Area, color = Area)) +
   stat_summary(fun = mean, fun.min = mean, fun.max = mean,
                 geom = "crossbar", width = 0.6, aes(color = Area)) +
  geom_jitter(height = 0, width = 0.25) +
  scale_fill_colorblind() +
  scale_color_colorblind() +
  theme_classic() +
  theme(legend.position = "none")
```
:::

::: {.column width="20%"}
<br>

<br>

$\widehat{\mu}_1 = `r book_means[1]`$

$\widehat{\mu}_2 = `r book_means[2]`$
:::
:::

## Model review

In the two-sample problem, we can write the model as

$$y_{ij} = \underbrace{\mu_i}_{\rm \ \ mean \\ response} + \underbrace{\varepsilon_{ij}}_{\rm random \\ \ \  error}$$

. . .

<br>

What assumptions/conditions do we place on the random errors?

## Assumptions on error terms

$\varepsilon_{ij} \overset{\rm iid}{\sim} \mathcal{N}(0, \sigma^2)$

-   Errors are independent and identically distributed (iid)

-   Errors follow normal distribution

-   Errors have mean 0

-   Both groups have the same population standard deviation

::: aside
Note: The textbook uses variance rather than the standard deviation to specify the normal distribution.
:::

## Assumptions on error terms

```{r fig.height = 4.5, fig.width = 8, out.width="100%"}
library(ggdist)
dist_df = tribble(
    ~group, ~mean, ~sd,
    "Math & Science",        150,   50,
    "Social Sciences",        100,   50,
)

point_df <- data.frame(
  Area = rep(c("Math & Science", "Social Sciences"), each = 500),
  Price = c(rnorm(500, 150, 50), rnorm(500, 100, 50))
)


  ggplot(data = BookPrices, aes(x = Area, y = Price, fill = Area, color = Area)) +
   stat_summary(fun = mean, fun.min = mean, fun.max = mean,
                 geom = "crossbar", width = 0.6, aes(color = Area)) +
    geom_jitter(data = point_df, width = 0.2, alpha = 0.5) +
  stat_slab(aes(x = group, ydist = "norm", arg1 = mean, arg2 = sd, fill = group), data = dist_df, alpha = 0.4, inherit.aes = FALSE, scale = 0.6) +
  scale_fill_colorblind() +
  scale_color_colorblind() +
  theme_classic() +
  theme(legend.position = "none")

```

## Hypothesis testing review

1.  Formulate competing hypotheses: $H_0$ and $H_a$

2.  Choose a test statistic that characterizes the information in the sample relevant to $H_0$

3.  Determine the distribution of the chosen statistic **when** $H_0$ is true

4.  Compare the calculated test statistic to that distribution and determine whether is it "extreme" (i.e., too far in the tail)

## 1. Formulate competing hypotheses

::: incremental
-   Be sure to use parameters from the model in your hypotheses!

    $$y_{ij} = \mu_i + \varepsilon_{ij}$$

-   Link the hypotheses to the research question!

    Is the average textbook price different between STEM and the social sciences?
:::

. . .

::: columns
::: {.column width="25%"}
$H_0: \mu_1 = \mu_2$

$H_a: \mu_1 \ne \mu_2$
:::

::: {.column width="30%"}
::: fragment
$\Longleftrightarrow \mu_1-\mu_2=0$

$\Longleftrightarrow \mu_1-\mu_2\ne0$
:::
:::
:::

## 2. Choose a test statistic

Your test statistic should characterize the information in the sample relevant to your hypotheses

. . .

::: columns
::: {.column width="50%"}
$t = \dfrac{{\rm statistic} - {\rm H_0 \ value}}{SE}$
:::

::: {.column width="40%"}
::: fragment
$= \dfrac{(\bar{y}_1- \bar{y}_2) - (\mu_1-\mu_2)}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$
:::
:::
:::

. . .

where

$s_p = \sqrt{\dfrac{(n_1 - 1) s_1^2 + (n_2-1) s_2^2}{n_1 + n_2 - 2}}$

## 3. Determine the null distribution

Null distribution = distribution of the test statistic under $H_0$

. . .

<br>

Two-sample CLT assuming equal variances:

$$
\bar{y}_1- \bar{y}_2 \overset{\cdot}{\sim} \mathcal{N} \left( \mu_1 - \mu_2, \sigma^2 \left[\frac{1}{n_1} + \frac{1}{n_2} \right] \right)
$$

. . .

If we knew $\sigma^2$, then $\dfrac{(\bar{y}_1- \bar{y}_2) - (\mu_1-\mu_2)}{\sigma \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \overset{\cdot}{\sim} \mathcal{N}(0, 1)$

## 3. Determine the null distribution

When we don't know $\sigma^2$, we plug in $s^2_p$ as our best estimate (assuming equal variances)

$$\dfrac{(\bar{y}_1- \bar{y}_2) - (\mu_1-\mu_2)}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \overset{\cdot}{\sim} t_{n_1 + n_2 - 2}$$

## 4. Compare the test statistic to the null distribution

```{r fig.height = 3.25, fig.width = 3.25}
p1 <- ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dt, args = list(df=3), color = "navy", size = 1) +
  stat_function(fun = dt, args = list(df=3), xlim = c(-4, -2.249),
                  geom = "area", fill = "navy", alpha = .4) +
  labs(x = "t", y = "Density", title = expression(H[a]: mu[1]-mu[2]<0)) +
  theme_classic()  +
  geom_vline(xintercept = -2.249, linetype = 2) 
 
p2 <- ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dt, args = list(df=3), color = "navy", size = 1) +
  stat_function(fun = dt, args = list(df=3), xlim = c(2.249, 4),
                  geom = "area", fill = "navy", alpha = .4) +
  labs(x = "t", y = "Density", title = expression(H[a]: mu[1]-mu[2]>0)) +
  theme_classic()  +
  geom_vline(xintercept = 2.249, linetype = 2) 

p3 <- ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dt, args = list(df=3), color = "navy", size = 1) +
  stat_function(fun = dt, args = list(df=3), xlim = c(2.249, 4),
                  geom = "area", fill = "navy", alpha = .4) +
  stat_function(fun = dt, args = list(df=3), xlim = c(-4, -2.249),
                  geom = "area", fill = "navy", alpha = .4) +
  labs(x = "t", y = "Density", title = expression(H[a]: mu[1]-mu[2]!=0)) +
  theme_classic()  +
  geom_vline(xintercept = 2.249, linetype = 2)  +
  geom_vline(xintercept = -2.249, linetype = 2)
```

::: columns
::: {.column width="33%"}
```{r}
#| fig.height: 2.5
#| fig.width: 3
#| out.width: 100%
p3
```
:::

::: {.column width="33%"}
::: fragment
```{r}
#| fig.height: 2.5
#| fig.width: 3
#| out.width: 100%
p1
```
:::
:::

::: {.column width="33%"}
::: fragment
```{r}
#| fig.height: 2.5
#| fig.width: 3
#| out.width: 100%
p2
```
:::
:::
:::

::: aside
Look to the tail specified by $H_a$
:::

## 4. Compare the test statistic to the null distribution

```{r fig.height = 3.25, fig.width = 10.5}
p1 <- ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dt, args = list(df=3), color = "navy", size = 1) +
  stat_function(fun = dt, args = list(df=3), xlim = c(-4, -2.249),
                  geom = "area", fill = "navy", alpha = .4) +
  labs(x = "t", y = "Density", title = expression(H[a]: mu[1]-mu[2]<0)) +
  theme_classic()  +
  geom_vline(xintercept = -2.249, linetype = 2) +
  geom_curve(aes(x = -3.75, y = .08, xend = -3, yend = .01), arrow = arrow(length = unit(0.03, "npc")), curvature = .2) +
  annotate("text", x = -3.16, y = .1, label = "p-value", color = "navy", size = 5)

p2 <- ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dt, args = list(df=3), color = "navy", size = 1) +
  stat_function(fun = dt, args = list(df=3), xlim = c(2.249, 4),
                  geom = "area", fill = "navy", alpha = .4) +
  labs(x = "t", y = "Density", title = expression(H[a]: mu[1]-mu[2]>0)) +
  theme_classic()  +
  geom_vline(xintercept = 2.249, linetype = 2) +
  geom_curve(aes(x = 3.75, y = .08, xend = 3, yend = .01), arrow = arrow(length = unit(0.03, "npc")), curvature = -.2) +
  annotate("text", x = 3.2, y = .1, label = "p-value", color = "navy", size = 5)

p3 <- ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dt, args = list(df=3), color = "navy", size = 1) +
  stat_function(fun = dt, args = list(df=3), xlim = c(2.249, 4),
                  geom = "area", fill = "navy", alpha = .4) +
  stat_function(fun = dt, args = list(df=3), xlim = c(-4, -2.249),
                  geom = "area", fill = "navy", alpha = .4) +
  labs(x = "t", y = "Density", title = expression(H[a]: mu[1]-mu[2]!=0)) +
  theme_classic()  +
  geom_vline(xintercept = 2.249, linetype = 2) +
  geom_curve(aes(x = 3.75, y = .08, xend = 3, yend = .01), arrow = arrow(length = unit(0.03, "npc")), curvature = -.2) +
  geom_vline(xintercept = -2.249, linetype = 2) +
  geom_curve(aes(x = -3.75, y = .08, xend = -3, yend = .01), arrow = arrow(length = unit(0.03, "npc")), curvature = .2) +
  annotate("text", x = 3.2, y = .12, label = "1/2\np-value", color = "navy", size = 5) +
annotate("text", x = -3.16, y = .12, label = "1/2\np-value", color = "navy", size = 5)

```

::: columns
::: {.column width="33%"}
```{r}
#| fig.height: 2.5
#| fig.width: 3
#| out.width: 100%
p3
```
:::

::: {.column width="33%"}
```{r}
#| fig.height: 2.5
#| fig.width: 3
#| out.width: 100%
p1
```
:::

::: {.column width="33%"}
```{r}
#| fig.height: 2.5
#| fig.width: 3
#| out.width: 100%
p2
```
:::
:::

::: aside
p-value is the tail area specified by $H_a$
:::

## 5. Draw a conclusion in context

::: incremental
-   *p*-value: probability of observing a test statistic at least as contrary to $H_0$, given that $H_0$ is true.
-   Smaller *p*-values provide stronger evidence against $H_0$
-   A "small enough" *p*-value tells us that there is a *statistical discernible* difference
-   This does not mean the difference is *important*
:::

## Book price example {.smaller}

```{r}
favstats(Price ~ Area, data = BookPrices) %>%
  select(Area, mean, sd, n) %>% 
  kable(digits = 3)
```

$H_0: \mu_1-\mu_2=0$

. . .

$t=\dfrac{(\overline{y}_1-\overline{y}_2)-0}{s_p \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}$

. . .

::: columns
::: {.column width="40%"}
$s_p=\sqrt{\dfrac {(n_1 - 1) s_1^2 + (n_2-1) s_2^2}{n_1 + n_2 - 2}}$
:::

::: {.column width="50%"}
::: fragment
$= \sqrt{\dfrac{(27-1)39.145^2 + (17-1)71.914^2}{27 + 17 - 2}}$
:::
:::

::: {.column width="5%"}
::: fragment
$\phantom{\dfrac{1}{1}}\approx `r round(sqrt(((27-1)*39.145^2 + (17-1)*71.914^2)/(27 + 17 - 2)), 2)`$
:::
:::
:::

. . .

$t=\dfrac{(156.734-98.990)-0}{54.03\sqrt{\dfrac{1}{27} + \dfrac{1}{17}}} \approx `r round((156.734-98.990)/19, 2)`$

## Book price example {.smaller}

::: columns
::: {.column width="60%"}
$df = n_1 + n_2 - 2 = 27 + 17 - 2$

```{r}
#| fig.width: 5
#| fig.height: 3
ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dt, args = list(df=27 + 17 - 2), color = "navy", size = 1) +
  stat_function(fun = dt, args = list(df=27 + 17 - 2), xlim = c(3.04, 4),
                geom = "area", fill = "navy", alpha = .4) +
  labs(x = "t", y = "Density", title = expression(H[a]: mu[1]-mu[2]!=0)) +
  theme_classic()  +
  geom_vline(xintercept = 3.04, linetype = 2) +
  geom_vline(xintercept = -3.04, linetype = 2) +
  geom_curve(aes(x = 3.75, y = .08, xend = 3.2, yend = .01), arrow = arrow(length = unit(0.03, "npc")), curvature = -.2) +
  annotate("text", x = 3.75, y = .15, label = "1/2\np-value", color = "navy", size = 5)
```
:::

::: {.column width="40%"}
::: fragment
```{r}
#| echo: true
2 * (1 - pt(3.04, df = 27 + 17 - 2))
```
:::
:::
:::

. . .

There is a statistically discernible difference between the average textbook prices between STEM and social science sources ($t=3.04$, $df=42$, $p=0.004$).

## Confidence interval review

::: incremental
-   General equation for confidence interval from intro

    $$
      \rm statistic \pm q^* \times SE
      $$

-   Set of plausible values for our population parameter

-   Our confidence is in the entire process used to construct the interval
:::

## Two-sample interval

Construct an 89% confidence interval for the difference in average textbook price between the areas.

$$\rm statistic \pm q^* \times SE \Longrightarrow \bar{y}_1- \bar{y}_2 \pm t^*  \cdot s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$$ where

-   $s_p = \sqrt{\dfrac{(n_1 - 1) s_1^2 + (n_2-1) s_2^2}{n_1 + n_2 - 2}}$

-   ${\rm df} = n_1 + n_2 - 2$

## Book price example {.smaller}

```{r}
favstats(Price ~ Area, data = BookPrices) %>%
  select(Area, mean, sd, n) %>% 
  kable(digits = 3)
```

::: columns
::: {.column width="50%"}
Recall:

-   $SE = 54.03\sqrt{\dfrac{1}{27} + \dfrac{1}{17}} \approx `r round(54.03 * sqrt(1/27 + 1/17), 2)`$

-   $\rm{df}=27+17-2 = 42$
:::

::: {.column width="50%"}
::: fragment
Find the quantile

```{r}
#| fig.width: 5
#| fig.height: 3
#| out.width: 100%
quant <- qt((1-0.89)/2, df = 27 + 17 - 2)
ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dt, args = list(df=27 + 17 - 2), color = "navy", size = 1) +
  stat_function(fun = dt, args = list(df=27 + 17 - 2), xlim = c(quant, -quant),
                geom = "area", fill = "navy", alpha = .4) +
  labs(x = "t", y = "Density") +
  theme_classic()  +
  geom_curve(aes(x = 2.5, y = .1, xend = abs(quant), yend = .05), arrow = arrow(length = unit(0.03, "npc")), curvature = -.2) +
  annotate("label", x = 0, y = .15, label = "0.89", color = "navy", size = 6) +
  annotate("label", x = 2.5, y = .12, label = "t*", color = "navy", size = 6)
```
:::

::: fragment
```{r}
#| echo: true
qt(0.945, df= 42)
```
:::
:::
:::

## Book price example

```{=tex}
\begin{align*}
\bar{y}_1- \bar{y}_2 \pm t^*  \cdot s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}} &= (156.734-98.990) \pm `r round(qt(0.945, df= 42), 3)` (16.73)\\
&= (`r round((156.734-98.990) + c(-1,1)* round(qt(0.945, df= 42), 3) * 16.73, 2)`)
\end{align*}
```
We are 89% confident that, on average, STEM textbooks are between \$30.42 and \$85.06 more expensive than social science textbook prices.
