---
title: "Inference for Prediction"
subtitle: "Stat 230: Applied Regression Analysis"
format: 
  revealjs:
    theme: [serif, styles.scss]
editor: source
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false
library(tidyverse)
library(ggplot2)
library(rsample)
library(broom)
library(Stat2Data)
data("BlueJays")
library(purrr)
library(tidyverse)
library(ggformula)
library(patchwork)
library(ggdist)
library(distributional)
library(gganimate)
library(gifski)
library(fontawesome)

cars <- read.csv("https://aloy.rbind.io/kuiper_data/Cars.csv")

blue_jays_male <- BlueJays |> filter(Sex == 1)


theme_classic <- function() {
  ggplot2::theme_classic() +
    ggplot2::theme(
      plot.background = ggplot2::element_rect(fill = "#fffef5", color = NA),
      panel.background = ggplot2::element_rect(fill = "#fffef5", color = NA),
      axis.line = ggplot2::element_line(color = "#003069"),
      axis.text = ggplot2::element_text(color = "#003069"),
      axis.ticks = ggplot2::element_line(color = "#003069"),
      axis.title = ggplot2::element_text(color = "#003069")
    )
}

# set ggplot2 theme
ggplot2::theme_set(theme_classic())
```

# [PDF version of slides](pdf_slides/03-slr-prediction.pdf)


## Warm up

-   Work with a neighbor

-   Answer the questions associated with the warm up on the worksheet

-   Note that the explanatory variable is standardized <br> (mean 0, SD 1)

## Prediction

There are two types of predictions in regression

. . .

1.  Predicting the **mean response** at a specific value of $x$

    e.g., the average starting salary for some with a B.A. in statistics

<br>

. . .

2.  Predicting the response for a **specific future observation**

    e.g., predicting **your** starting salary (if you have a B.A. in statistics)

. . .

<br>

`r fa("people-arrows")` Think of two additional examples of each type of prediction.

## Inference for prediction

Best estimate $\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_0$

<br>

. . .

Interval formula: $\text{estimate} \pm q \times \text{SE}$

-   $\widehat{y}$ is our estimate
-   Use a $t$-distribution with $df=n-2$ to find $q$

<br>

. . .

We'll need different SEs depending on if we are building a

-   confidence interval for $\widehat{\mu}(Y|X_0)$

-   prediction interval for $\widehat{y}$ or $\text{Pred}(Y|X_0)$

## Standard errors

$\text{SE}(\widehat{\mu}(Y|X)) = \widehat{\sigma} \sqrt{\dfrac{1}{n} + \dfrac{(x_0-\overline{x})^2}{\sum_{i=1}^n (x_i - \overline{x})^2}}$

. . .

$\text{SE}(\widehat{y})= \widehat{\sigma} \sqrt{1+\dfrac{1}{n} + \dfrac{(x_0-\overline{x})^2}{\sum_{i=1}^n (x_i - \overline{x})^2}}$

::: {.fragment .fade-in-then-out}
`r fa("people-arrows")` Looking at the standard errors for the two intervals, which interval will be wider? Why does this make sense?
:::

::: {.fragment .fade-in}
`r fa("people-arrows")` As $x_0$ gets farther from $\overline{x}$ what happens to the standard errors?
:::

## R: Point estimate

We'll let R do the computational work

```{r}
#| include: false
car_lm <- lm(Price ~ Mileage, data = cars)
```

`predict` allows you to quickly calculate the value of $\widehat{y}$ for a given $x$ (or vector of $x$s)

```{r}
#| echo: true
predict(car_lm, newdata = data.frame(Mileage = 8221))
```

## R: Intervals

The `interval` argument allows you to specify the type of interval you want

```{r}
#| echo: true
predict(car_lm, newdata = data.frame(Mileage = 8221), 
        interval = "confidence")
```

<br>

```{r}
#| echo: true
predict(car_lm, newdata = data.frame(Mileage = 8221), 
        interval = "prediction")
```

## R: SEs

Adding `se.fit = TRUE`returns the necessary standard errors for "by hand" calculations

```{r}
predict(car_lm, newdata = data.frame(Mileage = 8221), se.fit = TRUE)
```


## Activity

-   Work with a neighbor

-   Work through the inference for prediction example on the worksheet

-   The R tutorial is linked on Moodle, also can follow the QR code

![](images/clipboard-1954158138.png){width="750"}

## Conditions required for inference

Our model must be valid for inference to be valid

$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ where $\varepsilon_i \overset{\rm iid}{\sim} N (0, \sigma^2)$

<br>

Conditions to check:

-   Linear relationship is appropriate
-   Errors are independent and identically distributed (iid)
-   Errors are normally distributed
-   Variance of the errors doesn't depend on $x$

## Regression conditions {.smaller}

What happens if our assumptions aren't valid?

-   **Linearity:** if nonlinear, everything breaks!

-   **Independence:** estimates are still unbiased (i.e. we fit the right line) but measures of the accuracy of those estimates (the SEs) are typically too small

-   **Normality:** estimates are still unbiased (i.e. we fit the right line), SEs are correct BUT confidence/prediction intervals are wrong (we can't use t-distribution)

-   **Constant error variance:** estimates are still unbiased but standard errors are wrong (and we don't know how wrong)

# RMarkdown demo
