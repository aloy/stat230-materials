---
title: "Multiple Regression Diagnostics"
subtitle: "Stat 230: Applied Regression Analysis"
format: 
  revealjs:
    chalkboard: 
      buttons: false
editor: source
---

```{r}
#| include: false
library(ggformula)
library(broom)
library(patchwork)
library(dplyr)
library(car)
library(gridExtra)
UBSprices <- read.csv("http://aloy.rbind.io/data/UBSprices.csv")
bigmac.lm <- lm(bigmac2009 ~ bigmac2003, data = UBSprices)

```

## Warm up

With your group

- List the conditions for MLR

- Brainstorm ways to check these conditions


## Regression is *not* resistant to outliers

Solid line = with outliers; dashed line = without outliers

```{r fig.height=4, fig.width=4, echo=FALSE}
resid_df <- augment(bigmac.lm)
resid_df <- mutate(resid_df, outlier = .std.resid > 3)

gf_point(bigmac2009 ~ bigmac2003, data = filter(resid_df, outlier == TRUE), shape = 17, color = "orange") %>%
gf_point(bigmac2009 ~ bigmac2003, data = filter(resid_df, outlier == FALSE), shape = 16) %>%
  gf_lm(bigmac2009 ~ bigmac2003, data = resid_df) %>%
  gf_lm(bigmac2009 ~ bigmac2003, data = resid_df[-c(12, 21),], linetype = 2) %>%
  gf_labs(x = "Price in 2003", y = "Price in 2009", 
          title = "Price of a Big Mac (minutes of labor)") |>
  gf_theme(theme_classic())
```

---

## SLR is *not* resistant to outliers

<br>

$$\widehat{\beta}_1 = r \cdot \dfrac{s_y}{s_x} \qquad \qquad \widehat{\beta}_0 = \bar{y} - \widehat{\beta}_1 \bar{x}$$

<br>

Why?

- $r$ is not resistant
- $s_y$, $s_x$ are not resistant
- $\bar{x}$, $\bar{y}$ are not resistant


## MLR is *not* resistant to outliers

We're still choosing the $\widehat{\beta}_i$ to minimize the sum of squared residuals

$$
\begin{split}
SSE &= \sum_{i=1}^n \left( y_i - \widehat{y}_i \right)^2\\ 
&= \sum_{i=1}^n \left( y_i - \widehat{\beta}_0 - \widehat{\beta}_1 x_{i1} - \widehat{\beta}_2 x_{i2} - ... - \widehat{\beta}_p x_{ip} \right)^2
\end{split}
$$

Sum's aren't resistant to outliers!


---

## Types of outliers

```{r echo=FALSE, fig.width = 7, fig.height =5}
N <- 25
std.dev <- .5
x <- rnorm(N)
y <- x+rnorm(N,sd=std.dev)
xnew <- c(0,10,10,10)
ynew <- c(10,0,predict(lm(y~x),newdata=list(x=10))+rnorm(1, sd = std.dev),20)
df <- data.frame(x = x, y = y)
df.out <- data.frame(x = xnew, y = ynew, type = factor(1:4))
p1 <- gf_point(y ~ x, data = df, title = "Outlier in Y", color = "black") |>
  gf_point(y ~ x, data = filter(df.out, type == 1), color = "orange")
inf1 <- p1 %>%
  gf_lm(y ~ x, data = df, color = "black") %>%
  gf_lm(y ~ x, data = rbind(df, c(x = xnew[1], y = ynew[1])), color = "orange")
p2 <- gf_point(y ~ x, data = df, title = "Outlier in X") %>%
  gf_point(y ~ x, data = filter(df.out, type == 2), color = "orange")
inf2 <- p2 %>%
  gf_lm(y ~ x, data = df, color = "black") %>%
  gf_lm(y ~ x, data = rbind(df, c(x = xnew[2], y = ynew[2])), color = "orange")
p3 <- gf_point(y ~ x, data = df, title = "Outlier in Y + X") %>%
  gf_point(y ~ x, data = filter(df.out, type == 3), color = "orange")
inf3 <- p3 %>%
  gf_lm(y ~ x, data = df, color = "black") %>%
  gf_lm(y ~ x, data = rbind(df, c(x = xnew[3], y = ynew[3])), color = "orange")
p4 <- gf_point(y ~ x, data = df, title = "Outlier in Y + X") %>%
  gf_point(y ~ x, data = filter(df.out, type == 4), color = "orange")
inf4 <- p4 %>%
  gf_lm(y ~ x, data = df, color = "black") %>%
  gf_lm(y ~ x, data = rbind(df, c(x = xnew[4], y = ynew[4])), color = "orange")
p1 + p2 + p3 + p4
```


## Outliers

```{r fig.height=3, fig.width=9, echo=FALSE, fig.align='center'}
ell1 <- gf_point(prestige ~ education, data = Duncan, ylab = "Y", xlab = expression(X[1])) +
  stat_ellipse(color = "blue", linetype = 2)

ell2 <- gf_point(prestige ~ income, data = Duncan, ylab = "Y", xlab = expression(X[2]),
                 title = "Outlier in Y") +
  stat_ellipse(color = "blue", linetype = 2)

ell3 <- gf_point(income ~ education, data = Duncan, ylab = expression(X[1]), xlab = expression(X[2]),
                 title = "Possible outliers in 'X'") +
  stat_ellipse(color = "blue", linetype = 2)

grid.arrange(ell1, ell2, ell3, ncol = 3)
```

- In higher dimensions, outliers can be tricky to detect

- To detect outliers in X, you have to consider the multivariate relationships between all of the predictors

---

## Not all outliers are *influential*

<font color="orange">Orange line</font> = SLR with outlier; Black line = SLR without outlier

```{r}
#| fig.width: 7
#| fig.height: 5
inf1 + inf2 + inf3 +inf4
```

---

## Influential points

Points that are able to substantially change the fitted model ares **influential**

<br>

. . .

How do we find outliers and determine if they are influential?

- Plot the data
- Plot the standardized residuals
- Fit the model with/without the point(s)
- Calculate to *influence diagnostics*



```{r, include=FALSE}
lm0 <- lm(y~x)
lm1 <- lm(c(y,ynew[1])~c(x,xnew[1]))
lm2 <- lm(c(y,ynew[2])~c(x,xnew[2]))
lm3 <- lm(c(y,ynew[3])~c(x,xnew[3]))
lm4 <- lm(c(y,ynew[4])~c(x,xnew[4]))

aug0 <- augment(lm0)
aug0$model <- 0
aug1 <- augment(lm1) %>%
  rename(y = 2, x = 3) %>%
  mutate(model = 1, .rownames = 1:26, .outlier = c( rep("no", 25),"yes"))
aug2 <- augment(lm2) %>%
  rename(y = 2, x = 3)  %>%
  mutate(model = 2, .rownames = 1:26, .outlier = c( rep("no", 25),"yes"))
aug3 <- augment(lm3)  %>%
  rename(y = 2, x = 3)  %>%
  mutate(model = 3, .rownames = 1:26, .outlier = c( rep("no", 25),"yes"))
aug4 <- augment(lm4)  %>%
  rename(y = 2, x = 3)  %>%
  mutate(model = 4, .rownames = 1:26, .outlier = c( rep("no", 25),"yes"))

aug <- rbind(aug1, aug2, aug3, aug4)
```

---

## Leverage {.smaller}

Measures the **potential to influence** the SLR line

:::{.columns}
::: {.column width="15%"}
For SLR:
:::
::: {.column width="85%"}
$$h_i = \frac{1}{n-1} \left[ \frac{x_i - \overline{x}}{s_x} \right]^2 + \frac{1}{n} = \dfrac{1}{n} + \frac{\left(x_i - \overline{x}\right)^2}{\sum \left(x_i - \overline{x}\right)^2}$$
:::
:::

:::{.columns}
::: {.column width="15%"}
For MLR:
:::
::: {.column width="85%"}
more complicated because we're looking at a distance of a case
from the average of p predictors
:::
:::


**Cutoff**

- $2(p+1)/n$
- $3(p+1)/n$
- Also a good idea to examine a histogram/index plot

::: footer
I use p to denote the number of slope coefficients, so there are p+1
regression coefficients
:::

---

## Leverage

"Flares up" when value of x is far from the mean

```{r}
#| fig.height: 5.5
#| fig.width: 10
levp <- gf_point(.hat ~ .rownames, data = aug, color = ~.outlier, group = ~model) %>%
  gf_hline(yintercept = (2*2)/26, linetype = 3) %>%
  gf_hline(yintercept = (3*2)/26, linetype = 2) %>%
  gf_facet_wrap(~model, ncol = 4) %>%
  gf_refine(scale_color_manual(values = c("black", "orange")),
            theme(legend.position = "none")) %>%
  gf_labs(x = "Index", y = "Leverage")

(p1 | p2 | p3 | p4) / levp
```

## Leverage

In higher dimensions, leverage is focusing on the distance from the average of all of the predictors

```{r fig.width = 9, fig.height = 3, echo=FALSE}
duncan_lm <- lm(prestige ~ income + education, data = Duncan)
duncan_aug <- augment(duncan_lm) %>%
  mutate(lev.flag = .hat > (2*3)/ length(.hat),
         cook.flag = .cooksd > 1)

le1 <- gf_point(prestige ~ education, data = duncan_aug, color = ~factor(lev.flag)) %>%
  gf_refine(scale_color_manual(values = c("black", "orange")),
            theme(legend.position = "none")) %>%
  gf_labs(x = expression(X[1]), y = "Y")

le2 <- gf_point(prestige ~ income, data = duncan_aug, color = ~lev.flag) %>%
  gf_refine(scale_color_manual(values = c("black", "orange")),
            theme(legend.position = "none")) %>%
  gf_labs(x = expression(X[2]), y = "Y")

le3 <- gf_point(income ~ education, data = duncan_aug, color = ~lev.flag) %>%
  gf_hline(yintercept = mean(Duncan$income), linetype = 2) %>%
  gf_vline(xintercept = mean(Duncan$education), linetype = 2) %>%
  gf_refine(scale_color_manual(values = c("black", "orange")),
            theme(legend.position = "none")) %>%
  gf_labs(x = expression(X[1]), y = expression(X[2]))

grid.arrange(le1, le2, le3, ncol = 3)
```


---

## Standardized residuals {.smaller}

Standardized residuals are calculated by dividing the residuals by their standard deviation

$$r_i = \frac{e_i}{\widehat{\sigma} \sqrt{1 - \dfrac{1}{n} - \dfrac{(x_i - \overline{x})^2}{\sum (x_i - \overline{x})^2}}} = \dfrac{e_i}{ \widehat{\sigma} \sqrt{1 - h_{i}}}$$


<br>

**Guidelines**: 

- $|r_i|>2$ for small data sets
- $|r_i|>4$ for large data sets

:::footer
Sleuth calls these (internally) studentized residuals while R calls these
standardized residuals
:::

---

## Standardized residuals

"Flare up" when value of y is far from $\widehat{y}$


```{r}
#| fig.height: 5.5
#| fig.width: 10
stdres <- gf_point(.std.resid ~ .rownames, data = aug, color = ~.outlier, group = ~model) %>%
  gf_hline(yintercept = 2, linetype = 3) %>%
  # gf_hline(yintercept = 3, linetype = 2) %>%
  gf_hline(yintercept = -2, linetype = 3) %>%
  # gf_hline(yintercept = -3, linetype = 2) %>%
  gf_facet_wrap(~model, ncol = 4) %>%
  gf_refine(scale_color_manual(values = c("black", "orange")),
            theme(legend.position = "none")) %>%
  gf_labs(x = "Index", y = "Standardized Residual")

p11 <- p1 |>
  gf_lm(y ~ x, data = rbind(df, c(x = xnew[1], y = ynew[1])), color = "orange")

p22 <- p2 |>
  gf_lm(y ~ x, data = rbind(df, c(x = xnew[2], y = ynew[2])), color = "orange")

p33 <- p3 |>
  gf_lm(y ~ x, data = rbind(df, c(x = xnew[3], y = ynew[3])), color = "orange")

p44 <- p4 |>
  gf_lm(y ~ x, data = rbind(df, c(x = xnew[1], y = ynew[4])), color = "orange")



(p11 | p22 | p33 | p44 ) / stdres
```

## DFFITS {.smaller}

Measures effect the i<sup>th</sup> case has on its **own** fitted value


$$DFFITS_i = \frac{\widehat{y}_i - \widehat{y}_{i(i)}}{\widehat{\sigma}_{(i)} \sqrt{h_i}}$$
where the subscript $(i)$ indicates that the value is based on a model when observation i is omitted

<br>

**Cutoff**

- 1 for small/medium data sets
- $2\sqrt{\dfrac{p+1}{n}}$ for large data sets


:::footer
DFFITS is not discussed in *Sleuth*
:::


---

## Cook's distance {.smaller}

Measures effect the i<sup>th</sup> case has on **all** of the fitted values

$$D_i= \sum_{j=1}^n\frac{ \left(\widehat{y}_{j(i)} - \widehat{y}_j \right)^2}{(p+1) \widehat{\sigma}} = \frac{r_i^2}{p+1} \left( \frac{h_i}{1-h_i} \right)$$

where $\widehat{y}_{j(i)}$ is the fitted value for observation j based on a model when observation i is omitted


<br>

**Cutoff**

- $D_i$ near or above 1 indicates large influence

- Find what quantile of $F_{p+1, n-p-1}$ $D_i$ corresponds to, larger than 0.5 is cause for concern

- Also can judge relative standing of  $D_i$ — make an index plot

---

## Cook's distance

```{r}
#| fig.height: 5.5
#| fig.width: 10
cookp <- gf_point(.cooksd ~ .rownames, data = aug, color = ~.outlier, group = ~model) %>%
  gf_hline(yintercept = 1, linetype = 2) %>%
  gf_facet_wrap(~model, ncol = 4) %>%
  gf_refine(scale_color_manual(values = c("black", "orange")),
            theme(legend.position = "none")) %>%
  gf_labs(x = "Index", y = "Cook's D")

(inf1 | inf2 | inf3 | inf4) / cookp
```

## Do we calculate these by hand?

No! 

- `augment()` from the `broom` package
- `influence.measures()` from the `car` package


## Advice from *Sleuth*

![](images/sleuth_outlier_flowchart.png){fig-position="center"}

:::footer
Source: Display 11.8 in *Sleuth* (3rd edition)
:::
