---
title: "Inference for SLR"
subtitle: "Stat 230: Applied Regression Analysis"
format: 
  revealjs:
    theme: [serif, styles.scss]
editor: source
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| include: false
library(tidyverse)
library(ggplot2)
library(rsample)
library(broom)
library(Stat2Data)
data("BlueJays")
library(purrr)
library(tidyverse)
library(ggformula)
library(patchwork)
library(ggdist)
library(distributional)
library(gganimate)
library(gifski)

blue_jays_male <- BlueJays |> filter(Sex == 1)


theme_classic <- function() {
  ggplot2::theme_classic() +
    ggplot2::theme(
      plot.background = ggplot2::element_rect(fill = "#fffef5", color = NA),
      panel.background = ggplot2::element_rect(fill = "#fffef5", color = NA),
      axis.line = ggplot2::element_line(color = "#003069"),
      axis.text = ggplot2::element_text(color = "#003069"),
      axis.ticks = ggplot2::element_line(color = "#003069"),
      axis.title = ggplot2::element_text(color = "#003069")
    )
}

# set ggplot2 theme
ggplot2::theme_set(theme_classic())
```

# Please sit with your group from last class

## Warm up

- Work through the warm-up questions with your group

- I'll ask half of the groups to start with hypothesis tests, the other half will start with confidence intervals


## Example

- A biologist collected data on the body measurements of captured blue jays
- Let's explore the association between body mass and head length

```{r}
#| fig.height: 3.5
#| fig.width: 5
#| fig.align: center

blue_jays_male |>
  ggplot(aes(x = Mass, y = Head)) +
  geom_point(alpha = 0.6, color = "#003069") +
  geom_lm(color = "#f15a31") +
  theme_classic() +
  labs(x = "Body mass (grams)", y = "Head length (mm)")

```

## A deterministic model

- $Y_i = \beta_0 + \beta_1 x_i$

- No uncertainty!

```{r}
#| fig.height: 3.5
#| fig.width: 5
#| fig.align: center

blue_jays_male |>
  ggplot(aes(x = Mass, y = Head)) +
  # geom_point(alpha = 0.6, color = "#003069") +
  geom_lm(color = "#f15a31") +
  theme_classic() +
  labs(x = "Body mass (grams)", y = "Head length (mm)") 
```



## Simple Linear Regression Model

$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ where $\varepsilon_i \overset{\rm iid}{\sim} N (0, \sigma^2)$

<br>

-   Linear relationship between $x$ and $y$
-   Errors are independent and identically distributed (iid)
-   Errors are normally distributed
-   Errors have mean 0
-   Variance of the errors doesn't depend on $x$

## Linear Regression Model

$Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ where $\varepsilon_i \overset{\rm iid}{\sim} N (0, \sigma^2)$

```{r}
#| fig.height: 3.5
#| fig.width: 5
#| fig.align: center

# bike_lm <- lm(rides ~ temp_actual, data = bikes)
# bike_dsn <- data.frame(
#   temp_actual = c(50, 60, 70, 80),
#   yhat = predict(bike_lm, data.frame(temp_actual = c(50, 60, 70, 80))),
#   resid_sd = rep(sigma(bike_lm), 4)
# )

mod <- lm(Head ~ Mass, data = blue_jays_male)

blue_jay_dsn <- 
  tibble(
    Mass = c(60, 65, 70, 75, 80),
    yhat = predict(mod, newdata = data.frame(Mass = c(60, 65, 70, 75, 80))),
    resid_sd = sigma(mod)
  )

blue_jays_male |>
  ggplot(aes(x = Mass, y = Head)) +
  stat_slab(aes(x = Mass, ydist = "norm", arg1 = yhat, arg2 = resid_sd), data = blue_jay_dsn, alpha = 0.4, inherit.aes = FALSE, scale = 0.6, fill = "#f15a31") +
  geom_lm(color = "#f15a31") +
  theme_classic() +
  labs(x = "Body mass (grams)", y = "Head length (mm)")

```

## Linear Regression Model

$Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ where $\varepsilon_i \overset{\rm iid}{\sim} N (0, \sigma^2)$

```{r}
#| fig.height: 3.5
#| fig.width: 5
#| fig.align: center

predict_head <- function(n, x) {
rnorm(n, mean = predict(mod, data.frame(Mass = x)), sd = sigma(mod))
}

set.seed(1234)

mod_sims <- tibble(
  Mass = rep( c(60, 65, 70, 75, 80), each = 20),
  Head = c(predict_head(20, 60), predict_head(20, 65), predict_head(20, 70), predict_head(20, 75), predict_head(20, 80))
)

p <- blue_jays_male |>
  ggplot(aes(x = Mass, y = Head)) +
  stat_slab(aes(x = Mass, ydist = "norm", arg1 = yhat, arg2 = resid_sd), data = blue_jay_dsn, alpha = 0.4, inherit.aes = FALSE, scale = 0.6, fill = "#f15a31") +
  geom_point(data = mod_sims, aes(Mass, Head), inherit.aes = FALSE, alpha = 0.4, color = "#003069") +
  geom_lm(color = "#f15a31") +
  theme_classic() +
  labs(x = "Body mass (grams)", y = "Head length (mm)")
p
```

## Linear Regression Model

$Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ where $\varepsilon_i \overset{\rm iid}{\sim} N (0, \sigma^2)$

```{r}
#| fig.height: 3.5
#| fig.width: 5
#| fig.align: center

blue_jays_male |>
  ggplot(aes(x = Mass, y = Head)) +
  geom_point(alpha = 0.6, color = "#003069") +
  geom_lm(color = "#f15a31") +
  theme_classic() +
  labs(x = "Body mass (grams)", y = "Head length (mm)") +
  xlim(60, 85) +
  ylim(51.5, 60.7)
```

## Notation

Mean function:

$\quad E(Y|X)=\mu\{Y|X\} = \beta_0 + \beta_1 x_i$

<br>

Fitted model equation:

$\quad \widehat{y}_i = \widehat{\mu}\{Y|X\} = \widehat{\beta}_0 + \widehat{\beta}_1 x_i$


## Uncertainty in the parameters

```{r}
#| echo: false
#| fig.height: 3
#| fig.width: 6



boots <- bootstraps(blue_jays_male, times = 20, apparent = TRUE)

fit_lm_on_bootstrap <- function(split) {
    lm(Head ~ Mass, data = analysis(split))
}

boot_models <-
  boots |>
  mutate(model = map(splits, fit_lm_on_bootstrap),
         coef_info = map(model, tidy))

boot_coefs <- 
  boot_models %>% 
  unnest(coef_info) |>
  select(-std.error, -statistic, -p.value) |>
  pivot_wider(names_from = term, values_from = estimate)

boot_aug3 <- 
  boot_models %>% 
  slice(1:3) %>% 
  mutate(augmented = map(model, augment)) %>% 
  unnest(augmented)

labels <- c("Bootstrap01" = "Sample 1", "Bootstrap02" = "Sample 2", "Bootstrap03" = "Sample 3")

ggplot(boot_aug3, aes(Mass, Head)) +
  geom_line(aes(y = .fitted, group = id), col = "blue") +
  geom_point() +
  facet_wrap(~id, labeller = as_labeller(labels)) +
  labs(x = "Body mass (grams)", y = "Head length (mm)")
```

## Uncertainty in the parameters

```{r}
#| echo: false
#| fig.height: 3
#| fig.width: 5
hop <- 
  ggplot(data = boot_coefs) +
  geom_abline(aes(slope = Mass, intercept = `(Intercept)`, group = id), color = "#0072B2", size = 0.5) +
  # theme_dviz_grid() +
  geom_point(data = blue_jays_male, aes(Mass, Head), color = "grey60", inherit.aes = FALSE) +
  labs(x = "Body mass (grams)", y = "Head length (mm)") +
  theme(
    strip.text = element_blank(),
    axis.ticks = element_blank(),
    axis.ticks.length = unit(0, "pt"),
    # plot.margin = margin(7, 1.5, 3.5, 1.5)
  ) +
  # panel_border() +
  transition_states(id, 0, 1) +
  shadow_mark(future = TRUE, color = "gray50", alpha = 1/20)

animate(hop)
```

## Uncertainty in the parameters

```{r}
#| echo: false
#| fig.width: 5
#| fig.height: 3
ggplot(data = boot_coefs) +
  geom_abline(aes(slope = Mass, intercept = `(Intercept)`, group = id), color = "#0072B2", size = 0.5, alpha = 0.2) +
  # theme_dviz_grid() +
  geom_point(data = blue_jays_male, aes(Mass, Head), color = "grey60", inherit.aes = FALSE) +
  theme(
    # strip.text = element_blank(),
    axis.ticks = element_blank(),
    axis.ticks.length = unit(0, "pt"),
    plot.margin = margin(7, 1.5, 3.5, 1.5)
  )
```


## Sampling distribution



```{r}
#| echo: false
#| fig.height: 3
#| fig.width: 6

boots1k <- bootstraps(blue_jays_male, times = 1000, apparent = TRUE)


boot_models_1k <-
  boots1k |>
  mutate(model = map(splits, fit_lm_on_bootstrap),
         coef_info = map(model, tidy))

boot_coefs <- 
  boot_models_1k %>% 
  unnest(coef_info) |>
  select(-std.error, -statistic, -p.value) |>
  pivot_wider(names_from = term, values_from = estimate) |>
  rename(
    intercept = `(Intercept)`,
    slope = Mass
  )

slope_dist <- ggplot(boot_coefs) +
   ggdist::stat_dotsinterval(aes(x = slope), quantiles = 500, scale = 0.6, position = position_nudge(y = 0.175)) +
  labs(title = "Distribution of the slope", y = "Density")

intercept_dist <- ggplot(boot_coefs) +
  ggdist::stat_dotsinterval(aes(x = intercept), quantiles = 500, scale = 0.6, position = position_nudge(y = 0.175))  +
  labs(title = "Distribution of the intercept", y = "Density")

slope_dist + intercept_dist 
```

## Sampling distribution

We use the t distribution with df =  n - 2 to model the sampling distribution of a regression coefficient

```{r}
#| echo: false
#| fig.height: 3
#| fig.width: 6

mod <- lm(Head ~ Mass, data = blue_jays_male)

sdsn_df <- tibble(
  term = c("slope", "intercept"),
  mean = c(mean(boot_coefs$slope), mean(boot_coefs$intercept)),
  df = c(62, 62),
  sd = diag(vcov(mod)) |> sqrt() |> unname() |> rev()
)



slope_slab <- 
  sdsn_df |> 
  filter(term == "slope") |>
  ggplot() +
  ggdist::stat_dotsinterval(data = boot_coefs, aes(x = slope), quantiles = 500, scale = 0.6, position = position_nudge(y = 0), inherit.aes = FALSE) +
  ggdist::stat_slab(aes(xdist = dist_student_t(df = df, mu = mean, sigma = sd)), 
                    slab_size = 0.5, 
                    fill = "#0072B2",
                    alpha = 0.4) +
  xlab("Slope")

intercept_slab <- 
  sdsn_df |> 
  filter(term == "intercept") |>
  ggplot() +
  ggdist::stat_dotsinterval(data = boot_coefs, aes(x = intercept), quantiles = 500, scale = 0.6, position = position_nudge(y = 0), inherit.aes = FALSE) +
  ggdist::stat_slab(aes(xdist = dist_student_t(df = df, mu = mean, sigma = sd)), 
                    slab_size = 0.5, 
                    fill = "#0072B2",
                    alpha = 0.4) +
  xlab("Intercept")

slope_slab + intercept_slab
```


## Sampling distribution

- t distribution with $df =  n - 2$

. . .

- Mean = $\beta_i$

. . .

- Standard deviation = $SE(\widehat{\beta}_1) = \widehat{\sigma} / \sqrt{\sum (x_i - \overline{x} )^2}$


## Confidence interval

::::{.columns}
::: {.column width="15%"}
statistic
:::
::: {.column width="7%"}
±
:::
::: {.column width="70%"}
critical value × standard error
:::
::::

<br>

::::{.columns}
::: {.column width="15%"}
$\widehat{\beta}_i$
:::
::: {.column width="7%"}
±
:::
::: {.column width="33%"}
$t^* \cdot SE(\widehat{\beta}_i)$
:::
::::



## Hypothesis test {.scrollable}



$H_0: \beta_i = \#$ vs. $H_a:$ choose one of $\begin{array}{c} \beta_i \ne \#\\ \beta_i <\#  \\ \beta_i >\# \end{array}$ 

<br>

. . .

::::{.columns}
::: {.column width="30%"}
test statistic
:::
::: {.column width="70%"}
$= \dfrac{\text{estimate} - \text{null value}}{\text{SE}}$
:::
::::
<br>

::::{.columns}
::: {.column width="30%"}

:::
::: {.column width="70%"}
$=\dfrac{\widehat{\beta}_i - \text{null value}}{SE(\widehat{\beta}_i)}$
:::
::::

. . .

<br>

Find appropriate tail areas under the  t-distribution with $df = n - 2$


## Coefficient table

Standard output for a regression model includes a coefficient table

```{r}
#| echo: false
tidy(mod)
```


## Your turn

- Work through the questions on the handout

- Write your group's answer to each question on the whiteboard

- Be ready to share your answers with the class -- a different person should speak than spoke last class


## A note on reporting test results


- Only report at most 4 decimals for a p-value

- Report the estimate, test statistic, df, and p-value

- Remember that $\alpha = 0.05$ isn't magic, it's simply "traditional" in many disciplines



## Conditions required for inference

Our model must be valid for inference to be valid


$y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$ where $\varepsilon_i \overset{\rm iid}{\sim} N (0, \sigma^2)$

<br>

Conditions to check:

- Linear relationship is appropriate
- Errors are independent and identically distributed (iid)
- Errors are normally distributed
- Variance of the errors doesn't depend on $x$