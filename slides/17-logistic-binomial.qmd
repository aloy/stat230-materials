---
title: "Modeling Binomial Counts"
subtitle: "Logistic regression -- Stat 230"
format: 
  revealjs:
    chalkboard: 
      buttons: false
editor: source
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE, comment = NULL)

library(ggplot2)
library(ggformula)
library(dplyr)
library(car)
library(gridExtra)
library(broom)
library(Sleuth3)
library(ggthemes)
```


## Moth coloration data {.smaller}


- J. A. Bishop studied how natural selection worked on moths in England. Trees near Liverpool England were blackened by air pollution from the mills (1970’s).

- 7 locations chosen, progressively farther from Liverpool

- At each location, 8 trees were chosen at random and equal number of light and dark moths were glued on the trees 

- After 24 hours, the number of moths taken (presumably by birds) were counted for each morph


```{r echo=FALSE}
knitr::kable(head(case2102, 4),  format = "html")
```

::: {.footer style="text-align: left; margin-left: 50px"}
Example adapted from *The Statistical Sleuth*
:::
## [Moth Coloration and Natural Selection]{style="color:#FFFFFF"} {background-image="http://catherinephamevolution.weebly.com/uploads/4/9/7/3/49739619/189059_orig.jpg" background-size="cover" background-position="bottom" style="color: "}


. . .

<br>

::: r-stack
:::{style="background: #FFFFFF; width: 1000px; height: 300px; border-radius: 10px;"}
:::{style="margin: 50px;"}
- Is the proportion of moths removed different between the light and dark trees?

- Does this proportion depend on distance?
:::
:::
:::




## Binomial response


- $Y_i =$ the number of moths removed (i.e. successes) on each tree, in each morph

. . .

- $Y_i =$ sum of $n_i$ success/failure (Bernoulli) trials

. . .

- We will **assume** that these trials are independent

. . .

::: r-stack
:::{style="background: #B7CFDC; width: 1000px; height: 300px; border-radius: 10px;"}
:::{style="margin: 50px;"}
**Binomial distribution**

The sum of independent and identically distributed success/failure trials follows a Binomial(n, p) distribution.
:::
:::
:::


## Binomial logistic regression {.smaller}


**Goal:** Model $\pi_i = {\rm P}(\text{success} | x_{1i}, x_{2i}, \ldots, x_{pi})$

. . .

<br>

::: r-stack
:::{style="background: #B7CFDC; width: 1000px; height: 150px; border-radius: 10px;"}
:::{style="margin: 50px;"}
${\rm logit}(\pi_i) = \log\left( \dfrac{\pi_i}{1- \pi_i} \right) = \beta_0 + \beta_1 x_{1i} +  \cdots + \beta_p x_{pi} = \eta_i$
:::
:::
:::

. . .

<br>

::: r-stack
:::{style="background: #D9E4EC; width: 1000px; height: 150px; border-radius: 10px;"}
:::{style="margin: 50px;"}
$\pi_i = \dfrac{e^{\eta_i}}{1 + e^{\eta_i}}= \dfrac{e^{\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_p x_{pi}}}{1 + e^{\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_p x_{pi}}}$
:::
:::
:::



<!-- $Y_i | x_1, \ldots, x_p \sim {\rm Binom}(n_i, \pi_i)$ -->


<!-- Thus, -->

<!-- - $\mu(Y_i | x_1, \ldots, x_p) = m_i \pi(X_i)$, where  -->

<!--     $\pi(X_i) = \dfrac{\exp(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_p x_{pi})}{1 + \exp(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_p x_{pi})}$ -->

<!-- - ${\rm Var}(Y_i | x_1, \ldots, x_p) = m_i \pi(X_i) (1 - \pi(X_i))$ -->



## EDA before modeling


The binomial logistic regression model **assumes** that the **logit is linearly related to the predictors**


```{r echo=FALSE, fig.height = 3, fig.width=5.5, fig.align='center', out.width = 700}
# Calculate the empirical logit
case2102 <- case2102 %>%
  mutate(prop = Removed / Placed,
         logit = log((Removed + 0.5) / (Placed - Removed + 0.5)))

# Plot the empirical logit vs. predictors
gf_point(logit ~ Distance, data = case2102, color = ~Morph, shape = ~Morph) %>%
  gf_lm() %>%
  gf_refine(scale_color_colorblind()) %>%
  gf_labs(x = "Distance (km) from Liverpool",
          y = "Sample Logit")
```



## Fitted model


\begin{aligned}
\log\left( \dfrac{\widehat{\pi}_i}{1- \widehat{\pi}_i} \right) = −1.289 + 0.0185 {\tt distance}_i + 0.415 {\tt morph}_i\\ − 0.0277 {\tt distance}_i \times {\tt morph}_i
\end{aligned}

. . .

```{r echo=FALSE, fig.height = 3, fig.width=5.5, out.width = 700, fig.align='center'}
moth_glm <- glm(Removed/Placed ~ Distance * Morph, data = case2102,
                family = binomial,
                weights = Placed)

plot_dark <- function(x) plogis(-1.289 + 0.0185 * x)

plot_light <- function(x) plogis(-0.717723 - 0.0092 * x)

gf_point(prop ~ Distance, data = case2102, color = ~Morph, shape = ~Morph) %>%
  # gf_line() %>%
  gf_refine(scale_color_colorblind()) %>%
  gf_labs(x = "Distance (km) from Liverpool",
          y = "Proportion removed") +
  stat_function(fun = plot_dark, color = "black") +
  stat_function(fun = plot_light, color = "darkorange")
```

::: {.footer style="text-align: left; margin-left: 50px"}
`morph = 1` for light and `0` for dark moths 
:::

## Interpretations

::: r-stack
:::{style="background: #B7CFDC; width: 1000px; height: 175px; border-radius: 10px;"}
:::{style="margin: 50px;"}
We interpret the fitted model just like we did in binary logistic regression!
:::
:::
:::


\begin{aligned}
\log\left( \dfrac{\widehat{\pi}_i}{1- \widehat{\pi}_i} \right) = −1.289 + 0.0185 {\tt distance}_i + 0.415 {\tt morph}_i\\ − 0.0277 {\tt distance}_i \times {\tt morph}_i
\end{aligned}


. . .


For dark moths, a 1 km increase in the distance from Liverpool is associated with about a 2% increase ($e^{0.0185} \approx 1.02$ factor increase) in the odds of being taken.


::: {.footer style="text-align: left; margin-left: 50px"}
`morph = 1` for light and `0` for dark moths 
:::

## Inference

::: r-stack
:::{style="background: #B7CFDC; width: 1000px; height: 175px; border-radius: 10px;"}
:::{style="margin: 50px;"}
We conduct inference for binomial logistic regression using the same tools as for binary logistic regression!
:::
:::
:::

. . .

::: {.r-fit-text}
```{r echo=FALSE}
knitr::kable(tidy(moth_glm, conf.int = TRUE), format = "html", digits = 3)
```
:::


- Wald tests and intervals for individual regression coefficients

. . .

- Drop-in deviance tests for sets of regression coefficients


## Model assumptions

- **Binomial counts**

- **Independence**: observations are independent

- **Linearity**: the log odds is a linear function of the predictors

- **Variance structure**: the variance of a binomial random variable is $n \pi (1 - \pi)$

    $\Longrightarrow$ so the variance of $Y_i$ is $n_i \pi_i (1 - \pi_i)$ (not constant!)


## Pearson residuals

::: r-stack
:::{style="background: #B7CFDC; border-radius: 10px; "}
:::{style="margin: 50px;"}
${\rm Pres}_i = \dfrac{Y_i - n_i \widehat{\pi}_i}{\sqrt{n_i \widehat{\pi}_i (1 - \widehat{\pi}_i)}}$
:::
:::
:::

For large enough $n_i$, Pearson residuals tend to behave like they come from $N(0,1)$

. . .

::: r-stack
:::{style="background: #D9E4EC; border-radius: 10px;"}
:::{style="margin: 50px;"}
How to use?

- Plot against fitted values, predictors

- Check for curvature, outliers
:::
:::
:::


## Pearson residuals

```{r fig.height = 2.5, fig.width = 7.5, echo=FALSE, out.width=775}
residualPlots(moth_glm, type = "pearson", layout = c(1,3), tests = FALSE)
```

- Evidence of a nonlinear relationship between the log odds and distance

- No obvious outliers


## Deviance residuals

::: r-stack
:::{style="background: #B7CFDC; width: 1000px; height: 175px; border-radius: 10px;"}
:::{style="margin: 50px;"}
::: {.r-fit-text}
${\rm Dres}_i = {\rm sign}(Y_i - n_i \widehat{\pi}_i) \sqrt{2 \left[ Y_i \log \left( \dfrac{Y_i}{n_i \widehat{\pi}_i} \right) + (n_i - Y_i) \log \left( \frac{n_i - Y_i}{n_i - n_i \widehat{\pi}_i} \right) \right]}$
:::
:::
:::
:::

For large enough $n_i$, deviance residuals tend to behave like they come from $N(0,1)$

. . .

::: r-stack
:::{style="background: #D9E4EC;  border-radius: 10px"}
:::{style="margin: 50px;"}
How to use?

- Plot against fitted values, predictors

- Check for curvature, outliers
:::
:::
:::




## Deviance residuals

```{r fig.height = 2.5, fig.width = 7.5, echo=FALSE, out.width=775}
residualPlots(moth_glm, type = "deviance", layout = c(1,3), tests = FALSE)
```

- Evidence of a nonlinear relationship between the log odds and distance

- No obvious outliers


## Drop-in deviance test


Do we need a quadratic term for distance?

\begin{align*}
{\rm H_0}: &  {\rm logit}(\pi_i)  = \beta_0 + \beta_1 {\tt dist} + \beta_2 {\tt morph} + \beta_3 {\tt dist} \times {\tt morph}\\
{\rm H_a}: & {\rm logit}(\pi_i)  = \beta_0 + \beta_1 {\tt dist} + \beta_2 {\tt morph} + \beta_3 {\tt dist} \times {\tt morph} \\ & \qquad \qquad + \beta_4 {\tt dist}^2 + \beta_45 {\tt dist}^2 \times {\tt morph}
\end{align*}

```{r echo=FALSE}
quad_glm <- update(moth_glm, . ~ . + I(Distance^2) * Morph)
```

<br>

Reduced model deviance = `r round(deviance(moth_glm), 4)`

Full model deviance = `r round(deviance(quad_glm), 4)`

<br>

```{r}
1 - pchisq(0.5016, df = 2)
```






<!-- ## Goodness-of-fit test -->


<!-- Deviance can be used to assess the binomial logistic regression -->

<!-- Observed | Predicted | True -->
<!-- :---------:|:---------:|:---------: -->
<!-- $Y_i$ | $\widehat{\pi}_i$ | $\pi_i$ -->


<!-- <br> -->



<!-- **Deviance:** $D = 2 \sum \left[ Y_i \log \left( \dfrac{Y_i}{m_i\widehat{\pi}_i} \right)  + (m_i - Y_i)  \log \left( \dfrac{m_i - Y_i}{m_i - m_i \widehat{\pi}_i} \right) \right]$ -->

<!-- $D \overset{\cdot}{\sim} \chi^2$ with $\text{d.f.}= n-(p+1)$, **if**s the model is correct and all $m_i$ are large enough (>5) -->


<!-- $n =$ \# of binomial observations (\# of proportions, not # of moths) -->






<!-- ## Goodness-of-fit test -->

<!-- <div style="float: right; clear: right; margin: 10px;">(the model is adequate)</div> -->

<!-- H<sub>0</sub>: ${\rm logit}(\pi_i) = \beta_0 + \beta_1 x_{1i} + \cdots + \beta_p x_{pi}$ -->

<!-- <div style="float: right; clear: right; margin: 10px;">(the model is inadequate)</div> -->

<!-- H<sub>a</sub>: ${\rm logit}(\pi_i) = \alpha_i$ -->


<!-- ```{r eval=FALSE} -->
<!-- summary(moth_glm) -->
<!-- ``` -->
<!-- ``` -->
<!-- Call: -->
<!-- glm(formula = Removed/Placed ~ Distance * Morph,  -->
<!--     family = binomial, data = case2102, weights = Placed) -->

<!--     Null deviance: 35.385  on 13  degrees of freedom -->
<!-- *Residual deviance: 13.230  on 10  degrees of freedom -->
<!-- ``` -->

<!-- ```{r} -->
<!-- 1- pchisq(13.230, 10) -->
<!-- ``` -->


<!-- :::{.aside} -->
<!-- Note: This test is *only valid for the binomial logistic regression*, it is not valid for binary logistic regression. -->
<!-- ::: -->










