---
title: "Polynomial Regression"
subtitle: "Stat 230: Applied Regression Analysis"
format: 
  revealjs:
    theme: [serif, styles.scss]
    scrollable: true
editor: source
editor_options: 
  chunk_output_type: console
code-annotations: hover
---

```{r setup}
#| include: false
library(tidyverse)
library(ggplot2)
library(rsample)
library(broom)
library(Stat2Data)
data("BlueJays")
library(purrr)
library(tidyverse)
library(ggformula)
library(patchwork)
library(ggdist)
library(fontawesome)
library(knitr)
library(gt)
library(gtsummary)
library(gridExtra)
library(here)
wildfires <- read.csv(here("data", "wildfires.csv"))
wildfires <- filter(wildfires, Year >= 1960)

regression_panel <- function(formula, scat.formula = NULL, data, xlab, ylab) {
  mod <- lm(formula, data = data)
  
  aug <- augment(mod)
  
  plot_fit <- makeFun(mod)
  
  if(is.null(scat.formula)) {
    orig_data <- gf_point(formula, data = data, xlab = xlab, ylab = ylab, color = "#003069")
  } else {
    orig_data <- gf_point(scat.formula, data = data, xlab = xlab, ylab = ylab, color = "#003069") 
  }
  
  orig_data <- orig_data %>% gf_fun(plot_fit(x) ~ x, color = "#f15a31")
  
  resid_plot <- gf_point(.std.resid ~ .fitted, data = aug, xlab = "Fitted values", ylab = "Standardized residuals") %>%
    gf_hline(yintercept = 0, linetype = 2, color = "#f15a31")
  
  qq_plot <- gf_qq(~.std.resid, data = aug, xlab = "N(0,1) quantiles", ylab = "Standardized residuals", color = "#003069") %>%
    gf_qqline(color = "#f15a31")
  
  grid.arrange(orig_data, resid_plot, qq_plot, ncol = 3)
}

theme_classic <- function() {
  ggplot2::theme_classic() +
    ggplot2::theme(
      plot.background = ggplot2::element_rect(fill = "#fffef5", color = NA),
      panel.background = ggplot2::element_rect(fill = "#fffef5", color = NA),
      axis.line = ggplot2::element_line(color = "#003069"),
      axis.text = ggplot2::element_text(color = "#003069"),
      axis.ticks = ggplot2::element_line(color = "#003069"),
      axis.title = ggplot2::element_text(color = "#003069")
    )
}

# set ggplot2 theme
ggplot2::theme_set(theme_classic())
```

# [PDF version of slides](pdf_slides/06-mlr-polynomial.pdf)

## Wildfires

- The National Interagency Coordination Center at the National Interagency Coordination Center compiles annual wildland fire statistics for federal and state agencies. 

- This information is provided through Situation Reports, which have been in use for several decades.

- Our goal is to model the number of acres burned over the years


::: footer
Data source: https://www.nifc.gov/fireInfo/fireInfo_stats_totalFires.html
:::

##

```{r fig.height=4, fig.width=5, message=FALSE, echo=FALSE, out.width=800}
gf_point(Acres ~ Year, data = wildfires, color = "#003069") %>%
  gf_labs(x = "Year", y = "Acres burned") %>%
  gf_smooth(size = .5)
```


## Option 1: SLR model


$\mu \lbrace y | x \rbrace = \beta_0 + \beta_1 x$

Is the fit reasonable?


```{r fig.height=2.75, fig.width=9, message=FALSE, echo=FALSE}
linear_mod <- lm(Acres ~ Year, data = wildfires)
regression_panel(Acres ~ Year, data = wildfires, xlab = "Year", ylab = "Acres burned")
```


## Option 2: Transform X


$\mu \lbrace y | x \rbrace = \beta_0 + \beta_1 x^2$

Is the fit reasonable?


```{r fig.height=2.75, fig.width=9, message=FALSE, echo=FALSE}
tform_mod <- lm(Acres ~ I(Year^2), data = wildfires)
regression_panel(Acres ~ I(Year^2), scat.formula = Acres ~ Year, data = wildfires, xlab = "Year", ylab = "Acres burned")
```

## Option 3: Polynomial model


$\mu \lbrace y | x \rbrace = \beta_0 + \beta_1 x + \beta_2x^2$

Is the fit reasonable?


```{r fig.height=2.75, fig.width=9, message=FALSE, echo=FALSE}
quad_mod <- lm(Acres ~ Year + I(Year^2), data = wildfires)
regression_panel(Acres ~ Year + I(Year^2), scat.formula = Acres ~ Year, data = wildfires, xlab = "Year", ylab = "Acres burned")
```


## The polynomial regression model


$$Y_i = \beta_0 + \beta_1 x_{i} + \beta_2 x_{i}^2 + \cdots + \beta_k x_{i}^k + \varepsilon_i, \quad \varepsilon_i \overset{iid}{\sim} N(0, \sigma)$$

Assumptions — same as in SLR

1. $\mu\{Y|x_i\}$, is a linear function

1. For each $x_i$, the sub-population of responses is normally distributed 

1. The standard deviation for each sub-population is $\sigma$

1.  Independent observations


## Interpreting the model

::: columns
:::{.column width="50%"}

```{r fig.height=3.25, fig.width=4, message=FALSE, echo=FALSE}
gf_point(Acres ~ Year, data = wildfires, xlab = "Year", ylab = "Acres burned", color = "#003069") %>%
  gf_lm(formula = y ~ x + I(x^2), color = "#f15a31")
```
:::
:::{.column width="50%"}
Focus on the expected change in $y$ for a specific one-unit increase in $x$

- e.g. change in acres burned from 1985 to 1990
    
- e.g. change in acres burned from 2005 to 2010
:::
:::

::: {style="font-size: 0.8em;"}
\begin{aligned}
\mu\{ y | x + 1 \} - \mu\{ y | x \} 
  &=  \left[ \beta_0 + \beta_1 (x+1) + \beta_2 (x+1)^2 \right] - \left[ \beta_0 + \beta_1 x + \beta_2 x^2 \right]\\
  &= \beta_1 + \beta_2 \left( 2x + 1 \right)
\end{aligned}

:::


## Inferences about coefficients

Inference uses the same t-based tools as SLR, but with 

$$\widehat{\sigma}  = \sqrt{\dfrac{{\sum_{i=1}^n e_i^2}}{n - (k+1)}}$$

i.e. the degrees of freedom of the t-distribution change



## Testing a single coefficient

::: {style="font-size: 0.9em;"}
**Hypotheses:** $H_0: \ \beta_j = \#$ vs. $H_a: \ \beta_j \underset{>}{\overset{<}{\ne}} \#$
:::

. . .

::: {style="font-size: 0.9em;"}
**Test statistic:** $t = \dfrac{\hat{\beta}_j - \#}{SE(\beta_j)}$
:::
. . .

::: {style="font-size: 0.9em;"}
**Reference distribution:** $t$ distribution with d.f. = $n-(k+1)$


**p-value:** Area in the tail(s) specified by $H_a$
:::


```{r echo=FALSE, fig.height = 2, fig.width = 7.5, out.width=850}
p1 <- ggplot(data = data.frame(x = c(-3.5, 3.5)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = 5)) +
  stat_function(fun = dt, args = list(df = 5), xlim = c(-3.5, -2), geom = "area", alpha = .4) +
  ggtitle(expression(H[a]: beta[j] < 0)) +
  theme(axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank())

p2 <- ggplot(data = data.frame(x = c(-3.5, 3.5)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = 5)) +
  stat_function(fun = dt, args = list(df = 5), xlim = c(-3.5, -2), geom = "area", alpha = .4) +
  stat_function(fun = dt, args = list(df = 5), xlim = c(2, 3.5), geom = "area", alpha = .4) +
  ggtitle(expression(H[a]: beta[j] != 0)) +
  theme(axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank())

p3 <- ggplot(data = data.frame(x = c(-3.5, 3.5)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = 5)) +
  stat_function(fun = dt, args = list(df = 5), xlim = c(2, 3.5), geom = "area", alpha = .4) +
  ggtitle(expression(H[a]: beta[j] > 0)) +
  theme(axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank())

grid.arrange(p1, p2, p3, ncol = 3)
```

## CIs for a single coefficient


$\widehat{\beta}_j \pm t^*_{n-(k+1)} \cdot SE(\widehat{\beta}_j )$


. . .

<br>

#### Wildfires example

::: {style="font-size: 0.8em;"}

```{r echo=FALSE, results='asis'}
coef_table <- tidy(quad_mod, conf.int = TRUE) |>
  select(-statistic, -p.value)


kable(coef_table, format = "pipe", digits = 0, booktabs = TRUE, col.names = c("Term", "Estimate", "SE", "Lower", "Upper"))
```

:::


## Your turn

Would a higher-order polynomial (e.g. cubic, quartic, quintic) provide a better fit to the wildfire data?

<br>

Work through that example on the handout with your neighbors


## A warning about this analysis


> Prior to 1983, sources of these figures are not known, or cannot be confirmed, and were not derived from the current situation reporting process. As a result the figures prior to 1983 should not be compared to later data.
>
>— NIFC

::: footer
https://www.nifc.gov/fireInfo/fireInfo_stats_totalFires.html]
:::


## A warning about polynomials


High-order polynomial regression models will over-fit your data (i.e. pick up on peculiarities specific to your one sample from the population)


```{r echo=FALSE, fig.height = 2.5, fig.width = 7.5, out.width=850}
poly2 <- makeFun(quad_mod)
p1 <- gf_point(Acres ~ Year, data = wildfires, color = "#003069") %>%
  gf_labs(x = "Year", y = "Acres burned") %>%
  gf_fun(poly2(x) ~ x, color = "#f15a31") %>%
  gf_labs(title = "2nd-order polynomial") +
  theme(axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank())

poly5_mod <- lm(Acres ~ poly(Year, 5), data = wildfires)
poly5 <- makeFun(poly5_mod)
p5 <- gf_point(Acres ~ Year, data = wildfires, color = "#003069") %>%
  gf_labs(x = "Year", y = "Acres burned") %>%
  gf_fun(poly5(x) ~ x, color = "#f15a31")  %>%
  gf_labs(title = "5th-order polynomial") +
  theme(axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank())

poly25_mod <- lm(Acres ~ poly(Year, 25), data = wildfires)
poly25 <- makeFun(poly25_mod)
p25 <- gf_point(Acres ~ Year, data = wildfires, color = "#003069") %>%
  gf_labs(x = "Year", y = "Acres burned") %>%
  gf_fun(poly25(x) ~ x, color = "#f15a31") %>%
  gf_labs(title = "25th-order polynomial") +
  theme(axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank())

grid.arrange(p1, p5, p25, ncol = 3)
```


## Multiple linear regression


Polynomial regression is one example of the multiple regression model, but there are numerous ways to incorporate multiple predictors into a model

$\mu \lbrace Y | X \rbrace = \beta_0 + \beta_1 x + \beta_2 x^2$

. . .

$\mu \lbrace Y | X \rbrace = \beta_0 + \beta_1 x_1 + \beta_2 x_2$

. . .


$\mu \lbrace Y | X \rbrace = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_2 x_1 x_2$


# Explained variance



## The "null" model

- Use the mean of Y as the prediction for all observations

- $\mu \lbrace Y | X \rbrace = \beta_0$

- Leaves a lot of variability unexplained

::: columns
:::{.column width="50%"}
```{r echo=FALSE, fig.height = 3, fig.width = 3.5, out.width="90%", fig.align='center'}
gf_point(Acres ~ Year, data = wildfires, ylab = "Acres burned", color = "#003069") %>%
  gf_hline(yintercept = mean(wildfires$Acres), color = "#f15a31")
```
:::
:::{.column width="50%"}
$SD(Y) = `r round(sd(wildfires[["Acres"]]), 2)`$ cm
:::
:::

## Polynomial model

- $\mu \lbrace Y | X \rbrace = \beta_0 + \beta_1x + \beta_2 x^2$

- Using a predictor explains more variability

<br>


::: columns
:::{.column width="50%"}
```{r echo=FALSE, fig.height = 3, fig.width = 3.5, out.width="90%", fig.align='center'}
gf_point(Acres ~ Year, data = wildfires, ylab = "Acres burned", color = "#003069") %>%
  gf_lm(formula = y ~ poly(x, 2), color = "#f15a31")
```
:::
:::{.column width="50%"}
$SD(Y) = `r round(sigma(quad_mod), 2)`$ cm
:::
:::



## SSTotal

::: columns
:::{.column width="50%"}
${\rm SSTotal} = \sum(Y_i - \bar{Y})^2$

Measures the overall variability in $Y$
:::

::: {.column width="50%"}
```{r echo=FALSE, fig.height = 3.25, fig.width = 3.75, fig.align='center'}
quad_aug <- augment(quad_mod)
quad_aug <- mutate(quad_aug, mean.y = mean(Acres))

gf_point(Acres ~ Year, data = quad_aug, ylab = "Acres burned") %>%
  gf_hline(yintercept = mean(quad_aug$Acres), color = "#f15a31") %>%
  gf_segment(mean.y + Acres ~ Year + Year, color = "gray70") 
```
:::
:::


## SSResidual (aka SSError)

::: columns
::: {.column width="50%"}
${\rm SSResidual} = \sum(Y_i - \widehat{Y}_i)^2$

Measures the variability unexplained by the model
:::

::: {.column width="50%"}
```{r echo=FALSE, fig.height = 3.25, fig.width = 3.75, fig.align='center'}

gf_point(Acres ~ Year, data = quad_aug, ylab = "Acres burned") %>%
  gf_lm(formula = y ~ poly(x, 2), color = "#f15a31") %>%
  gf_segment(.fitted + Acres ~ Year + Year, color = "gray70") 
```
:::
:::


## SSRegression

Measures the variability explained by the model

<br>

::: columns
:::{.column width="30%"}
```{r}
#| fig-width: 1.75
#| fig-height: 3
#| echo: FALSE
#| out-width: 100%
quad_anova <- anova(quad_mod)

ssdf <- tribble(
  ~Source, ~SS,
  "Error",      sigma(quad_mod)^2 * quad_anova$Df[3],
  "Total",       sum(quad_anova$`Sum Sq`),
  "Regression", sum(quad_anova$`Sum Sq`[1:2])
)

sst_plot <- ggplot(data = ssdf |> filter(Source == "Total"), aes(x = "", y = SS)) +
  geom_bar(stat="identity", width=1) +
  theme_void() +
  labs(title = "SSTotal")


decomp_plot <- ggplot(data = ssdf |> filter(Source != "Total"), aes(x = "", y = SS, fill = Source)) +
  geom_bar(stat="identity", width=1) +
  geom_text(aes(label = paste0(Source)), position = position_stack(vjust = 0.5), color = "white") +
  theme_void() +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("#f15a31", "#003069"))

sst_plot + decomp_plot
```
:::
:::{.column width="70%"}
<br>

<br>

<br>

${\rm SSRegression} = SSTotal - SSError$
:::
:::

## Coefficient of Determination: R<sup>2</sup>

Proportion of the total variation in $y$ explained by the linear regression model

::: columns
:::{.column width="30%"}
```{r}
#| fig-width: 1.75
#| fig-height: 3
#| echo: FALSE
#| out-width: 100%
sst_plot + decomp_plot
```
:::
:::{.column width="70%"}
<br>

<br>



$\begin{split}
R^2 &= `r round(glance(quad_mod)[["r.squared"]], 3)`\\
&= \dfrac{SSRegr}{SST} \\
&= 1 - \dfrac{SSE}{SST} 
\end{split}$
:::
:::


## ⚠️ Caution {.center}

$R^2$ only addresses how close the fitted values are to the data, on average. It says nothing about the validity of the model.


# Comparing models


## Wildfires example

Suppose we wish to compare a quadratic and quartic model for the wildfires data set:

- Quadratic: $\mu \lbrace Y | X \rbrace = \beta_0 + \beta_1 x + \beta_2 x^2$
- Quartic: $\mu \lbrace Y | X \rbrace = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4$

How do we decide which model is preferred?

```{r}
#| echo: FALSE
full <- lm(Acres ~ poly(Year, 4), data = wildfires)
reduced <- lm(Acres ~ poly(Year, 2), data = wildfires)
```





## Comparing models

Can we compare $R^2$ values?

- No! More complex models will always have a higher $R^2$ value, even if the additional predictors are not useful.


Can we run individual t-tests?

- No! The tests are not necessarily independent, and the Type I error rate will be inflated.

## Comparing models with an F-test

:::: {.columns align=center}
::: {.column width="25%"}
Full model
:::

::: {.column width="70%"}
$\mu \lbrace Y | X \rbrace = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4$
:::

::: {.column width="25%"}
Reduced model
:::

::: {.column width="70%"}
$\mu \lbrace Y | X \rbrace = \beta_0 + \beta_1 x + \beta_2 x^2$
:::

::::

. . .

<br>


:::: {.columns align=center}
::: {.column width="25%"}
Hypotheses
:::

::: {.column width="70%"}
$H_0:\ \beta_{3} = \beta_4 = 0$

$H_a:$ at least one $\beta_j \ne 0$, $j=3,4$
:::

::::

## Comparing models with an F-test



Test statistic


$$
\begin{split}
F &= \frac{(R^2_{\text{full}} - R^2_{\text{reduced}}) / d}{(1 - R^2_{\text{full}}) / df_{\text{full}}}\\
&= \frac{(\text{SSR}_{\text{full}} - \text{SSR}_{\text{reduced}}) / d}{\text{MSE}_{\text{full}}}\\
&= \frac{(\text{SSE}_{\text{reduced}} - \text{SSE}_{\text{full}}) / d}{\text{MSE}_{\text{full}}}
\end{split}
$$

where 

- $d = \text{df}_{full} - \text{df}_{reduced}$ = \# betas being tested
- $\text{df}_{i} = n - (p + 1)=$ error d.f. for model $i$
- $\text{MSE}_{\text{full}} = \dfrac{\text{SSE}_{\text{full}}}{\text{df}_{\text{full}}}$



## F distribution


The F-statistics follows an $F$ distribution with $\text{df}_{full} - \text{df}_{reduced}$ and $n-p-1$ d.f.

```{r fig.height=3, fig.width=5, echo=FALSE, message=FALSE, fig.align='center'}
par(mar = c(2, .1, .1, .1))
plot(c(0, 5), c(0, df(.01, 1, 1)), type = "n", axes = FALSE)
at <- seq(0, 5, 1)
axis(1, at)
# axis(1, at, rep("", length(at)), tcl = -0.1)
X <- seq(0, 5, 0.01)
DF1 <- c(1, 2, 5, 10, 100)
DF2 <- c(1, 1, 2, 1, 100)
COL. <- RColorBrewer::brewer.pal(n = 7, name = "Blues")[-1]
for (i in 1:5) {
  Y <- df(X, as.numeric(DF1[i]), as.numeric(DF2[i]))
  lines(X, Y, col = COL.[i])
}
legend(2.5, 3.25,
       legend = c(paste('df1 = ', DF1[1:5], ', df2 = ', DF2[1:5], sep = '')),
       col = COL.,
       text.col = COL.,
       lty = rep(1, 5))
```

```{r include=FALSE}
dev.off
```

## Upper-tail p-values

```{r fig.height=3, fig.width=5, echo=FALSE, message=FALSE, fig.align='center'}
 ggplot(data = data.frame(x = c(.00001, 5.5)), aes(x)) +
  stat_function(fun = df, args = list(df1=4, df2=23), color = "navy", size = 1) +
  stat_function(fun = df, args = list(df1=4, df2=23), xlim = c(3, 5.5),
                  geom = "area", fill = "navy", alpha = .4) +
  labs(x = "F", y = "Density") +
  scale_y_continuous(limits = c(0, 0.8), expand = c(0,0)) +
   geom_curve(aes(x = 5, y = .22, xend = 4.25, yend = .03), arrow = arrow(length = unit(0.03, "npc")), curvature = .2, color = "navy") +
  annotate("text", x = 5.26, y = .25, label = "p-value", color = "navy", size = 6) +
  theme_classic()
```


To obtain upper-tail areas: `1 - pf(stat, df1, df2)`


## Putting it all together



```{r}
full_table <- glance(full) |> 
  select(r.squared, df, df.residual, nobs) |>
  mutate(model = "Full")

reduced_table <- glance(reduced) |> 
  select(r.squared, df, df.residual, nobs) |>
  mutate(model = "Reduced")

bind_rows(full_table, reduced_table) |>
  select(model, everything()) |>
  kable(digits = 3)
```

. . .

$$
F = \frac{(R^2_{\text{full}} - R^2_{\text{reduced}}) / d}{(1 - R^2_{\text{full}}) / df_{\text{full}}} = \frac{(0.453 - 0.421) / 2}{(1 - 0.453) / 54} \approx 1.58
$$

. . .

```{r}
#| echo: true
1 - pf(1.58, 2, 54)
```

. . .

There is no evidence that the quartic model is an improvement over the quadratic model ($F=4.467$, $df = 2, 54$, $p = 0.215$).


## Reading R output

```
Call:
lm(formula = Acres ~ poly(Year, 4), data = wildfires)

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)     4641410     245955  18.871  < 2e-16 ***
poly(Year, 4)1  9025270    1889218   4.777 1.40e-05 ***
poly(Year, 4)2  8177004    1889218   4.328 6.55e-05 ***
poly(Year, 4)3 -1194695    1889218  -0.632   0.5298    
poly(Year, 4)4 -3177711    1889218  -1.682   0.0983 .  

Residual standard error: 1889000 on 54 degrees of freedom
Multiple R-squared:  0.4534,	Adjusted R-squared:  0.4129 
F-statistic:  11.2 on 4 and 54 DF,  p-value: 1.096e-06
```


::: footer
Output was edited to fit on one slide
:::


## Extra sums of squares F-test in R

```{r}
#| echo: true
full <- lm(Acres ~ poly(Year, 4), data = wildfires)    #<1>
reduced <- lm(Acres ~ poly(Year, 2), data = wildfires) #<2>
anova(reduced, full) #<3>
```

1. Fit the full model
2. Fit the reduced model
3. Use `anova()` to compare the two models. The first argument should be the reduced model.
