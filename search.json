[
  {
    "objectID": "activity/01-slr-review.html",
    "href": "activity/01-slr-review.html",
    "title": "Simple linear regression review ‚Äì Stat 230",
    "section": "",
    "text": "Researchers examined the relationship between sleep duration and simple reaction time performance in healthy young adults in a controlled laboratory setting. Sixty-five college-aged participants (ages 18-25) were recruited and randomly assigned to different sleep conditions ranging from 3 to 9 hours of sleep per night over a one-week period. Each morning, participants completed a computerized simple reaction time task where they responded to visual stimuli as quickly as possible, with reaction times measured in milliseconds. The researchers fit a simple linear regression model to their data and found a slope of -18.7 and a y-intercept of 520.1.\n\nReport the equation of the fitted regression equation.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(\\hat{y} = -18.7x + 520.1\\)\n\n\n\n\nInterpret the value of the slope in context. Be sure to talk about association or expectation.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nA one hour increase in sleep duration is associated with a decrease in reaction time of 18.7 milliseconds.\n\n\n\n\nInterpret the value of the y-intercept in context.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe y-intercept of 520.1 milliseconds represents the expected reaction time for a young adult who has slept 0 hours per night over the past week. We should be cautious about interpreting this value since it is outside the range of the data.\n\n\n\n\nUse the fitted model to predict the reaction time of a young adult who has slept 5 hours per night over the past week.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(\\hat{y} = -18.7(5) + 520.1 = 426.6\\) milliseconds\n\n\n\n\nCan you use the model to predict the reaction time of a young adult who has slept 10 hours per night over the past week? Briefly justify your answer.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nNo, we should not use the model to predict the reaction time for a young adult who has slept 10 hours per night because this value is outside the range of the data used to fit the model (3 to 9 hours). Predictions made outside the range of the data are considered extrapolations.\n\n\n\n\nWhy did researchers randomly assign participants to a sleep condition?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nRandom assignment helps to ensure that any confounding variables are evenly distributed across the different sleep conditions. This allows the researchers to make a causal conclusion about the effect of sleep duration on reaction time.\n\n\n\n\nCan you generalize these results to all young adults in the U.S.? Why or why not?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nNo, we cannot generalize these results to all young adults in the U.S. because it may not be representative of all young adults in the U.S. It we truly wanted to generalize to the entire population, then we should draw a random sample from the population of interest."
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html",
    "href": "activity/09H-mlr-linear-combos.html",
    "title": "Including Categorical Predictors",
    "section": "",
    "text": "In this activity you will learn how to include build multiple regression models in R, including models with categorical predictors. The cars data set loaded below contains information on used cars for sale, including their Price, Mileage, and Make (a categorical variable with six levels).\n\ncars &lt;- read.csv(\"https://aloy.github.io/stat230-materials/data/Cars.csv\", stringsAsFactors = TRUE)"
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#overview",
    "href": "activity/09H-mlr-linear-combos.html#overview",
    "title": "Including Categorical Predictors",
    "section": "",
    "text": "In this activity you will learn how to include build multiple regression models in R, including models with categorical predictors. The cars data set loaded below contains information on used cars for sale, including their Price, Mileage, and Make (a categorical variable with six levels).\n\ncars &lt;- read.csv(\"https://aloy.github.io/stat230-materials/data/Cars.csv\", stringsAsFactors = TRUE)"
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#thinking-about-the-data",
    "href": "activity/09H-mlr-linear-combos.html#thinking-about-the-data",
    "title": "Including Categorical Predictors",
    "section": "Thinking about the data",
    "text": "Thinking about the data\nTo begin, create a scatterplot of Price versus Mileage.\n\n# put your code here\n\nQ1. What do you notice about the relationship between these two variables? Is a transformation necessary?\nQ2. The Make variable is categorical with six levels: Buick, Cadillac, Chevrolet, Pontiac, SAAB, and Saturn. To include this variable in a regression model, we need use indicator (i.e., dummy) variables. How many do we need to make?"
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#fitting-an-mlr-model",
    "href": "activity/09H-mlr-linear-combos.html#fitting-an-mlr-model",
    "title": "Including Categorical Predictors",
    "section": "Fitting an MLR model",
    "text": "Fitting an MLR model\nBuild (fit) a multiple regression model using log(Price) as the response with Mileage and Make as the predictor variables. To do this, you use + to separate the predictor variable on the right side of the ~ in the model formula. R will automatically convert a categorical explanatory variable into a set of indicator variables.\n\n# Fill in the blanks to fit the MLR model\ncar_lm &lt;- lm(___ ~ ___ + ___, data = ___)\nsummary(car_lm)\n\nQ3. Report the fitted regression equation and the \\(R^2\\) value.\nQ4.Which level of Make is the baseline? The baseline level is represented by 0‚Äôs across all of the indicator variables and won‚Äôt have a coefficient in the output.\nQ5.What strategy did R use to create the indicator variables for Make? In other words, how did R decide which level of Make to use as the baseline?\nQ6. Which of the following models best describes the one just fit: parallel lines, different slopes, or separate lines?"
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#plotting-the-fitted-model",
    "href": "activity/09H-mlr-linear-combos.html#plotting-the-fitted-model",
    "title": "Including Categorical Predictors",
    "section": "Plotting the fitted model",
    "text": "Plotting the fitted model\nTo plot a fitted regression model, we can use the ggpredict() and plot() functions in the {ggeffects} package. The ggpredict() function creates a data frame of predicted values from the model for each Make across a range of Mileage values. The plot() function then creates a plot of these predicted values.\n\n# Be sure to load ggeffects!\nlibrary(ggeffects)\n\n# First, make predictions from the model\n# Fill in the blank with your model name\ncars_pred &lt;- ggpredict(___, terms = ~Mileage + Make)\n\n# Now plot the predictions\nplot(cars_pred)\n\nQ7. Does this plot confirm your answer to the previous question?\n\n\n\n\n\n\nNoteTips on plotting the model\n\n\n\n\nHolding other variables at ‚Äútypical‚Äù values\nIf you have more predictor variables in your model, then variables not specified in the terms argument the ggpredict() function are set to a specific value. Quantitative variables are set to their mean. Categorical variables are set to the mode (the level with the most observations).\n\n\nLabels\nIf you need to adjust your axis labels or title, then you will need to add a labs() layer to your plot. For example,\n\nplot(cars_pred) +\n  labs(x = \"My new x label\", y = \"My new y label\",\n       title = \"My new title\")"
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#holding-other-variables-at-typical-values",
    "href": "activity/09H-mlr-linear-combos.html#holding-other-variables-at-typical-values",
    "title": "Including Categorical Predictors",
    "section": "Holding other variables at ‚Äútypical‚Äù values",
    "text": "Holding other variables at ‚Äútypical‚Äù values\nIf you have more predictor variables in your model, then variables not specified in the terms argument the ggpredict() function are set to a specific value. Quantitative variables are set to their mean. Categorical variables are set to the mode (the level with the most observations)."
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#labels",
    "href": "activity/09H-mlr-linear-combos.html#labels",
    "title": "Including Categorical Predictors",
    "section": "Labels",
    "text": "Labels\nIf you need to adjust your axis labels or title, then you will need to add a labs() layer to your plot. For example,\n\nplot(cars_pred) +\n  labs(x = \"My new x label\", y = \"My new y label\",\n       title = \"My new title\")"
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#fitting-a-model-with-interactions",
    "href": "activity/09H-mlr-linear-combos.html#fitting-a-model-with-interactions",
    "title": "Including Categorical Predictors",
    "section": "Fitting a model with interactions",
    "text": "Fitting a model with interactions\nIf we believe that the association between Price and Mileage differs by Make, then we can include an interaction term in our model. To do this, we use * instead of + to separate the predictor variables on the right side of the ~ in the model formula. This will include both main effects and the interaction term.\n\n# Fill in the blanks to fit the MLR model with interaction\ncar_lm2 &lt;- lm(___ ~ ___ * ___, data = ___)\nsummary(car_lm2)\n\nQ8. Report the fitted regression equation for Buicks and Cadillacs.\nQ9. What is the \\(R^2\\) value for this model? Does this model appear to fit the data better than the previous model?"
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#plotting-the-fitted-model-with-interactions",
    "href": "activity/09H-mlr-linear-combos.html#plotting-the-fitted-model-with-interactions",
    "title": "Including Categorical Predictors",
    "section": "Plotting the fitted model with interactions",
    "text": "Plotting the fitted model with interactions\nTo plot the interaction model, we again use the ggpredict() and plot() functions in the {ggeffects} package.\n\n# Fill in the blank with your model name\ncars_pred2 &lt;- ggpredict(___, terms = ~Mileage + Make)\n\n# Now plot the predictions\nplot(cars_pred2)\n\nüõë Stop here. We will have a class discussion before moving on so that the next section makes sense."
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#calculating-cis-for-linear-combinations",
    "href": "activity/09H-mlr-linear-combos.html#calculating-cis-for-linear-combinations",
    "title": "Including Categorical Predictors",
    "section": "Calculating CIs for linear combinations",
    "text": "Calculating CIs for linear combinations\nTo calculate a confidence interval for a linear combination of coefficients, we must first calculate the estimate and standard error of the linear combination.\nLet‚Äôs calculate a 95% confidence interval for the slope of the Cadillac model. Refer to your notes/slides and determine the formulas for the estimate and standard error of this slope.\nNow that you know the formulas, you‚Äôll use R to implement them. First, extract the coefficients and covariance matrix from the model object. Try printing the results to see what they look like and make note of where the Cadillac coefficients are located.\n\n1car_coefs &lt;- coef(car_lm2)\n2car_vcov &lt;- vcov(car_lm2)\n\n\n1\n\nExtract the coefficients from the model object and store them in car_coefs.\n\n2\n\nExtract the covariance matrix from the model object and store it in car_vcov.\n\n\n\n\nNext, calculate the estimate of the slope for Cadillacs.\n\n3estimate &lt;- car_coefs[2] + car_coefs[8]\nestimate\n\n\n3\n\ncar_coefs is a vector, so we pull off the necessary coefficients using their position in square brackets. Here, car_coefs[2] pulls off the coefficient for Mileage and car_coefs[8] pulls off the coefficient for the interaction term Mileage:MakeCadillac.\n\n\n\n\nNow, calculate the standard error of the slope for Cadillacs.\n\n4se &lt;- sqrt(car_vcov[2,2] + car_vcov[8,8] + 2*car_vcov[2,8])\nse\n\n\n4\n\ncar_vcov is a matrix, so we pull off the necessary variances and covariances using their row and column positions in square brackets. Here, car_vcov[2,2] pulls off the variance for Mileage, car_vcov[8,8] pulls off the variance for the interaction term Mileage:MakeCadillac, and car_vcov[2,8] pulls off the covariance between these two coefficients.\n\n\n\n\nFinally, use the estimate and standard error to calculate a 95% confidence interval for the slope of the Cadillac model. You can use the qt() function to find the appropriate critical value.\nQ10. Calculate a 95% confidence interval for the slope of the Cadillac model.\n\n# Calculate the 95% CI here"
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#calculating-extra-sums-of-squares-f-tests",
    "href": "activity/09H-mlr-linear-combos.html#calculating-extra-sums-of-squares-f-tests",
    "title": "Including Categorical Predictors",
    "section": "Calculating extra sums of squares F-tests",
    "text": "Calculating extra sums of squares F-tests\nDo we really need the interaction terms? To answer this question, we can use an extra sums of squares F-test to compare the model with interaction terms to the model without interaction terms.\nWe have already seen how to use the anova() function to compare two nested models. Here, we will use it again to compare our two car models.\n\n# Fill in the blanks to run the extra sums of squares F-test\nanova(___, ___)\n\n\nUsing only the full model\nWe can also calculate the extra sums of squares F-test using only the full model. To do this, we need to extract the SSE and degrees of freedom for both models from the ANOVA table for the full model.\nQ11. Run the below code and verify that the df, sums of squares, mean squares, F value, and p-value match what you got from the previous anova() command.\n\nanova(car_lm2)"
  },
  {
    "objectID": "activity/09H-mlr-linear-combos.html#function-quick-reference",
    "href": "activity/09H-mlr-linear-combos.html#function-quick-reference",
    "title": "Including Categorical Predictors",
    "section": "Function quick reference",
    "text": "Function quick reference\nThe following table summarizes the functions we learned today:\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nlm()\nFit a linear model\n\n\nsummary()\nDisplay detailed results from a fitted model\n\n\ncoef()\nExtract model coefficients\n\n\nvcov()\nExtract the variance-covariance matrix of model coefficients\n\n\nggpredict()\nCreate a data frame of predicted values from a fitted model\n\n\nplot()\nCreate a plot of predicted values from a fitted model\n\n\nanova()\nCreate an ANOVA table for a fitted model or compare two nested models"
  },
  {
    "objectID": "activity/10R-mlr-diagnostics.html",
    "href": "activity/10R-mlr-diagnostics.html",
    "title": "Regression Diagnostics",
    "section": "",
    "text": "In this activity you will learn how to calculate diagnostic measures and create diagnostic plots for multiple linear regression models in R."
  },
  {
    "objectID": "activity/10R-mlr-diagnostics.html#overview",
    "href": "activity/10R-mlr-diagnostics.html#overview",
    "title": "Regression Diagnostics",
    "section": "",
    "text": "In this activity you will learn how to calculate diagnostic measures and create diagnostic plots for multiple linear regression models in R."
  },
  {
    "objectID": "activity/10R-mlr-diagnostics.html#model-overview",
    "href": "activity/10R-mlr-diagnostics.html#model-overview",
    "title": "Regression Diagnostics",
    "section": "Model overview",
    "text": "Model overview\nA hospital surgical unit was interested in predicting the survival in patients undergoing a particular type of liver operation. A random selection of 108 patients was selected for analysis. For each patient, the following information was extracted from the pre-operation evaluation and used by the research team to predict survival time (in days):\n\nblood_clot_score: a score based on blood clotting tests\nprognostic_index: a prognostic index based on various health factors\nenzyme_test: a score based on liver enzyme tests\nalchohol_use_heavy: an indicator variable for heavy alcohol use (1 = heavy use, 0 = not heavy use)\n\nThe research team determined that the following multiple regression model is a reasonable starting point:\n\\[\n\\begin{split}\n\\text{survival time} = \\beta_0 &+ \\beta_1(\\text{blood clot score}) + \\beta_2(\\text{prognostic index}) + \\beta_3(\\text{enzyme test})\\\\\n&+ \\beta_4(\\text{heavy alcohol use}) + \\epsilon\n\\end{split}\n\\]\nThis model is fit using the below code:"
  },
  {
    "objectID": "activity/10R-mlr-diagnostics.html#residual-analysis-in-r",
    "href": "activity/10R-mlr-diagnostics.html#residual-analysis-in-r",
    "title": "Regression Diagnostics",
    "section": "Residual analysis in R",
    "text": "Residual analysis in R\nWhen we have multiple predictors, we can still use residual plots to assess the assumptions of the linear regression model. You can still use the resid_panel(model, type = \"standardized\") function in the ggResidpanel package to create a panel of residual plots that includes a plot of the residuals vs.¬†fitted values, a histogram of the residuals, and a Q-Q plot of the residuals.\n\n\n\n\n\n\n\n\nTo get plots of the standardized residuals against the individual predictors, I recommend using the residualPlots() command in the car package. Below is the code to create these plots for the surgical data example.\n\n\n\n\n\n\n\n\nKey arguments to the residualPlots() function include:\n\nmodel: the linear model object created using the lm() function\ntype = \"rstandard\": specifies that we want to plot the standardized residuals\ntests = FALSE: removes the automatic addition of significance tests for non-linearity\nquadratic = FALSE: removes a quadratic smoother on the residual plot\nlayout = c(2, 3): specifies the layout of the plots (2 rows and 3 columns), which is useful for organizing multiple plots in a single view\n\nQuestion: Based on the residual plots, do you think the current multiple linear regression model is appropriate for this data? Why or why not?\nQuestion: Regardless of your answer, apply a log transformation to the response variable and refit the model using the code below. Then, create the same residual plots for the new model. Do you think the log transformation improved the model fit? Why or why not?"
  },
  {
    "objectID": "activity/10R-mlr-diagnostics.html#influence-analysis",
    "href": "activity/10R-mlr-diagnostics.html#influence-analysis",
    "title": "Regression Diagnostics",
    "section": "Influence analysis",
    "text": "Influence analysis\nLet‚Äôs continue working with the log-transformed model. We can use the influenceIndexPlot() function in the car package to create influence plots for the log-transformed model. Each plot will display one of the diagnostic measures and plot it against the row number (index). Below is the code to create these plots for the surgical data example.\n\n\n\n\n\n\n\n\nKey arguments to the influenceIndexPlot() function include:\n\nmodel: the linear model object created using the lm() function\nvars = c(\"hat\", \"student\", \"Cook\", ): specifies which influence measures to plot. In this case, we are plotting Cook‚Äôs distance, leverage (hat values), and studentized (standardized) residuals\n\nQuestion: What observations (row numbers) are identified as possibly influential in each of the three plots?\nQuestion: Do you think any of these observations are truly influential? Why or why not? It may be useful to look back through your notes to review the definitions of these influence measures and the suggested cutoffs."
  },
  {
    "objectID": "activity/10R-mlr-diagnostics.html#alternative-r-implementation-for-influence-measures",
    "href": "activity/10R-mlr-diagnostics.html#alternative-r-implementation-for-influence-measures",
    "title": "Regression Diagnostics",
    "section": "Alternative R implementation for influence measures",
    "text": "Alternative R implementation for influence measures\nWhile the use of the influenceIndexPlot() function is a quick way to visualize influence measures, it does not provide the actual values of these measures. To obtain the actual values, we can use the augment() function in the broom R package. Below is the code to obtain these measures for the log-transformed model.\n\n\n\n\n\n\n\n\nThe augment() function adds several columns to the original data frame, including:\n\n.fitted: the fitted values from the model\n.resid: the residuals from the model\n.std.resid: the standardized residuals from the model\n.hat: the leverage (hat values) from the model\n.cooksd: Cook‚Äôs distance from the model"
  },
  {
    "objectID": "activity/10R-mlr-diagnostics.html#function-quick-reference",
    "href": "activity/10R-mlr-diagnostics.html#function-quick-reference",
    "title": "Regression Diagnostics",
    "section": "Function quick reference",
    "text": "Function quick reference\n\n\n\n\n\n\n\n\nFunction\nPackage\nDescription\n\n\n\n\nresid_panel()\nggResidpanel\nCreates a panel of residual plots for a linear model\n\n\nresidualPlots()\ncar\nCreates residual plots for each predictor in a linear model\n\n\ninfluenceIndexPlot()\ncar\nCreates influence plots for a linear model\n\n\naugment()\nbroom\nAdds columns to a data frame with information about a model fit, including residuals and influence measures"
  },
  {
    "objectID": "activity/07-sol-mlr-polynomial.html",
    "href": "activity/07-sol-mlr-polynomial.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "To load the wildfires data set, run the following code chunk:\n\nwildfires &lt;- read.csv(\"https://aloy.github.io/stat230-materials/data/wildfires.csv\")"
  },
  {
    "objectID": "activity/07-sol-mlr-polynomial.html#loading-data",
    "href": "activity/07-sol-mlr-polynomial.html#loading-data",
    "title": "Polynomial Regression",
    "section": "",
    "text": "To load the wildfires data set, run the following code chunk:\n\nwildfires &lt;- read.csv(\"https://aloy.github.io/stat230-materials/data/wildfires.csv\")"
  },
  {
    "objectID": "activity/07-sol-mlr-polynomial.html#fitting-a-polynomial-regression-model",
    "href": "activity/07-sol-mlr-polynomial.html#fitting-a-polynomial-regression-model",
    "title": "Polynomial Regression",
    "section": "Fitting a polynomial regression model",
    "text": "Fitting a polynomial regression model\nTo fit a polynomial regression model we still use the lm() command, but we expand our formula to include polynomial terms. To include polynomial terms in a regression model, we need to use the I() function to indicate that we want to calculate a polynomial term. For example, to fit the quadratic model we have already discussed in class, we use the following code:\n\nquadratic_lm &lt;- lm(Acres ~ Year + I(Year^2), data = wildfires)\n\nOnce you have your fitted model, we can explore it like we have with simple linear regression models."
  },
  {
    "objectID": "activity/07-sol-mlr-polynomial.html#exploring-a-cubic-model",
    "href": "activity/07-sol-mlr-polynomial.html#exploring-a-cubic-model",
    "title": "Polynomial Regression",
    "section": "Exploring a cubic model",
    "text": "Exploring a cubic model\nLet‚Äôs fit a cubic model to the wildfires data set. The cubic model has the form \\[\\mu \\lbrace y | x \\rbrace = \\beta_0 + \\beta_1 x + \\beta_2x^2 + \\beta_3x^3.\\] Use the lm() command to fit the cubic model where Year is used to predict Acres.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\ncubic_lm &lt;- lm(Acres ~ Year + I(Year^2) + I(Year^3), data = wildfires)\n\n\n\n\nTo plot the fitted cubic model, you can use the gf_point() and gf_lm() functions from the ggformula package. The following code will create a scatter plot of the data and add the fitted cubic regression line:\n\ngf_point(Acres ~ Year, data = wildfires, xlab = \"Year\", ylab = \"Acres burned\") |&gt;\n  gf_lm(formula = y ~ poly(x, 3), linewidth = .5)\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the gf_lm() layer we use the poly(x, 3) function to specify that we want to fit a cubic polynomial. You can use poly() to fit polynomials of any degree by changing the second argument, and you can also use this function within the lm() function to fit polynomial regression models if you‚Äôd like.\n\n\nDoes the cubic model appear to be necessary? Use the summary() function to explore the fitted model and run a hypothesis test for the cubic term. What do you conclude?\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe t-test for \\(H_0: \\beta_3 = 0\\) vs.¬†\\(H_a: \\beta_3 \\neq 0\\) shows that this term is not statistically significant, so we do not have sufficient evidence to conclude that the cubic term is necessary in our model.\n\nsummary(cubic_lm)\n\n\nCall:\nlm(formula = Acres ~ Year + I(Year^2) + I(Year^3), data = wildfires)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-4042634 -1462388  -220730  1720630  3904566 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  3.320e+11  5.078e+11   0.654    0.516\nYear        -4.928e+08  7.660e+08  -0.643    0.523\nI(Year^2)    2.437e+05  3.851e+05   0.633    0.530\nI(Year^3)   -4.015e+01  6.454e+01  -0.622    0.536\n\nResidual standard error: 1920000 on 55 degrees of freedom\nMultiple R-squared:  0.4247,    Adjusted R-squared:  0.3933 \nF-statistic: 13.54 on 3 and 55 DF,  p-value: 9.971e-07\n\n\n\n\n\nWhat degrees of freedom did R for the t-distribution used to calculate the p-value for the test of the cubic term?\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe test uses df = n - (3 + 1) = 55.\n\n\n\nDo you notice anything curious about the inferential results for the linear and quadratic terms?\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nYes! All of the polynomial terms are not statistically significant, even though the linear and quadratic terms were in the previous model.\n\n\n\nThe issue here is that the polynomial terms for year are highly correlated with each other (i.e., year, year\\(^2\\), and year\\(^3\\) are correlated). This can lead to numerical instability and make it difficult to interpret the coefficients. This is a situation called multicollinearity. We‚Äôll talk more about this later. One way to remedy this issue in polynomial regression is to use orthogonal polynomials, which are uncorrelated with each other."
  },
  {
    "objectID": "activity/07-sol-mlr-polynomial.html#an-alternative-way-to-fit-polynomials",
    "href": "activity/07-sol-mlr-polynomial.html#an-alternative-way-to-fit-polynomials",
    "title": "Polynomial Regression",
    "section": "An alternative way to fit polynomials",
    "text": "An alternative way to fit polynomials\nTo fit polynomial model with uncorrelated polynomial terms use the poly() function. For example, to fit a cubic model using orthogonal polynomials, we can run the following code:\n\ncubic_lm_ortho &lt;- lm(Acres ~ poly(Year, 3), data = wildfires)\n\nUse the summary() function to explore the fitted model. What do you notice about the inferential results for the linear, quadratic, and cubic terms? How does this compare to the previous cubic model we fit? How does it compare to the quadratic model?\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nUsing an orthogonal polynomial via poly() results in the linear and quadratic terms being statistically significant, which is in line with what we saw in the quadratic model. The cubic term is not statistically significant, so we have no evidence that it is needed.\n\ncubic_lm_ortho &lt;- lm(Acres ~ poly(Year, 3), data = wildfires)\nsummary(cubic_lm_ortho)\n\n\nCall:\nlm(formula = Acres ~ poly(Year, 3), data = wildfires)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-4042634 -1462388  -220730  1720630  3904566 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     4641410     250012  18.565  &lt; 2e-16 ***\npoly(Year, 3)1  9025270    1920377   4.700 1.79e-05 ***\npoly(Year, 3)2  8177004    1920377   4.258 8.11e-05 ***\npoly(Year, 3)3 -1194695    1920377  -0.622    0.536    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1920000 on 55 degrees of freedom\nMultiple R-squared:  0.4247,    Adjusted R-squared:  0.3933 \nF-statistic: 13.54 on 3 and 55 DF,  p-value: 9.971e-07\n\n\n\n\n\n\nThe method of constructing the polynomial terms in our regression model does not change our predictions, but it can change the inferential results for the polynomial terms."
  },
  {
    "objectID": "activity/07-sol-mlr-polynomial.html#function-quick-reference",
    "href": "activity/07-sol-mlr-polynomial.html#function-quick-reference",
    "title": "Polynomial Regression",
    "section": "Function quick reference",
    "text": "Function quick reference\nThe following table summarizes the functions we learned today:\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nlm(formula, data)\nFit a linear model. For polynomial regression the formula should include polynomial terms or use poly().\n\n\nI()\nUsed to create polynomial terms in a regression model\n\n\npoly(x, degree)\nCreate orthogonal polynomial terms"
  },
  {
    "objectID": "activity/08H-sol-mlr-intro.html",
    "href": "activity/08H-sol-mlr-intro.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "Abalone are a type of mollusk that are harvested for their meat. Knowing the age of abalone can help set sustainable fishing quotas; however, the process is time consuming and labor intensive. Researchers collected various physical measurements, such as length, diameter, height, and weight, that are easy to collect to develop a method to predict abalone age. Below is R output for a multiple linear regression model predicting rings (a proxy for age) that uses length (mm), diameter (mm), height (mm), and whole_weight (in grams) as predictor variables.\nCall:\nlm(formula = rings ~ length + diameter + height + whole_weight, \n    data = abalone)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.90861    0.30922   9.406  &lt; 2e-16\nlength       -12.04857    2.10217  -5.731 1.07e-08\ndiameter      25.64381    2.57384   9.963  &lt; 2e-16\nheight        20.24276    1.78180  11.361  &lt; 2e-16\nwhole_weight   0.06589    0.22557   0.292     0.77    \n---\n\nResidual standard error: 2.59 on 4172 degrees of freedom\nMultiple R-squared:  0.3556,    Adjusted R-squared:  0.3549 \nF-statistic: 575.4 on 4 and 4172 DF,  p-value: &lt; 2.2e-16\n\nReport the estimated regression equation.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe estimated regression equation is: \\[\\widehat{rings} = 2.90861 - 12.04857(\\mathtt{length}) + 25.64381(\\mathtt{diameter}) + 20.24276(\\mathtt{height}) + 0.06589(\\mathtt{whole\\_weight})\\]\n\n\n\n\nUsing the model, predict the number of rings for an abalone with length 0.5 mm, diameter 0.4 mm, height 0.1 mm, and whole weight of 1 gram.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo make the prediction, we substitute the values into the regression equation: \\[\n\\widehat{rings} = 2.90861 - 12.04857(0.5) + 25.64381(0.4) + 20.24276(0.1) + 0.06589(1) = 9.23 \\text{ rings}\n\\]\n\n\n\n\nInterpret the slope coefficient for length in context.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe slope coefficient for length is -12.04857. This means that, holding all other predictor variables constant, for each additional millimeter increase in length, we expect the number of rings to decrease by approximately 12.05.\n\n\n\n\nState the hypotheses for the test of the slope coefficient for whole_weight. What do you conclude based on the results of this test?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe hypotheses for the test of the slope coefficient for whole_weight are:\n\nNull hypothesis (\\(H_0\\)): \\(\\beta_{\\text{whole\\_weight}} = 0\\) (there is no association between whole weight and rings, controlling for other variables)\n\nAlternative hypothesis (\\(H_A\\)): \\(\\beta_{\\text{whole\\_weight}} \\neq 0\\) (there is an association between whole weight and rings, controlling for other variables)\n\nThe p-value for the test is 0.77, which is very large. Therefore, we fail to reject the null hypothesis. There is no evidence to suggest that whole weight is associated with the number of rings, after accounting for length, diameter, and height.\n\n\n\n\nCalculate a 95% confidence interval for the slope coefficient for length. Interpret this interval in context.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo calculate the 95% confidence interval for the slope coefficient for length, we use the formula:\n\\[\\text{CI} = \\hat{\\beta} \\pm t^* \\cdot SE(\\hat{\\beta})\\]\nwhere \\(\\hat{\\beta}\\) is the estimated coefficient, \\(SE(\\hat{\\beta})\\) is the standard error, and \\(t^*\\) is the critical value from the t-distribution with 4172 degrees of freedom.\nUsing the output, we have:\n\n\\(\\hat{\\beta}_{\\text{length}} = -12.04857\\)\n\\(SE(\\hat{\\beta}_{\\text{length}}) = 2.10217\\)\nFor a 95% confidence level and 4172 degrees of freedom, \\(t^* \\approx 1.96\\) (using normal approximation for large df)\n\nThus, the confidence interval is: \\[\\text{CI} = -12.04857 \\pm 1.96 \\cdot 2.10217 =  (-16.169,  -7.928)\\]\nWe are 95% confident that the true slope coefficient for length is between -16.169 and -7.928. We are 95% confident that for each additional millimeter increase in length, the number of rings is expected to decrease by between 7.93 and 16.17, holding all other variables constant.\n\n\n\n\nWhat proportion of the variability in rings is explained by the model?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe proportion of variability in rings explained by the model is given by the R-squared value, which is 0.3556. This means that approximately 35.56% of the variability in the number of rings can be explained by the multiple regression model with the predictor variables length, diameter, height, and whole weight."
  },
  {
    "objectID": "activity/08H-sol-mlr-intro.html#example-abalone-age",
    "href": "activity/08H-sol-mlr-intro.html#example-abalone-age",
    "title": "Multiple Regression",
    "section": "",
    "text": "Abalone are a type of mollusk that are harvested for their meat. Knowing the age of abalone can help set sustainable fishing quotas; however, the process is time consuming and labor intensive. Researchers collected various physical measurements, such as length, diameter, height, and weight, that are easy to collect to develop a method to predict abalone age. Below is R output for a multiple linear regression model predicting rings (a proxy for age) that uses length (mm), diameter (mm), height (mm), and whole_weight (in grams) as predictor variables.\nCall:\nlm(formula = rings ~ length + diameter + height + whole_weight, \n    data = abalone)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.90861    0.30922   9.406  &lt; 2e-16\nlength       -12.04857    2.10217  -5.731 1.07e-08\ndiameter      25.64381    2.57384   9.963  &lt; 2e-16\nheight        20.24276    1.78180  11.361  &lt; 2e-16\nwhole_weight   0.06589    0.22557   0.292     0.77    \n---\n\nResidual standard error: 2.59 on 4172 degrees of freedom\nMultiple R-squared:  0.3556,    Adjusted R-squared:  0.3549 \nF-statistic: 575.4 on 4 and 4172 DF,  p-value: &lt; 2.2e-16\n\nReport the estimated regression equation.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe estimated regression equation is: \\[\\widehat{rings} = 2.90861 - 12.04857(\\mathtt{length}) + 25.64381(\\mathtt{diameter}) + 20.24276(\\mathtt{height}) + 0.06589(\\mathtt{whole\\_weight})\\]\n\n\n\n\nUsing the model, predict the number of rings for an abalone with length 0.5 mm, diameter 0.4 mm, height 0.1 mm, and whole weight of 1 gram.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo make the prediction, we substitute the values into the regression equation: \\[\n\\widehat{rings} = 2.90861 - 12.04857(0.5) + 25.64381(0.4) + 20.24276(0.1) + 0.06589(1) = 9.23 \\text{ rings}\n\\]\n\n\n\n\nInterpret the slope coefficient for length in context.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe slope coefficient for length is -12.04857. This means that, holding all other predictor variables constant, for each additional millimeter increase in length, we expect the number of rings to decrease by approximately 12.05.\n\n\n\n\nState the hypotheses for the test of the slope coefficient for whole_weight. What do you conclude based on the results of this test?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe hypotheses for the test of the slope coefficient for whole_weight are:\n\nNull hypothesis (\\(H_0\\)): \\(\\beta_{\\text{whole\\_weight}} = 0\\) (there is no association between whole weight and rings, controlling for other variables)\n\nAlternative hypothesis (\\(H_A\\)): \\(\\beta_{\\text{whole\\_weight}} \\neq 0\\) (there is an association between whole weight and rings, controlling for other variables)\n\nThe p-value for the test is 0.77, which is very large. Therefore, we fail to reject the null hypothesis. There is no evidence to suggest that whole weight is associated with the number of rings, after accounting for length, diameter, and height.\n\n\n\n\nCalculate a 95% confidence interval for the slope coefficient for length. Interpret this interval in context.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nTo calculate the 95% confidence interval for the slope coefficient for length, we use the formula:\n\\[\\text{CI} = \\hat{\\beta} \\pm t^* \\cdot SE(\\hat{\\beta})\\]\nwhere \\(\\hat{\\beta}\\) is the estimated coefficient, \\(SE(\\hat{\\beta})\\) is the standard error, and \\(t^*\\) is the critical value from the t-distribution with 4172 degrees of freedom.\nUsing the output, we have:\n\n\\(\\hat{\\beta}_{\\text{length}} = -12.04857\\)\n\\(SE(\\hat{\\beta}_{\\text{length}}) = 2.10217\\)\nFor a 95% confidence level and 4172 degrees of freedom, \\(t^* \\approx 1.96\\) (using normal approximation for large df)\n\nThus, the confidence interval is: \\[\\text{CI} = -12.04857 \\pm 1.96 \\cdot 2.10217 =  (-16.169,  -7.928)\\]\nWe are 95% confident that the true slope coefficient for length is between -16.169 and -7.928. We are 95% confident that for each additional millimeter increase in length, the number of rings is expected to decrease by between 7.93 and 16.17, holding all other variables constant.\n\n\n\n\nWhat proportion of the variability in rings is explained by the model?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe proportion of variability in rings explained by the model is given by the R-squared value, which is 0.3556. This means that approximately 35.56% of the variability in the number of rings can be explained by the multiple regression model with the predictor variables length, diameter, height, and whole weight."
  },
  {
    "objectID": "activity/11R-mlr-multicollinearity.html",
    "href": "activity/11R-mlr-multicollinearity.html",
    "title": "Regression Cautions",
    "section": "",
    "text": "We all love R2 right? It is ‚Äúeasy‚Äù to interpret and is a popular metric of model fit to report in many fields. While I agree that R2 is a nice metric for describing a single model, it has an issue when it‚Äôs used to compare models. In this example, you‚Äôll explore this issue.\nCase Study 10.1.1 in The Statistical Sleuth describes Galileo‚Äôs experiment that led him to discover that the trajectory of a body falling with horizontal velocity is a parabola (see page 272). The case1011 data set in the Sleuth3 R package contains the results of Galileo‚Äôs experiment. The data set contains seven observations on the following two variables: the horizontal distance traveled from the edge of the table to its landing spot, and the initial height of the object. Both measurements are in punti (1 punti = or 169/180 mm or about 0.037 in).\nFit each of the polynomial models specified on your worksheet and record the R2 value for each in the table on your handout. I‚Äôve gotten you started with the first-order model below.\n\n\n\n\n\n\n\n\nWhat do you observe about the R2 values as you add more polynomial terms to the model?"
  },
  {
    "objectID": "activity/11-mlr-multicollinearity.html",
    "href": "activity/11-mlr-multicollinearity.html",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "",
    "text": "In this activity you will learn how to calculate diagnostic measures and create diagnostic plots for multiple linear regression models in R."
  },
  {
    "objectID": "activity/11-mlr-multicollinearity.html#overview",
    "href": "activity/11-mlr-multicollinearity.html#overview",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "",
    "text": "In this activity you will learn how to calculate diagnostic measures and create diagnostic plots for multiple linear regression models in R."
  },
  {
    "objectID": "activity/11-mlr-multicollinearity.html#example-1-uncorrelated-predictors",
    "href": "activity/11-mlr-multicollinearity.html#example-1-uncorrelated-predictors",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "Example 1: Uncorrelated Predictors",
    "text": "Example 1: Uncorrelated Predictors\nAs a first example, let‚Äôs consider a multiple linear regression model where the two predictor variables are uncorrelated.\nTask 1. Inspect the below correlation matrix and verify that the correlation between the two predictor variables (x1 and x2) is 0.\n\n\n          x1        x2         y\nx1 1.0000000 0.0000000 0.7419309\nx2 0.0000000 1.0000000 0.6384057\ny  0.7419309 0.6384057 1.0000000\n\n\nTask 2. The below code chunk fits a simple linear regression model with y as the response variable and x1 as the predictor variables. The coefficients table was extract using the broom::tidy(). (Feel free to use broom::tidy() it in your work!)\n\nmod1 &lt;- lm(y ~ x1, data = ex1)\nbroom::tidy(mod1)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    23.5      10.1       2.32  0.0591\n2 x1              5.37      1.98      2.71  0.0351\n\n\nSimilarly, here is the output for the simple linear regression model with y as the response variable and x2 as the predictor variable.\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    23.5      10.1       2.32  0.0591\n2 x1              5.37      1.98      2.71  0.0351\n\n\nAnd finally, here is the output for the multiple linear regression model with y as the response variable and both x1 and x2 as the predictor variables.\n\n\n# A tibble: 3 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.375     4.74     0.0791 0.940   \n2 x1             5.37      0.664    8.10   0.000466\n3 x2             9.25      1.33     6.97   0.000937\n\n\nCompare the model summaries from the three models. What do you observe about the coefficients when both predictors are included in the model? What do you observe about the standard errors? The results of the hypothesis tests?"
  },
  {
    "objectID": "activity/11-mlr-multicollinearity.html#example-2-prefectly-correlated-predictors",
    "href": "activity/11-mlr-multicollinearity.html#example-2-prefectly-correlated-predictors",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "Example 2: Prefectly Correlated Predictors",
    "text": "Example 2: Prefectly Correlated Predictors\nIn many examples the predictor variables will be correlated. As an extreme example, let‚Äôs consider a situation where the predictors are perfectly correlated. The data set for this small example is printed below.\n\n\n  x1 x2   y\n1  2  6  23\n2  8  9  83\n3  6  8  63\n4 10 10 103\n\n\nTask 3. Inspect the below correlation matrix and verify that the correlation between the two predictor variables (x1 and x2) is 1.\n\n\n   x1 x2 y\nx1  1  1 1\nx2  1  1 1\ny   1  1 1\n\n\nTask 4. From the previous task, you should have seen that the correlation between x1 and x2 is 1, as is the correlation between both predictors and y. This sounds like a great situation, right? We‚Äôll explore two models to unpack this.\n\nConsider the fitted regression equation: \\(\\widehat{y} = -87 + x_1 + 18x_2\\). Verify that the predicted values are exactly equal to the observed values.\nConsider the fitted regression equation: \\(\\widehat{y} = -7 + 9x_1 + 2x_2\\). Verify that the predicted values are exactly equal to the observed values.\nDiscuss the following questions with your group:\n\nWould you be willing to use either model to make predictions for new observations? Why or why not?\nWould you be willing to interpret the coefficients in either model? Why or why not?\nDo you think the standard error of the slopes is reasonably small or large? Why?"
  },
  {
    "objectID": "activity/11-mlr-multicollinearity.html#example-3-a-more-realistic-example",
    "href": "activity/11-mlr-multicollinearity.html#example-3-a-more-realistic-example",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "Example 3: A more realistic example",
    "text": "Example 3: A more realistic example\nLet‚Äôs consider a more realistic example where the predictor variables are correlated, but not perfectly correlated. The data set bodyfat contains measurements on 20 healthy adults between 25 and 34 years of age. We‚Äôre interested in predicting body fat percentage using three body measurements: triceps skinfold thickness, thigh circumference, and midarm circumference.\nTask 5. Below is the correlation matrix for the data set. What do you observe about the correlations between the predictor variables?\n\n\n                     triceps_skinfold thigh_circumference midarm_circumference\ntriceps_skinfold            1.0000000           0.9238425            0.4577772\nthigh_circumference         0.9238425           1.0000000            0.0846675\nmidarm_circumference        0.4577772           0.0846675            1.0000000\nbody_fat                    0.8432654           0.8780896            0.1424440\n                      body_fat\ntriceps_skinfold     0.8432654\nthigh_circumference  0.8780896\nmidarm_circumference 0.1424440\nbody_fat             1.0000000\n\n\nTask 6. There are numerous models that we could consider. Below are summary tables from several models. Compare the model summaries from the fitted models. What do you observe about the coefficients when different predictors are included in the model? What do you observe about the standard errors? The results of the hypothesis tests?\nOnly triceps skinfold as a predictor:\n\n\n# A tibble: 2 √ó 5\n  term             estimate std.error statistic    p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)        -1.50      3.32     -0.451 0.658     \n2 triceps_skinfold    0.857     0.129     6.66  0.00000302\n\n\nOnly thigh circumference as a predictor:\n\n\n# A tibble: 2 √ó 5\n  term                estimate std.error statistic     p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)          -23.6       5.66      -4.18 0.000566   \n2 thigh_circumference    0.857     0.110      7.79 0.000000360\n\n\nOnly midarm circumference as a predictor:\n\n\n# A tibble: 2 √ó 5\n  term                 estimate std.error statistic p.value\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)            14.7       9.10      1.61    0.124\n2 midarm_circumference    0.199     0.327     0.611   0.549\n\n\nTriceps skinfold and thigh circumference as predictors:\n\n\n# A tibble: 3 √ó 5\n  term                estimate std.error statistic p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)          -19.2       8.36     -2.29   0.0348\n2 triceps_skinfold       0.222     0.303     0.733  0.474 \n3 thigh_circumference    0.659     0.291     2.26   0.0369\n\n\nTriceps skinfold and midarm circumference as predictors:\n\n\n# A tibble: 3 √ó 5\n  term                 estimate std.error statistic     p.value\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)          -26.0        7.00     -3.72  0.00172    \n2 midarm_circumference   0.0960     0.161     0.595 0.560      \n3 thigh_circumference    0.851      0.112     7.57  0.000000772\n\n\nThigh circumference and midarm circumference as predictors:\n\n\n# A tibble: 4 √ó 5\n  term                 estimate std.error statistic p.value\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)            117.       99.8       1.17   0.258\n2 triceps_skinfold         4.33      3.02      1.44   0.170\n3 thigh_circumference     -2.86      2.58     -1.11   0.285\n4 midarm_circumference    -2.19      1.60     -1.37   0.190\n\n\n Let me know when you have reached this spot. We‚Äôll regroup as a class to discuss your observations."
  },
  {
    "objectID": "activity/11-mlr-multicollinearity.html#what-about-predictions",
    "href": "activity/11-mlr-multicollinearity.html#what-about-predictions",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "What about predictions?",
    "text": "What about predictions?\nNow that we‚Äôve discussed the issues revolving around interpreting coefficients and drawing inference on them in the presence of multicollinearity, let‚Äôs consider how multicollinearity impacts predictions.\nRecall that the mean squared error (MSE) of a model measures the variability in the error terms (residuals). In other words, it is measuring the variability unexplained by the model.\nTask 7. Below are the MSE values for three of the models we considered in Task 6. What do you observe about the MSE values when different predictors are included in the model?\n\n\n\nPredictors\nMSE\n\n\n\n\ntriceps_skinfold\n7.95\n\n\ntriceps_skinfold, thigh_circumference\n6.47\n\n\ntriceps_skinfold, thigh_circumference, midarm_circumference\n6.15\n\n\n\nTask 8. Based on your observations, do you think we can use any of the models to make predictions for new observations? Why or why not?\n Let me know when you have reached this spot. We‚Äôll regroup as a class to discuss your observations."
  },
  {
    "objectID": "activity/11-mlr-multicollinearity.html#be-wary-of-r2-when-comparing-models",
    "href": "activity/11-mlr-multicollinearity.html#be-wary-of-r2-when-comparing-models",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "Be Wary of R2 when comparing models",
    "text": "Be Wary of R2 when comparing models\nWe all love R2 right? It is ‚Äúeasy‚Äù to interpret and is a popular metric of model fit to report in many fields. While I agree that R2 is a nice metric for describing a single model, it has an issue when it‚Äôs used to compare models. In this example, you‚Äôll explore this issue.\nCase Study 10.1.1 in The Statistical Sleuth describes Galileo‚Äôs experiment that led him to discover that the trajectory of a body falling with horizontal velocity is a parabola (see page 272). The case1011 data set in the Sleuth3 R package contains the results of Galileo‚Äôs experiment. The data set contains seven observations on the following two variables: the horizontal distance traveled from the edge of the table to its landing spot, and the initial height of the object. Both measurements are in punti (1 punti = or 169/180 mm or about 0.037 in).\nTask 9. Fit each of the following models and report the R2 value for each. Fill in the table below.\n\n\n\n\n\n\n\nModel\nR2\n\n\n\n\n\\(\\text{distance} \\sim \\text{height}\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3 + \\text{height}^4\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3 + \\text{height}^4 + \\text{height}^5\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3 + \\text{height}^4 + \\text{height}^5+ \\text{height}^6\\)\n\n\n\n\nWhat do you observe about the R2 values as you add more polynomial terms to the model?"
  },
  {
    "objectID": "activity/08H-sol-mlr-categorical.html",
    "href": "activity/08H-sol-mlr-categorical.html",
    "title": "Categorical Predictor Variables",
    "section": "",
    "text": "For each regression model, sketch the fitted model. To do this, first write the expression for the fitted model for smokers and the fitted model for non-smokers (e.g., plug in 0 and 1 for the indicator variable). Once you have these two equations, use them to sketch the lines on the provided graph.\nModel 1: \\(\\widehat{\\mu}(y|x) = 10 +  1 \\mathtt{age} -2 \\mathtt{smoker}\\)\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFitted equation for non-smokers: \\(\\widehat{\\mu}(y|x) = 10 +  1 \\mathtt{age}\\)\n\nFitted equation for smokers: \\(\\widehat{\\mu}(y|x) = 8 +  1 \\mathtt{age}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 2: \\(\\widehat{\\mu}(y|x) = 5 +  1 \\mathtt{age} -0.5 \\mathtt{age \\times smoker}\\)\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFitted equation for non-smokers: \\(\\widehat{\\mu}(y|x) = 5 +  1 \\mathtt{age}\\)\nFitted equation for smokers: \\(\\widehat{\\mu}(y|x) = 5 + 0.5 \\mathtt{age}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 3: \\(\\widehat{\\mu}(y|x) = 4  +  0.5 \\mathtt{age} + 3\\mathtt{smoker} -0.5 \\mathtt{age \\times smoker}\\)\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFitted equation for non-smokers: \\(\\widehat{\\mu}(y|x) = 4  +  0.5 \\mathtt{age}\\)\n\nFitted equation for smokers: \\(\\widehat{\\mu}(y|x) = 7  +  0 \\mathtt{age}\\)"
  },
  {
    "objectID": "activity/08H-sol-mlr-categorical.html#example-1",
    "href": "activity/08H-sol-mlr-categorical.html#example-1",
    "title": "Categorical Predictor Variables",
    "section": "",
    "text": "For each regression model, sketch the fitted model. To do this, first write the expression for the fitted model for smokers and the fitted model for non-smokers (e.g., plug in 0 and 1 for the indicator variable). Once you have these two equations, use them to sketch the lines on the provided graph.\nModel 1: \\(\\widehat{\\mu}(y|x) = 10 +  1 \\mathtt{age} -2 \\mathtt{smoker}\\)\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFitted equation for non-smokers: \\(\\widehat{\\mu}(y|x) = 10 +  1 \\mathtt{age}\\)\n\nFitted equation for smokers: \\(\\widehat{\\mu}(y|x) = 8 +  1 \\mathtt{age}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 2: \\(\\widehat{\\mu}(y|x) = 5 +  1 \\mathtt{age} -0.5 \\mathtt{age \\times smoker}\\)\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFitted equation for non-smokers: \\(\\widehat{\\mu}(y|x) = 5 +  1 \\mathtt{age}\\)\nFitted equation for smokers: \\(\\widehat{\\mu}(y|x) = 5 + 0.5 \\mathtt{age}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 3: \\(\\widehat{\\mu}(y|x) = 4  +  0.5 \\mathtt{age} + 3\\mathtt{smoker} -0.5 \\mathtt{age \\times smoker}\\)\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nFitted equation for non-smokers: \\(\\widehat{\\mu}(y|x) = 4  +  0.5 \\mathtt{age}\\)\n\nFitted equation for smokers: \\(\\widehat{\\mu}(y|x) = 7  +  0 \\mathtt{age}\\)"
  },
  {
    "objectID": "activity/08H-sol-mlr-categorical.html#example-2",
    "href": "activity/08H-sol-mlr-categorical.html#example-2",
    "title": "Categorical Predictor Variables",
    "section": "Example 2",
    "text": "Example 2\nResearchers investigated the association of depression with age and education, based on a nationwide (U.S.) telephone survey of 2,031 adults aged 18 to 90. Of particular interest was their finding that the association of depression with education strengthens with increasing age‚Äîa phenomenon they called the ‚Äúdivergence hypothesis.‚Äù\nThe researchers constructed a depression score from responses to several related questions. Education was categorized as (i) college degree, (ii) high school degree plus some college, or (iii) high school degree only.\n\nDefine indicator variables that represents the education category. (Start by determining wow many indicators do you need. Then define them.)\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe need two indicator variables to represent the three categories of education. We can define them as follows: - Let \\(\\mathtt{edu1} = 1\\) if the individual has a college degree, and 0 otherwise. - Let \\(\\mathtt{edu2} = 1\\) if the individual has a high school degree plus some college, and 0 otherwise. - The reference category (when both \\(\\mathtt{edu1}\\) and \\(\\mathtt{edu2}\\) are 0) is having a high school degree only.\nYou could have chosen other baseline categories, but you need two indicators to represent three categories.\n\n\n\n\nSuppose that one researcher wants to fit a model that represents the below sketch (note that the line type represent education). Carefully write the mean function for the regression model that you should fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\(\\widehat{\\mu}(y|x) = \\beta_0 + \\beta_1 \\mathtt{age} + \\beta_2 \\mathtt{edu1} + \\beta_3 \\mathtt{edu2}\\)\n\n\n\n\nSuppose that one researcher wants to fit a model that represents the below sketch (note that the line type represent education). Carefully write the mean function for the regression model that you should fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\(\\widehat{\\mu}(y|x) = \\beta_0 + \\beta_1 \\mathtt{age} + \\beta_2 \\mathtt{age \\times edu1} + \\beta_3 \\mathtt{age \\times edu2}\\)\n\n\n\n\nSuppose that one researcher wants to fit a model that represents the below sketch (note that the line type represent education). Carefully write the mean function for the regression model that you should fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\\(\\widehat{\\mu}(y|x) = \\beta_0 + \\beta_1 \\mathtt{age} + \\beta_2 \\mathtt{edu1} + \\beta_3 \\mathtt{edu2} + \\beta_4 \\mathtt{age \\times edu1} + \\beta_5 \\mathtt{age \\times edu2}\\)"
  },
  {
    "objectID": "activity/08H-mlr-categorical.html",
    "href": "activity/08H-mlr-categorical.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "For each regression model, sketch the fitted model. To do this, first write the expression for the fitted model for smokers and the fitted model for non-smokers (e.g., plug in 0 and 1 for the indicator variable). Once you have these two equations, use them to sketch the lines on the provided graph.\nModel 1: \\(\\widehat{\\mu}(y|x) = 10 +  1 \\mathtt{age} -2 \\mathtt{smoker}\\)\nFitted equation for non-smokers:\n\nFitted equation for smokers:\n\n\n\n\n\n\n\n\n\n\nModel 2: \\(\\widehat{\\mu}(y|x) = 5 +  1 \\mathtt{age} -0.5 \\mathtt{age \\times smoker}\\)\nFitted equation for non-smokers:\n\nFitted equation for smokers:\n\n\n\n\n\n\n\n\n\n\nModel 3: \\(\\widehat{\\mu}(y|x) = 4  +  0.5 \\mathtt{age} + 3\\mathtt{smoker} -0.5 \\mathtt{age \\times smoker}\\)\nFitted equation for non-smokers:\n\nFitted equation for smokers:"
  },
  {
    "objectID": "activity/08H-mlr-categorical.html#example-1",
    "href": "activity/08H-mlr-categorical.html#example-1",
    "title": "Polynomial Regression",
    "section": "",
    "text": "For each regression model, sketch the fitted model. To do this, first write the expression for the fitted model for smokers and the fitted model for non-smokers (e.g., plug in 0 and 1 for the indicator variable). Once you have these two equations, use them to sketch the lines on the provided graph.\nModel 1: \\(\\widehat{\\mu}(y|x) = 10 +  1 \\mathtt{age} -2 \\mathtt{smoker}\\)\nFitted equation for non-smokers:\n\nFitted equation for smokers:\n\n\n\n\n\n\n\n\n\n\nModel 2: \\(\\widehat{\\mu}(y|x) = 5 +  1 \\mathtt{age} -0.5 \\mathtt{age \\times smoker}\\)\nFitted equation for non-smokers:\n\nFitted equation for smokers:\n\n\n\n\n\n\n\n\n\n\nModel 3: \\(\\widehat{\\mu}(y|x) = 4  +  0.5 \\mathtt{age} + 3\\mathtt{smoker} -0.5 \\mathtt{age \\times smoker}\\)\nFitted equation for non-smokers:\n\nFitted equation for smokers:"
  },
  {
    "objectID": "activity/08H-mlr-categorical.html#example-2",
    "href": "activity/08H-mlr-categorical.html#example-2",
    "title": "Polynomial Regression",
    "section": "Example 2",
    "text": "Example 2\nResearchers investigated the association of depression with age and education, based on a nationwide (U.S.) telephone survey of 2,031 adults aged 18 to 90. Of particular interest was their finding that the association of depression with education strengthens with increasing age‚Äîa phenomenon they called the ‚Äúdivergence hypothesis.‚Äù\nThe researchers constructed a depression score from responses to several related questions. Education was categorized as (i) college degree, (ii) high school degree plus some college, or (iii) high school degree only.\n\nDefine indicator variables that represents the education category. (Start by determining wow many indicators do you need. Then define them.)\nSuppose that one researcher wants to fit a model that represents the below sketch (note that the line type represent education). Carefully write the mean function for the regression model that you should fit.\n\n\n\n\n\n\n\n\n\n\n\nSuppose that one researcher wants to fit a model that represents the below sketch (note that the line type represent education). Carefully write the mean function for the regression model that you should fit.\n\n\n\n\n\n\n\n\n\n\n\nSuppose that one researcher wants to fit a model that represents the below sketch (note that the line type represent education). Carefully write the mean function for the regression model that you should fit."
  },
  {
    "objectID": "activity/03-intro-r.html",
    "href": "activity/03-intro-r.html",
    "title": "Introduction to R for Stat 230",
    "section": "",
    "text": "In Stat 230 we will use the R statistical programming language to visualize our data and fit our regression models. Many of you have seen R before, but if you are coming straight from AP statistics this might be your first time using R. Today our goal it to learn the basics of writing and running code in R. To do this, we‚Äôll run our code in a tutorial webpage. Next class I‚Äôll introduce you to the RStudio server and how to create R Markdown files."
  },
  {
    "objectID": "activity/03-intro-r.html#overview",
    "href": "activity/03-intro-r.html#overview",
    "title": "Introduction to R for Stat 230",
    "section": "",
    "text": "In Stat 230 we will use the R statistical programming language to visualize our data and fit our regression models. Many of you have seen R before, but if you are coming straight from AP statistics this might be your first time using R. Today our goal it to learn the basics of writing and running code in R. To do this, we‚Äôll run our code in a tutorial webpage. Next class I‚Äôll introduce you to the RStudio server and how to create R Markdown files."
  },
  {
    "objectID": "activity/03-intro-r.html#loading-data",
    "href": "activity/03-intro-r.html#loading-data",
    "title": "Introduction to R for Stat 230",
    "section": "Loading data",
    "text": "Loading data\nToday we will work with the movies data set. The data set contains the critic‚Äôs score and the audience score for movie released in 2014-2015. The critic‚Äôs score is the percentage of critics who had a favorable view of the movie. Similarly, the audience score is the percentage of users who had a favorable view of the movie. Today, you‚Äôll explore the relationship between the audience and critic scores.\nTo load data into R, we use the read.csv() command and pass in either a file path or URL in quotes. Below, the data are being loaded from the web. Click ‚ÄúRun code‚Äù to load the data.\n\n\n\n\n\n\n\n\nNotice that we use parentheses with functions and place the arguments for that function inside those parentheses.\n\n\n\n\n\n\nNote\n\n\n\nYou should notice that R did not appear to do anything when you ran the above code. That‚Äôs because everything was done in the background and R will only show you what is requested. So don‚Äôt panic if you don‚Äôt see any output!"
  },
  {
    "objectID": "activity/03-intro-r.html#getting-a-glimpse-of-the-data-set",
    "href": "activity/03-intro-r.html#getting-a-glimpse-of-the-data-set",
    "title": "Introduction to R for Stat 230",
    "section": "Getting a glimpse of the data set",
    "text": "Getting a glimpse of the data set\nThroughout the term I‚Äôll refer to small pieces of code as ‚Äúcode chunks.‚Äù This is both descriptive and will be what RStudio (our interface) calls them later on. For today, we‚Äôll focus on how to write small code chunks to accomplish specific tasks for simple linear regression.\nFor example, we might wish to get an overview of the data set by printing the first few rows via head()\n\n\n\n\n\n\n\n\nI prefer to use a different function, glimpse(), which is in the {dplyr} package. To load a package we first must run the library() command. The below code chunk loads this package and gives a ‚Äúglimpse‚Äù of the movies data set:\n\n\n\n\n\n\n\n\n\nCheckpoint questions:\nUsing the output from the above code chunk, answer the following questions in the code box below:\n\nHow many rows are in the movies data set?\nHow many variables are in the movies data set? Are any categorical? Are any quantitative?"
  },
  {
    "objectID": "activity/03-intro-r.html#univariate-exploration",
    "href": "activity/03-intro-r.html#univariate-exploration",
    "title": "Introduction to R for Stat 230",
    "section": "Univariate exploration",
    "text": "Univariate exploration\nBefore jumping into a regression model, it can be useful to understand more about each variable. This can be accomplished using intro stat tools such as histograms and summary statistics.\nThe fastest way to get started is using the summary() function, which returns a brief set of summary statistics for each variable in the data set.\n\n\n\n\n\n\n\n\nR also has built-in functions to compute summary statistics one by one for a specific column. To ‚Äúextract‚Äù a variable from the data set, we use a $. For example, we can extract the critics column from the movies data frame and calculate the standard deviation:\n\n\n\n\n\n\n\n\nOther useful summary statistics for quantitative variables include (here x denotes an extracted column):\n\n\n\nFunction\nSummary statistic\n\n\n\n\nmedian(x)\nmedian\n\n\nmean(x)\nmean\n\n\nvar(x)\nvariance\n\n\nIQR(x)\ninterquartile range\n\n\n\n\nTo create graphics in this course we‚Äôll use tools from the {ggformula} R package. All of the functions will begin with gf_ and have similar syntax. If you‚Äôre familiar with the {ggplot2} package, feel free to use it for this class, but most students like the ‚Äúone-liners‚Äù that {ggformula} allows.\nTo begin, load the {ggformula} package using the library command:\n\n\n\n\n\n\n\n\nFor example, to create a histogram of the critic‚Äôs score we can run\n\n\n\n\n\n\n\n\nHere we use the ~ to tell R that critics is a variable in the data set. We must also specify data and adjust the number of bins as needed.\nSimilar syntax is used to create a boxplot\n\n\n\n\n\n\n\n\n\nCheckpoint questions\nUse the below code box for the checkpoint questions:\n\n\n\n\n\n\n\n\n\nCalculate the mean audience score using the mean() function.\nCreate a histogram of the audience score and describe what you see.\nChange the number of bins for a histogram of audience. What number of bins seems ‚Äúabout right‚Äù in your opinion?"
  },
  {
    "objectID": "activity/03-intro-r.html#bivariate-exploration",
    "href": "activity/03-intro-r.html#bivariate-exploration",
    "title": "Introduction to R for Stat 230",
    "section": "Bivariate exploration",
    "text": "Bivariate exploration\n\nScatterplots\nIn this course we are interested in exploring relationships between variables. If we have two quantitative variables, then a scatterplot is a natural way to visually explore this relationship. To create a scatterplot we can use the gf_point() function (since scatterplots are just points):\n\n\n\n\n\n\n\n\nNotice that we put the response variable on the left of the ~ and the explanatory variable on the right.\n\nCheckpoint questions\nUse the below code box for the checkpoint questions:\n\n\n\n\n\n\n\n\n\nCreate a scatterplot where critics score is the response variable and audience score is the explanatory variable.\nYou can adjust the axis labels by adding an xlab or ylab argument. Add xlab = \"Your axis label\" and ylab = \"Your other axis label\" to your scatterplot and give more verbose axis labels.\n\n\n\n\nScatterplots with regression lines\nYou can also add a simple linear regression line to your scatterplot by adding a layer. To add a layer, we append the |&gt; pipe operator to the end of our gf_point() code and use the gf_lm() command to add a line:\n\n\n\n\n\n\n\n\n\n\nCorrelation\nIf you want a numeric summary of the strength of the linear association between two quantitative variables, then you can calculate the correlation. In R, you do this using the cor() function. Similar to calculating the mean, you need to extract the columns using $ as shown below:\n\n\n\n\n\n\n\n\n\nCheckpoint questions\nUse the below code box for the checkpoint question:\n\n\n\n\n\n\n\n\n\nSwap the order of the variables in the cor() command. Does the correlation change? Is this what you expected?"
  },
  {
    "objectID": "activity/03-intro-r.html#fitting-a-regression-model",
    "href": "activity/03-intro-r.html#fitting-a-regression-model",
    "title": "Introduction to R for Stat 230",
    "section": "Fitting a regression model",
    "text": "Fitting a regression model\nFinally, lets fit a simple linear regression model. To do this we use the lm() command (which stands for linear model). Here, we use the same syntax as when creating a scatterplot:\n\n\n\n\n\n\n\n\nWhen you run this command, R prints some very basic information about the SLR model.\nCheckpoint: What information do you get when you run the above command?\nOften, you will want to obtain more information than you get from simply printing the fitted model. The summary() function provides much more information. I recommend storing your fitted model as a named object (such as movie_lm) and then running the summary():\n\n\n\n\n\n\n\n\n\nCheckpoint question\nWhat information do you get when you run the above summary() command? If you‚Äôre not sure what something means, discuss it with your group and then ask about it in our debrief."
  },
  {
    "objectID": "activity/03-intro-r.html#constructing-confidence-intervals",
    "href": "activity/03-intro-r.html#constructing-confidence-intervals",
    "title": "Introduction to R for Stat 230",
    "section": "Constructing confidence intervals",
    "text": "Constructing confidence intervals\nThe table obtained by summary() contains all of the necessary information to conduct hypothesis tests for regression coefficients; however, it does not provide confidence intervals. To instruct R to construct a confidence interval for a regression coefficient, we can use the confint() function:\n\n\n\n\n\n\n\n\n\nCheckpoint questions\n\nWhat information do you get when you run the above confint() command?\nChange the level argument to construct a 90% confidence interval. How does the interval change?"
  },
  {
    "objectID": "activity/03-intro-r.html#making-predictions",
    "href": "activity/03-intro-r.html#making-predictions",
    "title": "Introduction to R for Stat 230",
    "section": "Making predictions",
    "text": "Making predictions\nTo make predictions, we use our fitted regression equation. While you can (and should be able to) do this by hand, you can also use R as your calculator.\nSuppose we can use our model to predict the audience score for more recent movies. Then we could predict the audience score for the Barbie Movie by running:\n\n\n\n\n\n\n\n\nHere, we first need to create a new data set with a column named identically to the original data set. Here we have a single column named critics with the value of the explanatory variable we want to use. Next, we use the predict() function, passing in the name of our model and the new data set we want to make predictions for.\n\nCheckpoint questions\nUse the below code box for the checkpoint questions:\n\n\n\n\n\n\n\n\n\nThe critics score for Asteroid City was 75. What is the expected audience score for this movie?\nThe actual (observed) value for Asteroid City was 62. Did the mode over- or under-predict the audience score?\nDo you think it‚Äôs reasonable to use our model to predict the audience score for movies released in 2023? Why or why not?\n\n\n\nMaking multiple predictions\nIf you want to make multiple predictions, then you can create a new data frame with multiple values for the explanatory variable. For example, we can use c(88, 75) to make a data frame with two values in the critics column:"
  },
  {
    "objectID": "activity/03-intro-r.html#function-quick-reference",
    "href": "activity/03-intro-r.html#function-quick-reference",
    "title": "Introduction to R for Stat 230",
    "section": "Function quick reference",
    "text": "Function quick reference\nThe following table summarizes the functions we learned today:\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nread.csv(\"file_path_or_URL\")\nLoad a CSV data set from a file or the web\n\n\nhead(data)\nPrint the first few rows of a data set\n\n\nglimpse(data)\nGet a quick overview of a data set (from {dplyr} package)\n\n\nsummary(data)\nGet summary statistics for each variable in a data set\n\n\nmean(x)\nCalculate the mean of a quantitative variable\n\n\nmedian(x)\nCalculate the median of a quantitative variable\n\n\nsd(x)\nCalculate the standard deviation of a quantitative variable\n\n\nvar(x)\nCalculate the variance of a quantitative variable\n\n\ngf_histogram(~x, data = data, bins = n)\nCreate a histogram of a quantitative variable (from {ggformula} package)\n\n\ngf_boxplot(~x, data = data)\nCreate a boxplot of a quantitative variable (from {ggformula} package)\n\n\ngf_point(y ~ x, data = data)\nCreate a scatterplot of two quantitative variables (from {ggformula} package)\n\n\ngf_lm()\nAdd a simple linear regression line to a scatterplot (from {ggformula} package)\n\n\ncor(x, y)\nCalculate the correlation between two quantitative variables\n\n\nlm(y ~ x, data = data)\nFit a simple linear regression model\n\n\nsummary(model)\nGet detailed information about a fitted regression model\n\n\nconfint(model, level = 0.95)\nConstruct a confidence interval for regression coefficients\n\n\npredict(model, newdata = new_data)\nMake predictions using a fitted regression model for new data"
  },
  {
    "objectID": "activity/03-intro-r.html#acknowledgements",
    "href": "activity/03-intro-r.html#acknowledgements",
    "title": "Introduction to R for Stat 230",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe movie example was adapted from Maria Tackett‚Äôs draft of Introduction to Regression Analysis: A Data Science Approach."
  },
  {
    "objectID": "slides/05-transformation-interpretation.html",
    "href": "slides/05-transformation-interpretation.html",
    "title": "Summary: Interpretation of slope under log transformations",
    "section": "",
    "text": "\\(\\mu \\lbrace \\log(Y) | x \\rbrace = \\beta_0 + \\beta_1x\\)\n\\[\n\\frac{\\text{median of } Y | x+1}{\\text{median of } Y | x} = \\frac{e^{\\beta_0 + \\beta_1 (x+1)}}{e^{\\beta_0 + \\beta_1 x}} = e^{\\beta_1}\n\\]\nThe median of \\(Y\\) at \\(x + 1\\) is \\(e^\\beta_1\\) times larger (smaller) than the median of \\(Y\\) at \\(x\\). Increasing \\(x\\) by 1 increases (decreases) the median of \\(Y\\) by a factor of \\(e^\\beta_1\\).\n\n\n\n\\(\\mu \\lbrace Y | x \\rbrace = \\beta_0 + \\beta_1 \\log(x)\\)\nDouble x: \\(\\mu \\lbrace Y | 2x \\rbrace - \\mu \\lbrace Y | x \\rbrace = \\beta_1 \\log(2)\\)\nA doubling of \\(x\\) is associated with the mean response (\\(Y\\)) increasing (decreasing) by \\(\\beta_1 \\log(2)\\) units.\nNote that you can use other multiplicative factors other than a doubling. For instance, if you want \\(x\\) to increase by a factor of 1.25 (that is, a 25% increase), then a similar derivation would yield\n\\(\\mu \\lbrace Y | 1.25x \\rbrace - \\mu \\lbrace Y | x \\rbrace = \\beta_1 \\log(1.25)\\)\nIncreasing \\(x\\) by 25% results in the mean of \\(Y\\) increasing (decreasing) by \\(\\beta_1 \\log(1.25)\\) units.\n\n\n\n\\(\\mu \\lbrace \\log(Y) | x \\rbrace = \\beta_0 + \\beta_1 \\log(x)\\)\n\\[\n\\frac{\\text{median of } Y | 2x}{\\text{median of } Y | x} = e^{\\beta_1 \\log(2)} = 2^{\\beta_1}\n\\]\nThe median of \\(Y\\) at \\(2x\\) is \\(e^{\\beta_1 \\log(2)}\\) times greater (smaller) than the median of \\(Y\\) at \\(x\\). Or equivalently, a doubling of x is associated with the median of Y increasing (decreasing) by a factor of \\(e^{\\beta_1 \\log(2)}\\).\nAs above, you can use other multiplicative factors other than a doubling. For a 25% increase,\n\\[\n\\frac{\\text{median of } Y | 1.25x}{\\text{median of } Y | x} = e^{\\beta_1 \\log(1.25)} = 1.25^{\\beta_1}\n\\]\n\n\n\n\nMultiplicative changes can also be stated in terms of percentage increases/decreases.\n\n\nEx. 1 The median of Y at 2x is 1.83 times greater than the median of Y at x.\n\\((1.83 - 1) \\times 100\\% = 83\\%\\)\nA doubling of x is associated with an 83% increase in the median of Y .\nEx. 2 The median of Y at 2x is 0.75 times smaller than the median of Y at x.\n\\((0.75 - 1) \\times 100\\% = -25\\%\\)\nA doubling of x is associated with a 25% decrease in the median of Y\n\n\nAs noted above, when your explanatory variable is logged, instead of ‚Äúdoubling x,‚Äù you can use other multiplicative factors.\n\n\nEx. 3 \\(\\hat{\\mu} \\lbrace Y | x \\rbrace = 1.4 + 3.1 \\log(x)\\)\nSuppose you want to consider a 10% increase in x, so this corresponds to a factor of \\(0.10 + 1 = 1.10\\).\n\\(3.1 \\log(1.1) = 0.297\\)\nA 10% increase in x is associated with the mean of Y increasing by about 0.3 units. (Note: this is an additive increase in the mean of Y ).\nEx. 4 \\(\\hat{\\mu} \\lbrace \\log(Y) | x \\rbrace = -0.54 - 1.8 \\log(x)\\)\nSuppose you want to consider a 30% increase in x. This corresponds to a multiplicative change of \\(0.30 + 1 = 1.30\\). That is, going from x to 1.3x.\n\\(e^{-1.8 \\log(1.3)} = 1.3^{-1.8} = 0.6236\\).\nA 30% increase in x is associated with the median of Y decreasing by a factor of 0.624.\nA 30% increase in x is associated with the median of Y decreasing by about 38%. \\((0.624-1) \\times 100\\%\\).\n\n\n\n\nAfter fitting the transformed model, our regression line gives predictions for the mean of \\(\\log(Y)\\) for a given value of \\(x\\), which we denote \\(\\mu\\lbrace\\log(Y) | x\\rbrace\\). A sketch of what this model looks like is shown below. Notice that on the log scale, the regression line describes the center (the mean) of a symmetric distribution around the regression line.\n\n\n\n\n\n\n\n\n\nWhen we exponentiate to move back to the original scale we ‚Äúdistort‚Äù the model. The plot below shows the back-transformed version of the previous plot. Notice that the distributions are no longer symmetric. What was the mean on the log scale corresponds to the median on the original scale.\n\n\n\n\n\n\n\n\n\nOn the univariate scale, consider the following example:\n\n\n\n\n\n\n\n\n\nNow let‚Äôs back transform and move back to the original scale from the log scale. Back-transforming the mean of log(y) gives us \\(e^{0.004}=1.004\\), which is much closer to the median of y than to the mean."
  },
  {
    "objectID": "slides/05-transformation-interpretation.html#log-transform-y-only",
    "href": "slides/05-transformation-interpretation.html#log-transform-y-only",
    "title": "Summary: Interpretation of slope under log transformations",
    "section": "",
    "text": "\\(\\mu \\lbrace \\log(Y) | x \\rbrace = \\beta_0 + \\beta_1x\\)\n\\[\n\\frac{\\text{median of } Y | x+1}{\\text{median of } Y | x} = \\frac{e^{\\beta_0 + \\beta_1 (x+1)}}{e^{\\beta_0 + \\beta_1 x}} = e^{\\beta_1}\n\\]\nThe median of \\(Y\\) at \\(x + 1\\) is \\(e^\\beta_1\\) times larger (smaller) than the median of \\(Y\\) at \\(x\\). Increasing \\(x\\) by 1 increases (decreases) the median of \\(Y\\) by a factor of \\(e^\\beta_1\\)."
  },
  {
    "objectID": "slides/05-transformation-interpretation.html#log-transform-x-only",
    "href": "slides/05-transformation-interpretation.html#log-transform-x-only",
    "title": "Summary: Interpretation of slope under log transformations",
    "section": "",
    "text": "\\(\\mu \\lbrace Y | x \\rbrace = \\beta_0 + \\beta_1 \\log(x)\\)\nDouble x: \\(\\mu \\lbrace Y | 2x \\rbrace - \\mu \\lbrace Y | x \\rbrace = \\beta_1 \\log(2)\\)\nA doubling of \\(x\\) is associated with the mean response (\\(Y\\)) increasing (decreasing) by \\(\\beta_1 \\log(2)\\) units.\nNote that you can use other multiplicative factors other than a doubling. For instance, if you want \\(x\\) to increase by a factor of 1.25 (that is, a 25% increase), then a similar derivation would yield\n\\(\\mu \\lbrace Y | 1.25x \\rbrace - \\mu \\lbrace Y | x \\rbrace = \\beta_1 \\log(1.25)\\)\nIncreasing \\(x\\) by 25% results in the mean of \\(Y\\) increasing (decreasing) by \\(\\beta_1 \\log(1.25)\\) units."
  },
  {
    "objectID": "slides/05-transformation-interpretation.html#log-transform-both-y-and-x",
    "href": "slides/05-transformation-interpretation.html#log-transform-both-y-and-x",
    "title": "Summary: Interpretation of slope under log transformations",
    "section": "",
    "text": "\\(\\mu \\lbrace \\log(Y) | x \\rbrace = \\beta_0 + \\beta_1 \\log(x)\\)\n\\[\n\\frac{\\text{median of } Y | 2x}{\\text{median of } Y | x} = e^{\\beta_1 \\log(2)} = 2^{\\beta_1}\n\\]\nThe median of \\(Y\\) at \\(2x\\) is \\(e^{\\beta_1 \\log(2)}\\) times greater (smaller) than the median of \\(Y\\) at \\(x\\). Or equivalently, a doubling of x is associated with the median of Y increasing (decreasing) by a factor of \\(e^{\\beta_1 \\log(2)}\\).\nAs above, you can use other multiplicative factors other than a doubling. For a 25% increase,\n\\[\n\\frac{\\text{median of } Y | 1.25x}{\\text{median of } Y | x} = e^{\\beta_1 \\log(1.25)} = 1.25^{\\beta_1}\n\\]"
  },
  {
    "objectID": "slides/05-transformation-interpretation.html#remarks",
    "href": "slides/05-transformation-interpretation.html#remarks",
    "title": "Summary: Interpretation of slope under log transformations",
    "section": "",
    "text": "Multiplicative changes can also be stated in terms of percentage increases/decreases.\n\n\nEx. 1 The median of Y at 2x is 1.83 times greater than the median of Y at x.\n\\((1.83 - 1) \\times 100\\% = 83\\%\\)\nA doubling of x is associated with an 83% increase in the median of Y .\nEx. 2 The median of Y at 2x is 0.75 times smaller than the median of Y at x.\n\\((0.75 - 1) \\times 100\\% = -25\\%\\)\nA doubling of x is associated with a 25% decrease in the median of Y\n\n\nAs noted above, when your explanatory variable is logged, instead of ‚Äúdoubling x,‚Äù you can use other multiplicative factors.\n\n\nEx. 3 \\(\\hat{\\mu} \\lbrace Y | x \\rbrace = 1.4 + 3.1 \\log(x)\\)\nSuppose you want to consider a 10% increase in x, so this corresponds to a factor of \\(0.10 + 1 = 1.10\\).\n\\(3.1 \\log(1.1) = 0.297\\)\nA 10% increase in x is associated with the mean of Y increasing by about 0.3 units. (Note: this is an additive increase in the mean of Y ).\nEx. 4 \\(\\hat{\\mu} \\lbrace \\log(Y) | x \\rbrace = -0.54 - 1.8 \\log(x)\\)\nSuppose you want to consider a 30% increase in x. This corresponds to a multiplicative change of \\(0.30 + 1 = 1.30\\). That is, going from x to 1.3x.\n\\(e^{-1.8 \\log(1.3)} = 1.3^{-1.8} = 0.6236\\).\nA 30% increase in x is associated with the median of Y decreasing by a factor of 0.624.\nA 30% increase in x is associated with the median of Y decreasing by about 38%. \\((0.624-1) \\times 100\\%\\)."
  },
  {
    "objectID": "slides/05-transformation-interpretation.html#why-the-median",
    "href": "slides/05-transformation-interpretation.html#why-the-median",
    "title": "Summary: Interpretation of slope under log transformations",
    "section": "",
    "text": "After fitting the transformed model, our regression line gives predictions for the mean of \\(\\log(Y)\\) for a given value of \\(x\\), which we denote \\(\\mu\\lbrace\\log(Y) | x\\rbrace\\). A sketch of what this model looks like is shown below. Notice that on the log scale, the regression line describes the center (the mean) of a symmetric distribution around the regression line.\n\n\n\n\n\n\n\n\n\nWhen we exponentiate to move back to the original scale we ‚Äúdistort‚Äù the model. The plot below shows the back-transformed version of the previous plot. Notice that the distributions are no longer symmetric. What was the mean on the log scale corresponds to the median on the original scale.\n\n\n\n\n\n\n\n\n\nOn the univariate scale, consider the following example:\n\n\n\n\n\n\n\n\n\nNow let‚Äôs back transform and move back to the original scale from the log scale. Back-transforming the mean of log(y) gives us \\(e^{0.004}=1.004\\), which is much closer to the median of y than to the mean."
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#think-about-it",
    "href": "slides/11-mlr-multicollinearity.html#think-about-it",
    "title": "Regression Cautions",
    "section": "Think about it",
    "text": "Think about it\n\nWork through Examples 1-3 on worksheet\nWork with your group to complete the tasks\nLet me know when finish Task 6\nBe prepared to share thoughts with the class"
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#diagnosing-multicollinearity",
    "href": "slides/11-mlr-multicollinearity.html#diagnosing-multicollinearity",
    "title": "Regression Cautions",
    "section": "Diagnosing multicollinearity",
    "text": "Diagnosing multicollinearity\n\nExamine scatterplot matrix\nExamine pairwise correlations between predictors\nLook for \\(\\widehat{\\beta}_i\\)s with unusual signs\nNotice sensitivity to changes in the model/order of predictors\nCalculate the Variance Inflation Factor (VIF)"
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#variance-inflation-factor-vif",
    "href": "slides/11-mlr-multicollinearity.html#variance-inflation-factor-vif",
    "title": "Regression Cautions",
    "section": "Variance Inflation Factor (VIF)",
    "text": "Variance Inflation Factor (VIF)\n\\({\\rm VIF} = \\dfrac{1}{1 - R^2_i}\\)\n\\(R^2_i=R^2\\) from model predicting \\(x_i\\) using all other predictor variables\n\nGuidelines:\n\n\\(\\text{VIF}_i &gt; 5\\) suspicions begin; \\(R^2_i &gt; .8\\)\n\\(\\text{VIF}_i &gt; 10\\) indicates a problem; \\(R^2_i &gt; .9\\)\n\\(\\text{VIF}_i &gt; 100\\) indicates a big problem; \\(R^2_i &gt; .99\\)"
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#vifs-for-example-1",
    "href": "slides/11-mlr-multicollinearity.html#vifs-for-example-1",
    "title": "Regression Cautions",
    "section": "VIFs for Example 1",
    "text": "VIFs for Example 1\n\nex1_mod &lt;- lm(y ~ x1 + x2, data = ex1)\ncar::vif(ex1_mod)\n\n\n\n\n\n\n\n\n\n\nVariable\nVIF\n\n\n\n\nx1\n1\n\n\nx2\n1\n\n\n\n\n\n\n\n\nThe vif() command is in the car R package"
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#vifs-for-example-2",
    "href": "slides/11-mlr-multicollinearity.html#vifs-for-example-2",
    "title": "Regression Cautions",
    "section": "VIFs for Example 2",
    "text": "VIFs for Example 2\ncar::vif() throws an error! Why?\n\n\\(\\text{VIF} = \\dfrac{1}{1 - R^2_i}\\)\n\nSince \\(x_1\\) and \\(x_2\\) are perfectly correlated, \\(R^2_i = 1\\)\nThis makes the denominator \\(1 - R^2_i = 0\\)\nSo, \\(\\text{VIF} = \\dfrac{1}{0}\\) is undefined"
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#vifs-for-example-3",
    "href": "slides/11-mlr-multicollinearity.html#vifs-for-example-3",
    "title": "Regression Cautions",
    "section": "VIFs for Example 3",
    "text": "VIFs for Example 3\n\nbodyfat_mod &lt;- lm(body_fat ~ ., data = bodyfat)\ncar::vif(bodyfat_mod)\n\n\n\n\n\n\n\n\n\n\nVariable\nVIF\n\n\n\n\ntriceps_skinfold\n708.8\n\n\nthigh_circumference\n564.3\n\n\nmidarm_circumference\n104.6\n\n\n\n\n\n\n\n\nThe . on the right side of the model formula tells R to use all other variables in the data frame as predictors"
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#remedial-measures",
    "href": "slides/11-mlr-multicollinearity.html#remedial-measures",
    "title": "Regression Cautions",
    "section": "Remedial measures",
    "text": "Remedial measures\n\nUse the model only for prediction, if you think future observations will have the same relationships amongst variables\nFor polynomials, use poly(x, degree) in R, or center the variable\nDrop some of highly correlated variables ‚Äî USE CAUTION, doesn‚Äôt always work and loses information\nAdd cases that break the correlation between predictors ‚Äì reasonable in designed experiments\nCreate composite variables (e.g., sums, averages, principal components) ‚Äì can be harder to interpret\nUse ridge regression or LASSO ‚Äì need Stat 270 (Statistical Learning)"
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#the-problem-with-r2",
    "href": "slides/11-mlr-multicollinearity.html#the-problem-with-r2",
    "title": "Regression Cautions",
    "section": "The problem with R2",
    "text": "The problem with R2\nDifferent polynomial models were used to predict the horizontal distance based on the starting point in Galileo‚Äôs experimental data\n\n\n\n\nOrder\nR2\n\n\n\n\n1\n0.9264\n\n\n2\n0.9903\n\n\n3\n0.9994\n\n\n4\n0.9998\n\n\n5\n0.99996\n\n\n6\n1.0000"
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#adjusted-r2",
    "href": "slides/11-mlr-multicollinearity.html#adjusted-r2",
    "title": "Regression Cautions",
    "section": "Adjusted R2",
    "text": "Adjusted R2\nImposes a penalty for model complexity, so it increases only if the improvement outweighs the cost of making the model more complex\n\\[\nR^2_{\\text{adj}} = 1 - \\left( \\frac{n-1}{n-p-1} \\right)  \\cdot \\left(1 - R^2 \\right)\n\\]"
  },
  {
    "objectID": "slides/11-mlr-multicollinearity.html#adjusted-r2-1",
    "href": "slides/11-mlr-multicollinearity.html#adjusted-r2-1",
    "title": "Regression Cautions",
    "section": "Adjusted R2",
    "text": "Adjusted R2\n\n\n\nOrder\nR2\nR2adj\n\n\n\n\n1\n0.9264\n0.9116\n\n\n2\n0.9903\n0.9855\n\n\n3\n0.9994\n0.9988\n\n\n4\n0.9998\n0.9995\n\n\n5\n0.99996\n0.9998\n\n\n6\n1.0000\nundefined\n\n\n\n\nRecall that there are only n = 7 observations in this data set"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#warm-up",
    "href": "slides/10-mlr-diagnostics.html#warm-up",
    "title": "Multiple Regression Diagnostics",
    "section": "Warm up",
    "text": "Warm up\nWith your group\n\nList the conditions for MLR\nBrainstorm ways to check these conditions"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#regression-is-not-resistant-to-outliers",
    "href": "slides/10-mlr-diagnostics.html#regression-is-not-resistant-to-outliers",
    "title": "Multiple Regression Diagnostics",
    "section": "Regression is not resistant to outliers",
    "text": "Regression is not resistant to outliers\nSolid line = with outliers; dashed line = without outliers"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#slr-is-not-resistant-to-outliers",
    "href": "slides/10-mlr-diagnostics.html#slr-is-not-resistant-to-outliers",
    "title": "Multiple Regression Diagnostics",
    "section": "SLR is not resistant to outliers",
    "text": "SLR is not resistant to outliers\n\n\\[\\widehat{\\beta}_1 = r \\cdot \\dfrac{s_y}{s_x} \\qquad \\qquad \\widehat{\\beta}_0 = \\bar{y} - \\widehat{\\beta}_1 \\bar{x}\\]\n\nWhy?\n\n\\(r\\) is not resistant\n\\(s_y\\), \\(s_x\\) are not resistant\n\\(\\bar{x}\\), \\(\\bar{y}\\) are not resistant"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#mlr-is-not-resistant-to-outliers",
    "href": "slides/10-mlr-diagnostics.html#mlr-is-not-resistant-to-outliers",
    "title": "Multiple Regression Diagnostics",
    "section": "MLR is not resistant to outliers",
    "text": "MLR is not resistant to outliers\nWe‚Äôre still choosing the \\(\\widehat{\\beta}_i\\) to minimize the sum of squared residuals\n\\[\n\\begin{split}\nSSE &= \\sum_{i=1}^n \\left( y_i - \\widehat{y}_i \\right)^2\\\\\n&= \\sum_{i=1}^n \\left( y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 x_{i1} - \\widehat{\\beta}_2 x_{i2} - ... - \\widehat{\\beta}_p x_{ip} \\right)^2\n\\end{split}\n\\]\nSum‚Äôs aren‚Äôt resistant to outliers!"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#types-of-outliers",
    "href": "slides/10-mlr-diagnostics.html#types-of-outliers",
    "title": "Multiple Regression Diagnostics",
    "section": "Types of outliers",
    "text": "Types of outliers"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#outliers",
    "href": "slides/10-mlr-diagnostics.html#outliers",
    "title": "Multiple Regression Diagnostics",
    "section": "Outliers",
    "text": "Outliers\n\n\nIn higher dimensions, outliers can be tricky to detect\nTo detect outliers in X, you have to consider the multivariate relationships between all of the predictors"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#not-all-outliers-are-influential",
    "href": "slides/10-mlr-diagnostics.html#not-all-outliers-are-influential",
    "title": "Multiple Regression Diagnostics",
    "section": "Not all outliers are influential",
    "text": "Not all outliers are influential\nOrange line = SLR with outlier; Black line = SLR without outlier"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#influential-points",
    "href": "slides/10-mlr-diagnostics.html#influential-points",
    "title": "Multiple Regression Diagnostics",
    "section": "Influential points",
    "text": "Influential points\nPoints that are able to substantially change the fitted model ares influential\n\n\nHow do we find outliers and determine if they are influential?\n\nPlot the data\nPlot the standardized residuals\nFit the model with/without the point(s)\nCalculate to influence diagnostics"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#leverage",
    "href": "slides/10-mlr-diagnostics.html#leverage",
    "title": "Multiple Regression Diagnostics",
    "section": "Leverage",
    "text": "Leverage\nMeasures the potential to influence the SLR line\n\n\nFor SLR:\n\n\\[h_i = \\frac{1}{n-1} \\left[ \\frac{x_i - \\overline{x}}{s_x} \\right]^2 + \\frac{1}{n} = \\dfrac{1}{n} + \\frac{\\left(x_i - \\overline{x}\\right)^2}{\\sum \\left(x_i - \\overline{x}\\right)^2}\\]\n\n\n\nFor MLR:\n\nmore complicated because we‚Äôre looking at a distance of a case from the average of p predictors\n\nCutoff\n\n\\(2(p+1)/n\\)\n\\(3(p+1)/n\\)\nAlso a good idea to examine a histogram/index plot\n\n\nI use p to denote the number of slope coefficients, so there are p+1 regression coefficients"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#leverage-1",
    "href": "slides/10-mlr-diagnostics.html#leverage-1",
    "title": "Multiple Regression Diagnostics",
    "section": "Leverage",
    "text": "Leverage\n‚ÄúFlares up‚Äù when value of x is far from the mean"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#leverage-2",
    "href": "slides/10-mlr-diagnostics.html#leverage-2",
    "title": "Multiple Regression Diagnostics",
    "section": "Leverage",
    "text": "Leverage\nIn higher dimensions, leverage is focusing on the distance from the average of all of the predictors"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#standardized-residuals",
    "href": "slides/10-mlr-diagnostics.html#standardized-residuals",
    "title": "Multiple Regression Diagnostics",
    "section": "Standardized residuals",
    "text": "Standardized residuals\nStandardized residuals are calculated by dividing the residuals by their standard deviation\n\\[r_i = \\frac{e_i}{\\widehat{\\sigma} \\sqrt{1 - \\dfrac{1}{n} - \\dfrac{(x_i - \\overline{x})^2}{\\sum (x_i - \\overline{x})^2}}} = \\dfrac{e_i}{ \\widehat{\\sigma} \\sqrt{1 - h_{i}}}\\]\n\nGuidelines:\n\n\\(|r_i|&gt;2\\) for small data sets\n\\(|r_i|&gt;4\\) for large data sets\n\n\nSleuth calls these (internally) studentized residuals while R calls these standardized residuals"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#standardized-residuals-1",
    "href": "slides/10-mlr-diagnostics.html#standardized-residuals-1",
    "title": "Multiple Regression Diagnostics",
    "section": "Standardized residuals",
    "text": "Standardized residuals\n‚ÄúFlare up‚Äù when value of y is far from \\(\\widehat{y}\\)"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#dffits",
    "href": "slides/10-mlr-diagnostics.html#dffits",
    "title": "Multiple Regression Diagnostics",
    "section": "DFFITS",
    "text": "DFFITS\nMeasures effect the ith case has on its own fitted value\n\\[DFFITS_i = \\frac{\\widehat{y}_i - \\widehat{y}_{i(i)}}{\\widehat{\\sigma}_{(i)} \\sqrt{h_i}}\\] where the subscript \\((i)\\) indicates that the value is based on a model when observation i is omitted\n\nCutoff\n\n1 for small/medium data sets\n\\(2\\sqrt{\\dfrac{p+1}{n}}\\) for large data sets\n\n\nDFFITS is not discussed in Sleuth"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#cooks-distance",
    "href": "slides/10-mlr-diagnostics.html#cooks-distance",
    "title": "Multiple Regression Diagnostics",
    "section": "Cook‚Äôs distance",
    "text": "Cook‚Äôs distance\nMeasures effect the ith case has on all of the fitted values\n\\[D_i= \\sum_{j=1}^n\\frac{ \\left(\\widehat{y}_{j(i)} - \\widehat{y}_j \\right)^2}{(p+1) \\widehat{\\sigma}} = \\frac{r_i^2}{p+1} \\left( \\frac{h_i}{1-h_i} \\right)\\]\nwhere \\(\\widehat{y}_{j(i)}\\) is the fitted value for observation j based on a model when observation i is omitted\n\nCutoff\n\n\\(D_i\\) near or above 1 indicates large influence\nFind what quantile of \\(F_{p+1, n-p-1}\\) \\(D_i\\) corresponds to, larger than 0.5 is cause for concern\nAlso can judge relative standing of \\(D_i\\) ‚Äî make an index plot"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#cooks-distance-1",
    "href": "slides/10-mlr-diagnostics.html#cooks-distance-1",
    "title": "Multiple Regression Diagnostics",
    "section": "Cook‚Äôs distance",
    "text": "Cook‚Äôs distance"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#do-we-calculate-these-by-hand",
    "href": "slides/10-mlr-diagnostics.html#do-we-calculate-these-by-hand",
    "title": "Multiple Regression Diagnostics",
    "section": "Do we calculate these by hand?",
    "text": "Do we calculate these by hand?\nNo!\n\naugment() from the broom package\ninfluence.measures() from the car package"
  },
  {
    "objectID": "slides/10-mlr-diagnostics.html#advice-from-sleuth",
    "href": "slides/10-mlr-diagnostics.html#advice-from-sleuth",
    "title": "Multiple Regression Diagnostics",
    "section": "Advice from Sleuth",
    "text": "Advice from Sleuth\n\n\nSource: Display 11.8 in Sleuth (3rd edition)"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#fev-example",
    "href": "slides/09-mlr-inference2.html#fev-example",
    "title": "Additional Inferential Tools for MLR",
    "section": "FEV example",
    "text": "FEV example"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#fev-example-1",
    "href": "slides/09-mlr-inference2.html#fev-example-1",
    "title": "Additional Inferential Tools for MLR",
    "section": "FEV example",
    "text": "FEV example\n\\[{\\rm E}(Y|X) = \\beta_0 + \\beta_1 {\\tt age} + \\beta_2 {\\tt smoke} + \\beta_3 {\\tt age \\times smoke}\\] \n\nMean functions for each group\n\n\n\n\nSmokers:\n\n\\({\\rm E}(Y|X) = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) {\\tt age}\\)\n\n\n\n\n\nNon-smokers:\n\n\\({\\rm E}(Y|X) = \\beta_0 + \\beta_1  {\\tt age}\\)"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#ses-for-linear-combinations",
    "href": "slides/09-mlr-inference2.html#ses-for-linear-combinations",
    "title": "Additional Inferential Tools for MLR",
    "section": "SEs for linear combinations",
    "text": "SEs for linear combinations\nEstimate:\n\\[\\widehat{\\gamma} = c_1 \\widehat{\\beta}_i + c_2 \\widehat{\\beta}_j\\]\n\n\nStandard error:\n\n\\[\nSE(\\widehat{\\gamma}) = \\sqrt{c_1^2 \\lbrace SE(\\widehat{\\beta}_i)\\rbrace^2 + c_2^2 \\lbrace SE(\\widehat{\\beta}_j) \\rbrace^2 + 2c_1c_2 Cov(\\widehat{\\beta}_i, \\widehat{\\beta}_j)}\n\\]"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#fev-example-2",
    "href": "slides/09-mlr-inference2.html#fev-example-2",
    "title": "Additional Inferential Tools for MLR",
    "section": "FEV example",
    "text": "FEV example\n\n\n\n\n\n\nWarning\n\n\nOur standard table of coefficients doesn‚Äôt contain the SEs we need to construct CIs or perform tests for these linear combinations.\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.675\n0.251\n2.684\n0.008\n\n\nAge\n0.210\n0.021\n9.972\n&lt;0.001\n\n\nSmokeSmoker\n1.720\n0.563\n3.053\n0.002\n\n\nAge:SmokeSmoker\n‚àí0.143\n0.042\n‚àí3.397\n&lt;0.001"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#covariance-matrix",
    "href": "slides/09-mlr-inference2.html#covariance-matrix",
    "title": "Additional Inferential Tools for MLR",
    "section": "Covariance matrix",
    "text": "Covariance matrix\n\nThe diagonal (top left to lower right) displays variances, \\(\\lbrace SE(\\widehat{\\beta}_j) \\rbrace^2\\)\nThe off-diagonals display covariances, \\(Cov(\\widehat{\\beta}_i, \\widehat{\\beta}_j)\\)\nCovariance is like correlation, but not scaled to be between \\(-1\\) and \\(1\\)\n\n\n\n                (Intercept)        Age SmokeSmoker Age:SmokeSmoker\n(Intercept)        0.063199 -0.0052210   -0.063199       0.0052210\nAge               -0.005221  0.0004423    0.005221      -0.0004423\nSmokeSmoker       -0.063199  0.0052210    0.317248      -0.0234032\nAge:SmokeSmoker    0.005221 -0.0004423   -0.023403       0.0017798"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#se-for-a-linear-combination",
    "href": "slides/09-mlr-inference2.html#se-for-a-linear-combination",
    "title": "Additional Inferential Tools for MLR",
    "section": "SE for a linear combination",
    "text": "SE for a linear combination\n\nSmokers: \\({\\rm E}(Y|X) = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) {\\tt age}\\)\nTarget: \\(\\widehat{\\gamma} = \\widehat{\\beta}_1 + \\widehat{\\beta}_3\\)\n\n\n\n                (Intercept)        Age SmokeSmoker Age:SmokeSmoker\n(Intercept)        0.063199 -0.0052210   -0.063199       0.0052210\nAge               -0.005221  0.0004423    0.005221      -0.0004423\nSmokeSmoker       -0.063199  0.0052210    0.317248      -0.0234032\nAge:SmokeSmoker    0.005221 -0.0004423   -0.023403       0.0017798\n\n\n\n\n\\[\nSE(\\widehat{\\gamma}) = \\sqrt{c_1^2 \\lbrace SE(\\widehat{\\beta}_1)\\rbrace^2 + c_2^2 \\lbrace SE(\\widehat{\\beta}_2) \\rbrace^2 + 2c_1c_2 Cov(\\widehat{\\beta}_1, \\widehat{\\beta}_2)}\n\\]\n\n\n\n\n\\(c_1 = 1\\), \\(c_2 = 1\\)\n\n\n\n\n\n\n\\(SE(\\widehat{\\beta_1})^2 = 0.0004423\\)\n\n\\(SE(\\widehat{\\beta_2})^2 = 0.0017798\\)\n\n\n\n\n\n\\(Cov(\\widehat{\\beta_1}, \\widehat{\\beta_2}) = -0.0004423\\)"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#ci-for-a-linear-combination",
    "href": "slides/09-mlr-inference2.html#ci-for-a-linear-combination",
    "title": "Additional Inferential Tools for MLR",
    "section": "CI for a linear combination",
    "text": "CI for a linear combination\nOnce we have \\(SE(\\widehat{\\gamma})\\), we can construct a CI as usual:\n\\[\n\\widehat{\\gamma} \\pm t^*_{n - (p+1)} \\cdot SE(\\widehat{\\gamma})\n\\]\n\n\\(\\widehat{\\gamma} = 0.0664\\)\n\n\n\\(SE(\\widehat{\\gamma}) = 0.0366\\)\n\n\n\\(df = n - (p+1) = 345 - 4 = 341\\)\n\n\n\\(t^*_{341} \\approx 1.649\\) for a 90% CI"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#test-for-a-linear-combination",
    "href": "slides/09-mlr-inference2.html#test-for-a-linear-combination",
    "title": "Additional Inferential Tools for MLR",
    "section": "Test for a linear combination",
    "text": "Test for a linear combination\nWe can also test hypotheses about linear combinations of coefficients using a t-test:\n\\(H_0: \\gamma = \\gamma_0\\) vs.¬†\\(H_a: \\gamma \\underset{&gt;}{\\overset{&lt;}{\\ne}} \\gamma_0\\)\n\\(t = \\dfrac{\\widehat{\\gamma} - \\gamma_0}{SE(\\widehat{\\gamma})}\\)\nReference distribution: \\(t_{n-(p+1)}\\)"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#example",
    "href": "slides/09-mlr-inference2.html#example",
    "title": "Additional Inferential Tools for MLR",
    "section": "Example",
    "text": "Example\n\nWork through the example on the handout with your neighbors\nBe prepared to share your strategy with the class"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#example-used-car-prices",
    "href": "slides/09-mlr-inference2.html#example-used-car-prices",
    "title": "Additional Inferential Tools for MLR",
    "section": "Example: Used car prices",
    "text": "Example: Used car prices\nWe have data on used car prices, including:\n\nPrice: Price (in dollars)\nMileage: Mileage (in thousands of miles)\nMake: Manufacturer (Buick, Cadillac, Chevrolet, Pontiac, SAAB, Saturn)\n\nAfter some initial EDA, researchers want to fit an MLR model to predict \\(\\log\\)(price) using mileage and make."
  },
  {
    "objectID": "slides/09-mlr-inference2.html#used-car-prices",
    "href": "slides/09-mlr-inference2.html#used-car-prices",
    "title": "Additional Inferential Tools for MLR",
    "section": "Used car prices",
    "text": "Used car prices\nIs Make a useful predictor of price, after accounting for Mileage?\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.090\n0.034\n293.975\n&lt;0.001\n\n\nMileage\n0.000\n0.000\n‚àí7.359\n&lt;0.001\n\n\nMakeCadillac\n0.649\n0.038\n17.016\n&lt;0.001\n\n\nMakeChevrolet\n‚àí0.296\n0.030\n‚àí9.809\n&lt;0.001\n\n\nMakePontiac\n‚àí0.144\n0.033\n‚àí4.299\n&lt;0.001\n\n\nMakeSAAB\n0.355\n0.035\n10.108\n&lt;0.001\n\n\nMakeSaturn\n‚àí0.397\n0.041\n‚àí9.650\n&lt;0.001"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#review-nested-models",
    "href": "slides/09-mlr-inference2.html#review-nested-models",
    "title": "Additional Inferential Tools for MLR",
    "section": "Review: Nested models",
    "text": "Review: Nested models\n\nModel 1:  \\({\\rm E}(\\log(Y)|X) = \\beta_0 + \\beta_1 {\\tt Mileage}\\)\n\nModel 2: \n\\(\\begin{split}\n{\\rm E}(\\log(Y)|X) = \\beta_0 &+ \\beta_1 {\\tt Mileage} + \\beta_2 {\\tt Cadillac} + \\beta_3 {\\tt Chevy}\\\\\n&+ \\beta_4 {\\tt Pontiac} + \\beta_5 {\\tt SAAB} + \\beta_6 {\\tt Saturn}\n\\end{split}\\)\n\n\n\n\nExtra SS F-test:\nNull hypothesis tells us how we can simplify Model 2 (full) to get Model 1 (reduced):\n\\(H_0\\): \\(\\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = \\beta_6 = 0\\)"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#extra-sums-of-squares",
    "href": "slides/09-mlr-inference2.html#extra-sums-of-squares",
    "title": "Additional Inferential Tools for MLR",
    "section": "Extra Sums of Squares",
    "text": "Extra Sums of Squares\nDefinition: the marginal reduction in the error sum of squares when one (or more) explanatory variable(s) is added to the model\n\n\nF-test for comparing nested models:\n\\(F = \\dfrac{(SSE_{full} - SSE_{reduced}) / (df_{full} - df_{reduced})}{MSE_{full}}\\)"
  },
  {
    "objectID": "slides/09-mlr-inference2.html#anova-table-from-r",
    "href": "slides/09-mlr-inference2.html#anova-table-from-r",
    "title": "Additional Inferential Tools for MLR",
    "section": "ANOVA table from R",
    "text": "ANOVA table from R\n\nanova(car_lm) produces an ANOVA table for the MLR model, it gives sequential sums of squares, not the extra sums of squares we need for our F-test.\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nMileage\n1\n2.959\n2.959\n50.941\n&lt;0.001\n\n\nMake\n5\n85.802\n17.160\n295.414\n&lt;0.001\n\n\nResiduals\n797\n46.297\n0.058\nNA\nNA\n\n\n\n\n\n\n\n\n\nMileage row: extra SS for Mileage (compared to intercept-only model)\nMake row: extra SS for Make after accounting for Mileage\nResiduals row: SSE for the full model"
  },
  {
    "objectID": "slides/05-slr-transformations.html#easing-skew",
    "href": "slides/05-slr-transformations.html#easing-skew",
    "title": "Remedial Measures: Transformations",
    "section": "Easing skew",
    "text": "Easing skew\nIf a set of data values is skewed to the right, taking the (natural) log of each data value can result in a data set that is roughly symmetric and often roughly normal."
  },
  {
    "objectID": "slides/05-slr-transformations.html#section",
    "href": "slides/05-slr-transformations.html#section",
    "title": "Remedial Measures: Transformations",
    "section": "",
    "text": "How are tree height and tree diameter related for the western red cedar?"
  },
  {
    "objectID": "slides/05-slr-transformations.html#are-the-conditions-violated",
    "href": "slides/05-slr-transformations.html#are-the-conditions-violated",
    "title": "Remedial Measures: Transformations",
    "section": "Are the conditions violated?",
    "text": "Are the conditions violated?\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlinearity\nconstant errors\nindependent errors\n\n\n\nnormal errors\noutliers\nnone"
  },
  {
    "objectID": "slides/05-slr-transformations.html#does-transforming-x-help",
    "href": "slides/05-slr-transformations.html#does-transforming-x-help",
    "title": "Remedial Measures: Transformations",
    "section": "Does transforming X help?",
    "text": "Does transforming X help?"
  },
  {
    "objectID": "slides/05-slr-transformations.html#does-transforming-y-help",
    "href": "slides/05-slr-transformations.html#does-transforming-y-help",
    "title": "Remedial Measures: Transformations",
    "section": "Does transforming Y help?",
    "text": "Does transforming Y help?"
  },
  {
    "objectID": "slides/05-slr-transformations.html#transforming-both-x-and-y",
    "href": "slides/05-slr-transformations.html#transforming-both-x-and-y",
    "title": "Remedial Measures: Transformations",
    "section": "Transforming both X and Y?",
    "text": "Transforming both X and Y?"
  },
  {
    "objectID": "slides/05-slr-transformations.html#your-turn",
    "href": "slides/05-slr-transformations.html#your-turn",
    "title": "Remedial Measures: Transformations",
    "section": "Your turn",
    "text": "Your turn\n\nWork through the first example on the handout with your neighbor(s)\nOnline version with R chunks:"
  },
  {
    "objectID": "slides/05-slr-transformations.html#back-transforming-1",
    "href": "slides/05-slr-transformations.html#back-transforming-1",
    "title": "Remedial Measures: Transformations",
    "section": "Back-Transforming",
    "text": "Back-Transforming\n\n\nLog scale\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\n\n\nmean\n\n\n\n\n2.146\n\n\n\n\n\n\nOriginal scale\n\n\n\n\n\nmean\n\n\n\n\n98.558\n\n\n\n\n\n\n\n\nBack-transformed mean: \\(e^{2.146} \\approx 8.55\\)\n\n\n\n\n\n\n\n\nR Note\n\n\n\nlog is the natural log\nexp(x) calculated \\(e^x\\)"
  },
  {
    "objectID": "slides/05-slr-transformations.html#displaying-a-transformed-model",
    "href": "slides/05-slr-transformations.html#displaying-a-transformed-model",
    "title": "Remedial Measures: Transformations",
    "section": "Displaying a transformed model",
    "text": "Displaying a transformed model\n\nWarning: Using the `size` aesthetic with geom_ribbon was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\n\n\n\n95% confidence intervals for the mean response are displayed"
  },
  {
    "objectID": "slides/05-slr-transformations.html#rules-of-thumb",
    "href": "slides/05-slr-transformations.html#rules-of-thumb",
    "title": "Remedial Measures: Transformations",
    "section": "Rules of thumb",
    "text": "Rules of thumb\n\ntransform x: if mean function is nonlinear, but is monotonic and the residual variance is constant\ntransform y: if mean function is nonlinear and the residual variance increases as the mean increases (log, reciprocal, or square root often work)\nlog rule: if values range over more than 1 order of magnitude and are strictly positive, then the natural log is likely helpful\nrange rule: if the range is considerably less than 1 order of magnitude, then transformations are unlikely to help\nsquare roots are useful for count data"
  },
  {
    "objectID": "slides/05-slr-transformations.html#ladder-of-transformations",
    "href": "slides/05-slr-transformations.html#ladder-of-transformations",
    "title": "Remedial Measures: Transformations",
    "section": "Ladder of transformations",
    "text": "Ladder of transformations\n\n\nImage credit: Andrew Zieffler"
  },
  {
    "objectID": "slides/05-slr-transformations.html#rule-of-the-bulge",
    "href": "slides/05-slr-transformations.html#rule-of-the-bulge",
    "title": "Remedial Measures: Transformations",
    "section": "Rule of the Bulge",
    "text": "Rule of the Bulge\nIntroduced by John Tukey and Frederick Mosteller for ‚Äústraightening‚Äù data to better meet the assumption of linearity\n\n\nImage credit: Andrew Zieffler"
  },
  {
    "objectID": "slides/05-slr-transformations.html#back-transforming-2",
    "href": "slides/05-slr-transformations.html#back-transforming-2",
    "title": "Remedial Measures: Transformations",
    "section": "Back-Transforming",
    "text": "Back-Transforming\n\n\nLog scale\n\n\n\n\n\nmean\nmedian\n\n\n\n\n2.146\n1.933\n\n\n\n\n\n\nOriginal scale\n\n\n\n\n\nmean\nmedian\n\n\n\n\n98.558\n6.925\n\n\n\n\n\n\n\n\nBack-transformed median: \\(e^{1.933} \\approx 6.91\\)\nBack-transformed mean: \\(e^{2.146} \\approx 8.55\\)"
  },
  {
    "objectID": "slides/05-slr-transformations.html#back-transforming-log-transformations",
    "href": "slides/05-slr-transformations.html#back-transforming-log-transformations",
    "title": "Remedial Measures: Transformations",
    "section": "Back-transforming log transformations",
    "text": "Back-transforming log transformations\n\nOften log-transforming a variable makes in approximately symmetric\nIf symmetric, then the median \\(\\approx\\) mean on the log scale\n\\(\\widehat{\\mu}(Y|X) \\approx \\widehat{\\text{median}}(Y|X)\\)\nInference made mean on the log scale can thought of as inference for the median on the log scale"
  },
  {
    "objectID": "slides/05-slr-transformations.html#log-transform-of-y-only",
    "href": "slides/05-slr-transformations.html#log-transform-of-y-only",
    "title": "Remedial Measures: Transformations",
    "section": "Log-transform of Y only",
    "text": "Log-transform of Y only\n\n\nThe median of \\(Y\\) at \\(x + 1\\) is \\(e^\\beta_1\\) times larger (smaller) than the median of \\(Y\\) at \\(x\\).\n\nOr‚Ä¶ increasing \\(x\\) by 1 increases (decreases) the median of \\(Y\\) by a factor of \\(e^\\beta_1\\)."
  },
  {
    "objectID": "slides/05-slr-transformations.html#log-transform-of-x-only",
    "href": "slides/05-slr-transformations.html#log-transform-of-x-only",
    "title": "Remedial Measures: Transformations",
    "section": "Log-transform of X only",
    "text": "Log-transform of X only\n\n\nA doubling of \\(x\\) is associated with the mean response increasing (decreasing) by \\(\\beta_1 \\log(2)\\) units."
  },
  {
    "objectID": "slides/05-slr-transformations.html#log-transform-both-y-and-x",
    "href": "slides/05-slr-transformations.html#log-transform-both-y-and-x",
    "title": "Remedial Measures: Transformations",
    "section": "Log-transform both Y and X",
    "text": "Log-transform both Y and X\n\n\nThe median of \\(Y\\) at \\(2x\\) is \\(2^{\\beta_1}\\) times greater (smaller) than the median of \\(Y\\) at \\(x\\).\n\nOr‚Ä¶ A doubling of x is associated with the median of Y increasing (decreasing) by a factor of \\(2^{\\beta_1}\\)."
  },
  {
    "objectID": "slides/05-slr-transformations.html#your-turn-1",
    "href": "slides/05-slr-transformations.html#your-turn-1",
    "title": "Remedial Measures: Transformations",
    "section": "Your turn",
    "text": "Your turn\nYou estimated the model \\(\\mu(\\mathtt{brain\\ weight}|\\mathtt{body \\ weight}) = \\beta_0 + \\beta_1 x\\)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.190\n0.176\n12.439\n&lt;0.001\n\n\nlog(bodyweight)\n0.759\n0.042\n18.163\n&lt;0.001\n\n\n\n\n\n\n\n\nInterpret the slope in context\nInterpret the intercept in context"
  },
  {
    "objectID": "slides/05-slr-transformations.html#modeling-is-an-iterative-process",
    "href": "slides/05-slr-transformations.html#modeling-is-an-iterative-process",
    "title": "Remedial Measures: Transformations",
    "section": "Modeling is an iterative process",
    "text": "Modeling is an iterative process"
  },
  {
    "objectID": "slides/05-slr-transformations.html#issues-with-transformations",
    "href": "slides/05-slr-transformations.html#issues-with-transformations",
    "title": "Remedial Measures: Transformations",
    "section": "Issues with transformations",
    "text": "Issues with transformations\n\nYou‚Äôre often guessing ‚Äî Statistics is an art AND a science! \nChanges the interpretation of the parameters ‚Äî need to back-transform to provide interpretable results \nChanges SEs of the parameters \nNot always easy to keep track of all your assumptions"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#welcome",
    "href": "slides/01-slides-slr-review.html#welcome",
    "title": "Welcome and Review",
    "section": "Welcome üëã",
    "text": "Welcome üëã\n\nI‚Äôm Adam (he/him)\nI teach statistics & data science\nI‚Äôm interested in statistics education, data visualization, and R programming"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#your-turn",
    "href": "slides/01-slides-slr-review.html#your-turn",
    "title": "Welcome and Review",
    "section": "Your turn",
    "text": "Your turn\n\nForm groups based on value (2, 3, 4‚Ä¶) of the card dealt\nIntroduce yourself to your group\nShare at least one thing other than just your name and major"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#regression-in-intro",
    "href": "slides/01-slides-slr-review.html#regression-in-intro",
    "title": "Welcome and Review",
    "section": "Regression in intro",
    "text": "Regression in intro\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescribe the scatterplot\nWrite the equation of the regression line\nInterpret the slope and intercept\nMake a prediction"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#your-turn-1",
    "href": "slides/01-slides-slr-review.html#your-turn-1",
    "title": "Welcome and Review",
    "section": "Your turn",
    "text": "Your turn\n\nWork with your group\nIf your card‚Äôs suit is clubs, you‚Äôre the designated speaker for your group (be ready!)\nWork through the review questions\nWrite your group‚Äôs answer to each question on the whiteboard"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#regression-in-stat-230",
    "href": "slides/01-slides-slr-review.html#regression-in-stat-230",
    "title": "Welcome and Review",
    "section": "Regression in Stat 230",
    "text": "Regression in Stat 230\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) where \\(\\varepsilon_i \\overset{\\rm iid}{\\sim} N (0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#simple-linear-regression-model",
    "href": "slides/01-slides-slr-review.html#simple-linear-regression-model",
    "title": "Welcome and Review",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) where \\(\\varepsilon_i \\overset{\\rm iid}{\\sim} N (0, \\sigma^2)\\)\n\n\nLinear relationship between \\(x\\) and \\(y\\)\nErrors are independent and identically distributed (iid)\nErrors are normally distributed\nErrors have mean 0\nVariance of the errors doesn‚Äôt depend on \\(x\\)"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#notation",
    "href": "slides/01-slides-slr-review.html#notation",
    "title": "Welcome and Review",
    "section": "Notation",
    "text": "Notation\nMean function:\n\\(\\quad E(Y|X)=\\mu\\{Y|X\\} = \\beta_0 + \\beta_1 x_i\\)\n\nFitted model equation:\n\\(\\quad \\widehat{y}_i = \\widehat{\\mu}\\{Y|X\\} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\)"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#overview-of-the-term",
    "href": "slides/01-slides-slr-review.html#overview-of-the-term",
    "title": "Welcome and Review",
    "section": "Overview of the term",
    "text": "Overview of the term\nGoal: Develop models to answer research questions\n\n\n\n\n\ntimeline\n%%{init: {'theme':'neutral'}}%%\n    \n    Weeks 1-2 : Simple linear regression\n           : Review\n           : Inference\n           : Model diagnostics\n           \n    Weeks 3-5 : Multiple linear regression\n           : Interpretation\n           : Inference\n           : Model diagnostics\n           : Model selection\n           \n    Weeks 6-8 : Logistic regression\n           : Interpretation\n           : Inference\n           : Model diagnostics\n           : Model selection\n           \n    Weeks 9-10 : Poisson regression\n           : Interpretation\n           : Inference"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#typical-week",
    "href": "slides/01-slides-slr-review.html#typical-week",
    "title": "Welcome and Review",
    "section": "Typical week",
    "text": "Typical week\n\n\n\nMonday\n\nPre-class reading/video\nReflection questions\n\n\n\nClass meeting\nWork on problems\n\n\n\nWednesday\n\nPre-class reading/video\nReflection questions\n\n\n\nClass meeting\nWork on problems\n\n\n\nFriday\n\nPre-class reading/video\nReflection questions\nHomework due by the start of class\nClass meeting\nWork on problems"
  },
  {
    "objectID": "slides/01-slides-slr-review.html#tools",
    "href": "slides/01-slides-slr-review.html#tools",
    "title": "Welcome and Review",
    "section": "Tools",
    "text": "Tools\n\n\nMoodle\n \nGradescope\n\nR and RStudio\n\nR Markdown\n\nClass website; look here for all materials, links, etc.\n\nSubmit assignments, get feedback\n\nOur computational engine\n\nDynamic documents for assignments, case studies, projects"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#wildfires",
    "href": "slides/06-mlr-polynomial.html#wildfires",
    "title": "Polynomial Regression",
    "section": "Wildfires",
    "text": "Wildfires\n\nThe National Interagency Coordination Center at the National Interagency Coordination Center compiles annual wildland fire statistics for federal and state agencies.\nThis information is provided through Situation Reports, which have been in use for several decades.\nOur goal is to model the number of acres burned over the years\n\n\nData source: https://www.nifc.gov/fireInfo/fireInfo_stats_totalFires.html"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#option-1-slr-model",
    "href": "slides/06-mlr-polynomial.html#option-1-slr-model",
    "title": "Polynomial Regression",
    "section": "Option 1: SLR model",
    "text": "Option 1: SLR model\n\\(\\mu \\lbrace y | x \\rbrace = \\beta_0 + \\beta_1 x\\)\nIs the fit reasonable?"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#option-2-transform-x",
    "href": "slides/06-mlr-polynomial.html#option-2-transform-x",
    "title": "Polynomial Regression",
    "section": "Option 2: Transform X",
    "text": "Option 2: Transform X\n\\(\\mu \\lbrace y | x \\rbrace = \\beta_0 + \\beta_1 x^2\\)\nIs the fit reasonable?"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#option-3-polynomial-model",
    "href": "slides/06-mlr-polynomial.html#option-3-polynomial-model",
    "title": "Polynomial Regression",
    "section": "Option 3: Polynomial model",
    "text": "Option 3: Polynomial model\n\\(\\mu \\lbrace y | x \\rbrace = \\beta_0 + \\beta_1 x + \\beta_2x^2\\)\nIs the fit reasonable?"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#the-polynomial-regression-model",
    "href": "slides/06-mlr-polynomial.html#the-polynomial-regression-model",
    "title": "Polynomial Regression",
    "section": "The polynomial regression model",
    "text": "The polynomial regression model\n\\[Y_i = \\beta_0 + \\beta_1 x_{i} + \\beta_2 x_{i}^2 + \\cdots + \\beta_k x_{i}^k + \\varepsilon_i, \\quad \\varepsilon_i \\overset{iid}{\\sim} N(0, \\sigma)\\]\nAssumptions ‚Äî same as in SLR\n\n\\(\\mu\\{Y|x_i\\}\\), is a linear function\nFor each \\(x_i\\), the sub-population of responses is normally distributed\nThe standard deviation for each sub-population is \\(\\sigma\\)\nIndependent observations"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#interpreting-the-model",
    "href": "slides/06-mlr-polynomial.html#interpreting-the-model",
    "title": "Polynomial Regression",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\n\n\n\n\n\n\n\n\n\n\n\nFocus on the expected change in \\(y\\) for a specific one-unit increase in \\(x\\)\n\ne.g.¬†change in acres burned from 1985 to 1990\ne.g.¬†change in acres burned from 2005 to 2010\n\n\n\n\\[\\begin{aligned}\n\\mu\\{ y | x + 1 \\} - \\mu\\{ y | x \\}\n  &=  \\left[ \\beta_0 + \\beta_1 (x+1) + \\beta_2 (x+1)^2 \\right] - \\left[ \\beta_0 + \\beta_1 x + \\beta_2 x^2 \\right]\\\\\n  &= \\beta_1 + \\beta_2 \\left( 2x + 1 \\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#inferences-about-coefficients",
    "href": "slides/06-mlr-polynomial.html#inferences-about-coefficients",
    "title": "Polynomial Regression",
    "section": "Inferences about coefficients",
    "text": "Inferences about coefficients\nInference uses the same t-based tools as SLR, but with\n\\[\\widehat{\\sigma}  = \\sqrt{\\dfrac{{\\sum_{i=1}^n e_i^2}}{n - (k+1)}}\\]\ni.e.¬†the degrees of freedom of the t-distribution change"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#testing-a-single-coefficient",
    "href": "slides/06-mlr-polynomial.html#testing-a-single-coefficient",
    "title": "Polynomial Regression",
    "section": "Testing a single coefficient",
    "text": "Testing a single coefficient\n\nHypotheses: \\(H_0: \\ \\beta_j = \\#\\) vs.¬†\\(H_a: \\ \\beta_j \\underset{&gt;}{\\overset{&lt;}{\\ne}} \\#\\)\n\n\n\nTest statistic: \\(t = \\dfrac{\\hat{\\beta}_j - \\#}{SE(\\beta_j)}\\)\n\n\n\n\nReference distribution: \\(t\\) distribution with d.f. = \\(n-(k+1)\\)\np-value: Area in the tail(s) specified by \\(H_a\\)"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#cis-for-a-single-coefficient",
    "href": "slides/06-mlr-polynomial.html#cis-for-a-single-coefficient",
    "title": "Polynomial Regression",
    "section": "CIs for a single coefficient",
    "text": "CIs for a single coefficient\n\\(\\widehat{\\beta}_j \\pm t^*_{n-(k+1)} \\cdot SE(\\widehat{\\beta}_j )\\)\n\n\nWildfires example\n\n\n\n\nTerm\nEstimate\nSE\nLower\nUpper\n\n\n\n\n(Intercept)\n16109806404\n3793719340\n8510073345\n23709539462\n\n\nYear\n-16264428\n3814896\n-23906583\n-8622273\n\n\nI(Year^2)\n4106\n959\n2185\n6027"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#your-turn",
    "href": "slides/06-mlr-polynomial.html#your-turn",
    "title": "Polynomial Regression",
    "section": "Your turn",
    "text": "Your turn\nWould a higher-order polynomial (e.g.¬†cubic, quartic, quintic) provide a better fit to the wildfire data?\n\nWork through that example on the handout with your neighbors"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#a-warning-about-this-analysis",
    "href": "slides/06-mlr-polynomial.html#a-warning-about-this-analysis",
    "title": "Polynomial Regression",
    "section": "A warning about this analysis",
    "text": "A warning about this analysis\n\nPrior to 1983, sources of these figures are not known, or cannot be confirmed, and were not derived from the current situation reporting process. As a result the figures prior to 1983 should not be compared to later data.\n‚Äî NIFC\n\n\nhttps://www.nifc.gov/fireInfo/fireInfo_stats_totalFires.html]"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#a-warning-about-polynomials",
    "href": "slides/06-mlr-polynomial.html#a-warning-about-polynomials",
    "title": "Polynomial Regression",
    "section": "A warning about polynomials",
    "text": "A warning about polynomials\nHigh-order polynomial regression models will over-fit your data (i.e.¬†pick up on peculiarities specific to your one sample from the population)"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#multiple-linear-regression",
    "href": "slides/06-mlr-polynomial.html#multiple-linear-regression",
    "title": "Polynomial Regression",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nPolynomial regression is one example of the multiple regression model, but there are numerous ways to incorporate multiple predictors into a model\n\\(\\mu \\lbrace Y | X \\rbrace = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\n\n\\(\\mu \\lbrace Y | X \\rbrace = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\)\n\n\n\\(\\mu \\lbrace Y | X \\rbrace = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_2 x_1 x_2\\)"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#the-null-model",
    "href": "slides/06-mlr-polynomial.html#the-null-model",
    "title": "Polynomial Regression",
    "section": "The ‚Äúnull‚Äù model",
    "text": "The ‚Äúnull‚Äù model\n\nUse the mean of Y as the prediction for all observations\n\\(\\mu \\lbrace Y | X \\rbrace = \\beta_0\\)\nLeaves a lot of variability unexplained\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(SD(Y) = 2.465548\\times 10^{6}\\) cm"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#polynomial-model",
    "href": "slides/06-mlr-polynomial.html#polynomial-model",
    "title": "Polynomial Regression",
    "section": "Polynomial model",
    "text": "Polynomial model\n\n\\(\\mu \\lbrace Y | X \\rbrace = \\beta_0 + \\beta_1x + \\beta_2 x^2\\)\nUsing a predictor explains more variability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(SD(Y) = 1.9098377\\times 10^{6}\\) cm"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#sstotal",
    "href": "slides/06-mlr-polynomial.html#sstotal",
    "title": "Polynomial Regression",
    "section": "SSTotal",
    "text": "SSTotal\n\n\n\\({\\rm SSTotal} = \\sum(Y_i - \\bar{Y})^2\\)\nMeasures the overall variability in \\(Y\\)"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#ssresidual-aka-sserror",
    "href": "slides/06-mlr-polynomial.html#ssresidual-aka-sserror",
    "title": "Polynomial Regression",
    "section": "SSResidual (aka SSError)",
    "text": "SSResidual (aka SSError)\n\n\n\\({\\rm SSResidual} = \\sum(Y_i - \\widehat{Y}_i)^2\\)\nMeasures the variability unexplained by the model"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#ssregression",
    "href": "slides/06-mlr-polynomial.html#ssregression",
    "title": "Polynomial Regression",
    "section": "SSRegression",
    "text": "SSRegression\nMeasures the variability explained by the model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\({\\rm SSRegression} = SSTotal - SSError\\)"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#coefficient-of-determination-r2",
    "href": "slides/06-mlr-polynomial.html#coefficient-of-determination-r2",
    "title": "Polynomial Regression",
    "section": "Coefficient of Determination: R2",
    "text": "Coefficient of Determination: R2\nProportion of the total variation in \\(y\\) explained by the linear regression model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\begin{split}\nR^2 &= 0.421\\\\\n&= \\dfrac{SSRegr}{SST} \\\\\n&= 1 - \\dfrac{SSE}{SST}\n\\end{split}\\)"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#caution",
    "href": "slides/06-mlr-polynomial.html#caution",
    "title": "Polynomial Regression",
    "section": "‚ö†Ô∏è Caution",
    "text": "‚ö†Ô∏è Caution\n\\(R^2\\) only addresses how close the fitted values are to the data, on average. It says nothing about the validity of the model."
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#wildfires-example-1",
    "href": "slides/06-mlr-polynomial.html#wildfires-example-1",
    "title": "Polynomial Regression",
    "section": "Wildfires example",
    "text": "Wildfires example\nSuppose we wish to compare a quadratic and quartic model for the wildfires data set:\n\nQuadratic: \\(\\mu \\lbrace Y | X \\rbrace = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nQuartic: \\(\\mu \\lbrace Y | X \\rbrace = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4\\)\n\nHow do we decide which model is preferred?"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#comparing-models-1",
    "href": "slides/06-mlr-polynomial.html#comparing-models-1",
    "title": "Polynomial Regression",
    "section": "Comparing models",
    "text": "Comparing models\nCan we compare \\(R^2\\) values?\n\nNo! More complex models will always have a higher \\(R^2\\) value, even if the additional predictors are not useful.\n\nCan we run individual t-tests?\n\nNo! The tests are not necessarily independent, and the Type I error rate will be inflated."
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#comparing-models-with-an-f-test",
    "href": "slides/06-mlr-polynomial.html#comparing-models-with-an-f-test",
    "title": "Polynomial Regression",
    "section": "Comparing models with an F-test",
    "text": "Comparing models with an F-test\n\n\nFull model\n\n\\(\\mu \\lbrace Y | X \\rbrace = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\beta_4 x^4\\)\n\nReduced model\n\n\\(\\mu \\lbrace Y | X \\rbrace = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\n\n\n\n\n\nHypotheses\n\n\\(H_0:\\ \\beta_{3} = \\beta_4 = 0\\)\n\\(H_a:\\) at least one \\(\\beta_j \\ne 0\\), \\(j=3,4\\)"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#comparing-models-with-an-f-test-1",
    "href": "slides/06-mlr-polynomial.html#comparing-models-with-an-f-test-1",
    "title": "Polynomial Regression",
    "section": "Comparing models with an F-test",
    "text": "Comparing models with an F-test\nTest statistic\n\\[\n\\begin{split}\nF &= \\frac{(R^2_{\\text{full}} - R^2_{\\text{reduced}}) / d}{(1 - R^2_{\\text{full}}) / df_{\\text{full}}}\\\\\n&= \\frac{(\\text{SSR}_{\\text{full}} - \\text{SSR}_{\\text{reduced}}) / d}{\\text{MSE}_{\\text{full}}}\\\\\n&= \\frac{(\\text{SSE}_{\\text{reduced}} - \\text{SSE}_{\\text{full}}) / d}{\\text{MSE}_{\\text{full}}}\n\\end{split}\n\\]\nwhere\n\n\\(d = \\text{df}_{full} - \\text{df}_{reduced}\\) = # betas being tested\n\\(\\text{df}_{i} = n - (p + 1)=\\) error d.f. for model \\(i\\)\n\\(\\text{MSE}_{\\text{full}} = \\dfrac{\\text{SSE}_{\\text{full}}}{\\text{df}_{\\text{full}}}\\)"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#f-distribution",
    "href": "slides/06-mlr-polynomial.html#f-distribution",
    "title": "Polynomial Regression",
    "section": "F distribution",
    "text": "F distribution\nThe F-statistics follows an \\(F\\) distribution with \\(\\text{df}_{full} - \\text{df}_{reduced}\\) and \\(n-p-1\\) d.f."
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#upper-tail-p-values",
    "href": "slides/06-mlr-polynomial.html#upper-tail-p-values",
    "title": "Polynomial Regression",
    "section": "Upper-tail p-values",
    "text": "Upper-tail p-values\n\nTo obtain upper-tail areas: 1 - pf(stat, df1, df2)"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#putting-it-all-together",
    "href": "slides/06-mlr-polynomial.html#putting-it-all-together",
    "title": "Polynomial Regression",
    "section": "Putting it all together",
    "text": "Putting it all together\n\n\n\n\n\nmodel\nr.squared\ndf\ndf.residual\nnobs\n\n\n\n\nFull\n0.453\n4\n54\n59\n\n\nReduced\n0.421\n2\n56\n59\n\n\n\n\n\n\n\\[\nF = \\frac{(R^2_{\\text{full}} - R^2_{\\text{reduced}}) / d}{(1 - R^2_{\\text{full}}) / df_{\\text{full}}} = \\frac{(0.453 - 0.421) / 2}{(1 - 0.453) / 54} \\approx 1.58\n\\]\n\n\n\n1 - pf(1.58, 2, 54)\n\n[1] 0.2153484\n\n\n\n\nThere is no evidence that the quartic model is an improvement over the quadratic model (\\(F=4.467\\), \\(df = 2, 54\\), \\(p = 0.215\\))."
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#reading-r-output",
    "href": "slides/06-mlr-polynomial.html#reading-r-output",
    "title": "Polynomial Regression",
    "section": "Reading R output",
    "text": "Reading R output\nCall:\nlm(formula = Acres ~ poly(Year, 4), data = wildfires)\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     4641410     245955  18.871  &lt; 2e-16 ***\npoly(Year, 4)1  9025270    1889218   4.777 1.40e-05 ***\npoly(Year, 4)2  8177004    1889218   4.328 6.55e-05 ***\npoly(Year, 4)3 -1194695    1889218  -0.632   0.5298    \npoly(Year, 4)4 -3177711    1889218  -1.682   0.0983 .  \n\nResidual standard error: 1889000 on 54 degrees of freedom\nMultiple R-squared:  0.4534,    Adjusted R-squared:  0.4129 \nF-statistic:  11.2 on 4 and 54 DF,  p-value: 1.096e-06\n\nOutput was edited to fit on one slide"
  },
  {
    "objectID": "slides/06-mlr-polynomial.html#extra-sums-of-squares-f-test-in-r",
    "href": "slides/06-mlr-polynomial.html#extra-sums-of-squares-f-test-in-r",
    "title": "Polynomial Regression",
    "section": "Extra sums of squares F-test in R",
    "text": "Extra sums of squares F-test in R\n\n1full &lt;- lm(Acres ~ poly(Year, 4), data = wildfires)\n2reduced &lt;- lm(Acres ~ poly(Year, 2), data = wildfires)\n3anova(reduced, full)\n\n\n1\n\nFit the full model\n\n2\n\nFit the reduced model\n\n3\n\nUse anova() to compare the two models. The first argument should be the reduced model.\n\n\n\n\nAnalysis of Variance Table\n\nModel 1: Acres ~ poly(Year, 2)\nModel 2: Acres ~ poly(Year, 4)\n  Res.Df        RSS Df  Sum of Sq      F Pr(&gt;F)\n1     56 2.0426e+14                            \n2     54 1.9273e+14  2 1.1525e+13 1.6146 0.2084"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Stat 230 Slide Decks",
    "section": "",
    "text": "All of the slide decks for Stat 230 are available here. You can filter the slide decks by topics or date using the filter box above. You can also click on any of the categories to see only slide decks in that category.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMLR Model Validation\n\n\n\nMLR\n\n\n\n\nOct 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLR Model Selection\n\n\n\nMLR\n\n\n\n\nOct 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulticollinearity\n\n\n\nMLR\n\n\n\n\nOct 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLR Diagnostics\n\n\n\nMLR\n\n\n\n\nOct 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical Predictor Variables\n\n\n\nMLR\n\n\n\n\nOct 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical Predictor Variables\n\n\n\nMLR\n\n\n\n\nOct 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Regression\n\n\n\nMLR\n\n\n\n\nOct 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression\n\n\n\nMLR\n\nPolynomial\n\n\n\n\nSep 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSLR Transformations\n\n\n\nSLR\n\nTransformations\n\n\n\n\nSep 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSLR Diagnostics and Transformations\n\n\n\nSLR\n\nDiagnostics\n\nTransformations\n\n\n\n\nSep 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference for SLR Prediction\n\n\n\nSLR\n\nInference\n\n\n\n\nSep 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference for SLR Coefficients\n\n\n\nSLR\n\nInference\n\n\n\n\nSep 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nCourse overview, SLR review\n\n\n\nSLR\n\n\n\n\nSep 15, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "Most materials for Stat 230 will be posted on Moodle, but some materials will also be hosted on this site for easier access. Everything here will also be linked on Moodle, but sometimes it‚Äôs easier to search through materials on this page rather than on Moodle.\n\nSlide decks"
  },
  {
    "objectID": "slides/02-slr-inference.html#warm-up",
    "href": "slides/02-slr-inference.html#warm-up",
    "title": "Inference for SLR",
    "section": "Warm up",
    "text": "Warm up\n\nWork through the warm-up questions with your group\nI‚Äôll ask half of the groups to start with hypothesis tests, the other half will start with confidence intervals"
  },
  {
    "objectID": "slides/02-slr-inference.html#example",
    "href": "slides/02-slr-inference.html#example",
    "title": "Inference for SLR",
    "section": "Example",
    "text": "Example\n\nA biologist collected data on the body measurements of captured blue jays\nLet‚Äôs explore the association between body mass and head length"
  },
  {
    "objectID": "slides/02-slr-inference.html#a-deterministic-model",
    "href": "slides/02-slr-inference.html#a-deterministic-model",
    "title": "Inference for SLR",
    "section": "A deterministic model",
    "text": "A deterministic model\n\n\\(Y_i = \\beta_0 + \\beta_1 x_i\\)\nNo uncertainty!"
  },
  {
    "objectID": "slides/02-slr-inference.html#simple-linear-regression-model",
    "href": "slides/02-slr-inference.html#simple-linear-regression-model",
    "title": "Inference for SLR",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) where \\(\\varepsilon_i \\overset{\\rm iid}{\\sim} N (0, \\sigma^2)\\)\n\n\nLinear relationship between \\(x\\) and \\(y\\)\nErrors are independent and identically distributed (iid)\nErrors are normally distributed\nErrors have mean 0\nVariance of the errors doesn‚Äôt depend on \\(x\\)"
  },
  {
    "objectID": "slides/02-slr-inference.html#linear-regression-model",
    "href": "slides/02-slr-inference.html#linear-regression-model",
    "title": "Inference for SLR",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) where \\(\\varepsilon_i \\overset{\\rm iid}{\\sim} N (0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/02-slr-inference.html#linear-regression-model-1",
    "href": "slides/02-slr-inference.html#linear-regression-model-1",
    "title": "Inference for SLR",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) where \\(\\varepsilon_i \\overset{\\rm iid}{\\sim} N (0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/02-slr-inference.html#linear-regression-model-2",
    "href": "slides/02-slr-inference.html#linear-regression-model-2",
    "title": "Inference for SLR",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) where \\(\\varepsilon_i \\overset{\\rm iid}{\\sim} N (0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/02-slr-inference.html#notation",
    "href": "slides/02-slr-inference.html#notation",
    "title": "Inference for SLR",
    "section": "Notation",
    "text": "Notation\nMean function:\n\\(\\quad E(Y|X)=\\mu\\{Y|X\\} = \\beta_0 + \\beta_1 x_i\\)\n\nFitted model equation:\n\\(\\quad \\widehat{y}_i = \\widehat{\\mu}\\{Y|X\\} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\)"
  },
  {
    "objectID": "slides/02-slr-inference.html#uncertainty-in-the-parameters",
    "href": "slides/02-slr-inference.html#uncertainty-in-the-parameters",
    "title": "Inference for SLR",
    "section": "Uncertainty in the parameters",
    "text": "Uncertainty in the parameters"
  },
  {
    "objectID": "slides/02-slr-inference.html#uncertainty-in-the-parameters-1",
    "href": "slides/02-slr-inference.html#uncertainty-in-the-parameters-1",
    "title": "Inference for SLR",
    "section": "Uncertainty in the parameters",
    "text": "Uncertainty in the parameters"
  },
  {
    "objectID": "slides/02-slr-inference.html#uncertainty-in-the-parameters-2",
    "href": "slides/02-slr-inference.html#uncertainty-in-the-parameters-2",
    "title": "Inference for SLR",
    "section": "Uncertainty in the parameters",
    "text": "Uncertainty in the parameters"
  },
  {
    "objectID": "slides/02-slr-inference.html#sampling-distribution",
    "href": "slides/02-slr-inference.html#sampling-distribution",
    "title": "Inference for SLR",
    "section": "Sampling distribution",
    "text": "Sampling distribution"
  },
  {
    "objectID": "slides/02-slr-inference.html#sampling-distribution-1",
    "href": "slides/02-slr-inference.html#sampling-distribution-1",
    "title": "Inference for SLR",
    "section": "Sampling distribution",
    "text": "Sampling distribution\nWe use the t distribution with df = n - 2 to model the sampling distribution of a regression coefficient"
  },
  {
    "objectID": "slides/02-slr-inference.html#sampling-distribution-2",
    "href": "slides/02-slr-inference.html#sampling-distribution-2",
    "title": "Inference for SLR",
    "section": "Sampling distribution",
    "text": "Sampling distribution\n\nt distribution with \\(df =  n - 2\\)\n\n\n\nMean = \\(\\beta_i\\)\n\n\n\n\nStandard deviation = \\(SE(\\widehat{\\beta}_1) = \\widehat{\\sigma} / \\sqrt{\\sum (x_i - \\overline{x} )^2}\\)"
  },
  {
    "objectID": "slides/02-slr-inference.html#confidence-interval",
    "href": "slides/02-slr-inference.html#confidence-interval",
    "title": "Inference for SLR",
    "section": "Confidence interval",
    "text": "Confidence interval\n\n\nstatistic\n\n¬±\n\ncritical value √ó standard error\n\n\n\n\n\\(\\widehat{\\beta}_i\\)\n\n¬±\n\n\\(t^* \\cdot SE(\\widehat{\\beta}_i)\\)"
  },
  {
    "objectID": "slides/02-slr-inference.html#hypothesis-test",
    "href": "slides/02-slr-inference.html#hypothesis-test",
    "title": "Inference for SLR",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\\(H_0: \\beta_i = \\#\\) vs.¬†\\(H_a:\\) choose one of \\(\\begin{array}{c} \\beta_i \\ne \\#\\\\ \\beta_i &lt;\\#  \\\\ \\beta_i &gt;\\# \\end{array}\\)\n\n\n\n\ntest statistic\n\n\\(= \\dfrac{\\text{estimate} - \\text{null value}}{\\text{SE}}\\)\n\n\n\n\n\n\n\\(=\\dfrac{\\widehat{\\beta}_i - \\text{null value}}{SE(\\widehat{\\beta}_i)}\\)\n\n\n\n\nFind appropriate tail areas under the t-distribution with \\(df = n - 2\\)"
  },
  {
    "objectID": "slides/02-slr-inference.html#coefficient-table",
    "href": "slides/02-slr-inference.html#coefficient-table",
    "title": "Inference for SLR",
    "section": "Coefficient table",
    "text": "Coefficient table\nStandard output for a regression model includes a coefficient table\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   44.3      2.25       19.6  4.62e-28\n2 Mass           0.170    0.0307      5.52 7.28e- 7"
  },
  {
    "objectID": "slides/02-slr-inference.html#your-turn",
    "href": "slides/02-slr-inference.html#your-turn",
    "title": "Inference for SLR",
    "section": "Your turn",
    "text": "Your turn\n\nWork through the questions on the handout\nWrite your group‚Äôs answer to each question on the whiteboard\nBe ready to share your answers with the class ‚Äì a different person should speak than spoke last class"
  },
  {
    "objectID": "slides/02-slr-inference.html#a-note-on-reporting-test-results",
    "href": "slides/02-slr-inference.html#a-note-on-reporting-test-results",
    "title": "Inference for SLR",
    "section": "A note on reporting test results",
    "text": "A note on reporting test results\n\nOnly report at most 4 decimals for a p-value\nReport the estimate, test statistic, df, and p-value\nRemember that \\(\\alpha = 0.05\\) isn‚Äôt magic, it‚Äôs simply ‚Äútraditional‚Äù in many disciplines"
  },
  {
    "objectID": "slides/02-slr-inference.html#conditions-required-for-inference",
    "href": "slides/02-slr-inference.html#conditions-required-for-inference",
    "title": "Inference for SLR",
    "section": "Conditions required for inference",
    "text": "Conditions required for inference\nOur model must be valid for inference to be valid\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) where \\(\\varepsilon_i \\overset{\\rm iid}{\\sim} N (0, \\sigma^2)\\)\n\nConditions to check:\n\nLinear relationship is appropriate\nErrors are independent and identically distributed (iid)\nErrors are normally distributed\nVariance of the errors doesn‚Äôt depend on \\(x\\)"
  },
  {
    "objectID": "slides/07-mlr-intro.html#multiple-linear-regression-mlr",
    "href": "slides/07-mlr-intro.html#multiple-linear-regression-mlr",
    "title": "Multiple Linear Regression",
    "section": "Multiple Linear Regression (MLR)",
    "text": "Multiple Linear Regression (MLR)\n\\[Y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip} + \\varepsilon_i, \\quad \\varepsilon_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\]\n\n\nThis model requires that‚Ä¶\n\n\\(\\mu\\{Y|x_1, x_2, \\ldots, x_p\\}\\) is a linear function\nThe error terms are independent and are from a single population distribution\nThe error terms follow a normal distribution with mean 0 and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/07-mlr-intro.html#example",
    "href": "slides/07-mlr-intro.html#example",
    "title": "Multiple Linear Regression",
    "section": "Example",
    "text": "Example\n\nIn an effort to understand the important aspects of a satisfactory supervisor, clerical employees at a large financial organization were asked to rate their immediate supervisor.\nThe survey questions were designed to measure overall performance of the supervisor, as well as six additional characteristics.\nEmployees were asked to rate the following statements on a scale from 0 to 100\n(0 = ‚Äúcompletely disagree‚Äù, 100 = ‚Äúcompletely agree‚Äù)"
  },
  {
    "objectID": "slides/07-mlr-intro.html#the-data",
    "href": "slides/07-mlr-intro.html#the-data",
    "title": "Multiple Linear Regression",
    "section": "The data",
    "text": "The data\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nrating\nOverall rating of supervisor performance\n\n\ncomplaints\nScore for ‚ÄúYour supervisor handles employee complaints appropriately.‚Äù\n\n\nprivileges\nScore for ‚ÄúYour supervisor allows special privileges.‚Äù\n\n\nlearn\nScore for ‚ÄúYour supervisor provides opportunities to learn new things.‚Äù\n\n\nraises\nScore for ‚ÄúYour supervisor bases raises on performance.‚Äù\n\n\ncritical\nScore for ‚ÄúYour supervisor is too critical of poor performance.‚Äù\n\n\nadvance\nScore for ‚ÄúI am not satisfied with the rate I am advancing in the company?‚Äù"
  },
  {
    "objectID": "slides/07-mlr-intro.html#eda-scatterplot-matrix",
    "href": "slides/07-mlr-intro.html#eda-scatterplot-matrix",
    "title": "Multiple Linear Regression",
    "section": "EDA: Scatterplot matrix",
    "text": "EDA: Scatterplot matrix"
  },
  {
    "objectID": "slides/07-mlr-intro.html#eda-correlation-matrix",
    "href": "slides/07-mlr-intro.html#eda-correlation-matrix",
    "title": "Multiple Linear Regression",
    "section": "EDA: Correlation matrix",
    "text": "EDA: Correlation matrix\n\ncor(supervisor)\n\n           overall complaints privileges  learn raises critical advance\noverall     1.0000     0.8254     0.4261 0.6237 0.5901   0.1564  0.1551\ncomplaints  0.8254     1.0000     0.5583 0.5967 0.6692   0.1877  0.2246\nprivileges  0.4261     0.5583     1.0000 0.4933 0.4455   0.1472  0.3433\nlearn       0.6237     0.5967     0.4933 1.0000 0.6403   0.1160  0.5316\nraises      0.5901     0.6692     0.4455 0.6403 1.0000   0.3769  0.5742\ncritical    0.1564     0.1877     0.1472 0.1160 0.3769   1.0000  0.2833\nadvance     0.1551     0.2246     0.3433 0.5316 0.5742   0.2833  1.0000"
  },
  {
    "objectID": "slides/07-mlr-intro.html#fitting-the-model",
    "href": "slides/07-mlr-intro.html#fitting-the-model",
    "title": "Multiple Linear Regression",
    "section": "Fitting the model",
    "text": "Fitting the model\nTarget:\n\nFitted regression equation: \\(\\widehat{Y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_{i1} + \\widehat{\\beta}_2 x_{i2} + \\cdots + \\widehat{\\beta}_p x_{ip}\\)\nStandard error for MLR model: \\(\\widehat{\\sigma}\\)"
  },
  {
    "objectID": "slides/07-mlr-intro.html#fitting-the-model-1",
    "href": "slides/07-mlr-intro.html#fitting-the-model-1",
    "title": "Multiple Linear Regression",
    "section": "Fitting the model",
    "text": "Fitting the model\nProcedure:\nLeast squares estimation: choose the coefficients to minimize \\[\\text{SSE} = \\sum_{i=1}^n \\left(Y_i - \\widehat{Y}_i \\right)^2\\] Then use \\(\\widehat{\\sigma} = \\sqrt{\\frac{\\text{SSE}}{n-(p+1)}}\\)"
  },
  {
    "objectID": "slides/07-mlr-intro.html#interpreting-widehatbeta_0",
    "href": "slides/07-mlr-intro.html#interpreting-widehatbeta_0",
    "title": "Multiple Linear Regression",
    "section": "Interpreting \\(\\widehat{\\beta}_0\\)",
    "text": "Interpreting \\(\\widehat{\\beta}_0\\)\n\n\n(Intercept)  complaints  privileges       learn      raises    critical \n     10.787       0.613      -0.073       0.320       0.082       0.038 \n    advance \n     -0.217 \n\n\n\nThe expected value of the response variable when all explanatory variables are 0\nThe expected overall supervisor score is about 10.8 when all individual scores are 0"
  },
  {
    "objectID": "slides/07-mlr-intro.html#interpreting-widehatbeta_j",
    "href": "slides/07-mlr-intro.html#interpreting-widehatbeta_j",
    "title": "Multiple Linear Regression",
    "section": "Interpreting \\(\\widehat{\\beta}_j\\)",
    "text": "Interpreting \\(\\widehat{\\beta}_j\\)\n\n\n(Intercept)  complaints  privileges       learn      raises    critical \n     10.787       0.613      -0.073       0.320       0.082       0.038 \n    advance \n     -0.217 \n\n\n\nThe expected change in the response for a 1-unit increase in \\(x_j\\), when all other variables are held constant.\nWe expect a 0.613 point increase in a supervisor‚Äôs overall score for a 1-point increase on the complaints scale, holding all other variables constant"
  },
  {
    "objectID": "slides/07-mlr-intro.html#testing-a-single-coefficient",
    "href": "slides/07-mlr-intro.html#testing-a-single-coefficient",
    "title": "Multiple Linear Regression",
    "section": "Testing a single coefficient",
    "text": "Testing a single coefficient\n\nHypotheses: \\(H_0: \\ \\beta_j = \\#\\) vs.¬†\\(H_a: \\ \\beta_j \\underset{&gt;}{\\overset{&lt;}{\\ne}} \\#\\)\n\n\n\nTest statistic: \\(t = \\dfrac{\\hat{\\beta}_j - \\#}{SE(\\beta_j)}\\)\n\n\n\n\nReference distribution: \\(t\\) distribution with d.f. = \\(n-(p+1)\\)\n\n\n\n\np-value: Area in the tail(s) specified by \\(H_a\\)"
  },
  {
    "objectID": "slides/07-mlr-intro.html#confidence-intervals",
    "href": "slides/07-mlr-intro.html#confidence-intervals",
    "title": "Multiple Linear Regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\\(\\widehat{\\beta}_j \\pm t^*_{n-(p+1)} \\cdot SE(\\widehat{\\beta}_j )\\)\n\n\n\\(t^*_{n-(p+1)}\\) is the \\(1 - \\alpha/2\\) quantile from the t-distribution with \\(d.f. = n-(p+1)\\)"
  },
  {
    "objectID": "slides/07-mlr-intro.html#example-1",
    "href": "slides/07-mlr-intro.html#example-1",
    "title": "Multiple Linear Regression",
    "section": "Example",
    "text": "Example\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.7871\n11.5893\n0.9308\n0.3616\n-13.1871\n34.7613\n\n\ncomplaints\n0.6132\n0.1610\n3.8090\n0.0009\n0.2802\n0.9462\n\n\nprivileges\n-0.0731\n0.1357\n-0.5382\n0.5956\n-0.3538\n0.2077\n\n\nlearn\n0.3203\n0.1685\n1.9009\n0.0699\n-0.0283\n0.6689\n\n\nraises\n0.0817\n0.2215\n0.3690\n0.7155\n-0.3764\n0.5399\n\n\ncritical\n0.0384\n0.1470\n0.2611\n0.7963\n-0.2657\n0.3425\n\n\nadvance\n-0.2171\n0.1782\n-1.2180\n0.2356\n-0.5857\n0.1516"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#conditions-required-for-inference",
    "href": "slides/04-slr-diagnostics.html#conditions-required-for-inference",
    "title": "Model Diagnostics",
    "section": "Conditions required for inference",
    "text": "Conditions required for inference\nOur model must be valid for inference to be valid\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) where \\(\\varepsilon_i \\overset{\\rm iid}{\\sim} N (0, \\sigma^2)\\)\n\nConditions to check:\n\nLinear relationship is appropriate\nErrors are independent and identically distributed (iid)\nErrors are normally distributed\nVariance of the errors doesn‚Äôt depend on \\(x\\)"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#residuals",
    "href": "slides/04-slr-diagnostics.html#residuals",
    "title": "Model Diagnostics",
    "section": "Residuals",
    "text": "Residuals\nDefinition: \\(e_i =\\widehat{\\varepsilon}_i = y_i - \\widehat{y}_i\\)\n\nProperties:\n\nsum to zero \\(\\Longrightarrow\\) mean is 0\nuncorrelated with \\(x\\) and \\(\\widehat{y}\\)\nnormally distributed\n\\(SD(e_i) = \\widehat{\\sigma} \\sqrt{1 - \\dfrac{1}{n} - \\dfrac{(x_i - \\overline{x})^2}{\\sum (x_i - \\overline{x})^2}}\\)"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#standardized-residuals",
    "href": "slides/04-slr-diagnostics.html#standardized-residuals",
    "title": "Model Diagnostics",
    "section": "Standardized residuals",
    "text": "Standardized residuals\n\\[r_i = \\frac{e_i}{\\widehat{\\sigma} \\sqrt{1 - \\dfrac{1}{n} - \\dfrac{(x_i - \\overline{x})^2}{\\sum (x_i - \\overline{x})^2}}}\\]\nProperties:\n\nsum to zero \\(\\Longrightarrow\\) mean is 0\nuncorrelated with \\(x\\) and \\(\\widehat{y}\\)\nnormally distributed\n\\(SD(r_i) = 1\\)"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#a-good-residual-plot",
    "href": "slides/04-slr-diagnostics.html#a-good-residual-plot",
    "title": "Model Diagnostics",
    "section": "A ‚Äúgood‚Äù residual plot",
    "text": "A ‚Äúgood‚Äù residual plot"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#your-turn",
    "href": "slides/04-slr-diagnostics.html#your-turn",
    "title": "Model Diagnostics",
    "section": "Your turn",
    "text": "Your turn\n\nWork in groups\nOn the whiteboards, sketch a plot of \\(y\\) vs.¬†\\(x\\) and a corresponding residual plot that would indicate a violation of the\n\nlinearity condition\nconstant variance condition"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#assessing-normality",
    "href": "slides/04-slr-diagnostics.html#assessing-normality",
    "title": "Model Diagnostics",
    "section": "Assessing normality",
    "text": "Assessing normality\n\nhistogram of residuals\nnormal Q-Q plot of residuals\n\nExamples of ‚Äúgood‚Äù plots:"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#assessing-independence",
    "href": "slides/04-slr-diagnostics.html#assessing-independence",
    "title": "Model Diagnostics",
    "section": "Assessing independence",
    "text": "Assessing independence\n\nplot residuals vs.¬†variable inducing dependence (e.g.¬†time, location, subject ID)\n\nExamples of ‚Äúgood‚Äù plots:"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#what-happens-the-conditions-arent-valid",
    "href": "slides/04-slr-diagnostics.html#what-happens-the-conditions-arent-valid",
    "title": "Model Diagnostics",
    "section": "What happens the conditions aren‚Äôt valid?",
    "text": "What happens the conditions aren‚Äôt valid?\n\nLinearity: if nonlinear, everything breaks!\nIndependence: estimates are still unbiased (i.e.¬†we fit the right line) but measures of the accuracy of those estimates (the SEs) are typically too small\nNormality: estimates are still unbiased (i.e.¬†we fit the right line), SEs are correct BUT confidence/prediction intervals are wrong (we can‚Äôt use t-distribution)\nConstant error variance: estimates are still unbiased but standard errors are wrong (and we don‚Äôt know how wrong)"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#what-do-we-do-if-our-assumptions-are-violated",
    "href": "slides/04-slr-diagnostics.html#what-do-we-do-if-our-assumptions-are-violated",
    "title": "Model Diagnostics",
    "section": "What do we do if our assumptions are violated?",
    "text": "What do we do if our assumptions are violated?\n\nChange our assumptions (hard, need more stats)\nTransform \\(y\\), \\(x\\), or both\nChange the type of inference (remember the bootstrap?)"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#transforming-variables-can",
    "href": "slides/04-slr-diagnostics.html#transforming-variables-can",
    "title": "Model Diagnostics",
    "section": "Transforming variables can",
    "text": "Transforming variables can\n\nAddress non-linear patterns (i.e., linear on transformed scale)\nStabilize variance\nCorrect skew\nMinimize the effects of outliers"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#applying-transformations",
    "href": "slides/04-slr-diagnostics.html#applying-transformations",
    "title": "Model Diagnostics",
    "section": "Applying transformations",
    "text": "Applying transformations\nTo apply a transformation, we calculate a new variable and use it in place of the original variable in our model\n\nExamples\n\\[\n\\begin{split}\n\\log(y) &= \\beta_0 + \\beta_1 x + \\varepsilon\\\\\ny &= \\beta_0 + \\beta_1 \\sqrt{x} + \\varepsilon\\\\\n\\log(y) &= \\beta_0 + \\beta_1 \\sqrt{x} + \\varepsilon\n\\end{split}\n\\]"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#review-of-logarithms",
    "href": "slides/04-slr-diagnostics.html#review-of-logarithms",
    "title": "Model Diagnostics",
    "section": "Review of logarithms",
    "text": "Review of logarithms\nThe logarithm \\(\\log_b(x)\\) is a function that is the exponent (power) that the base, \\(b\\), must be raised to produce the value \\(x\\):\n\n\\(\\log_{10}(100)=2\\) since \\(10^2 = 100\\)\n\\(\\log_{10}(10)=1\\) since \\(10^1 = 10\\)\n\\(\\log_2(1) = 0\\) since \\(2^0=1\\)\n\\(\\log_2(0.5)=-1\\) since \\(2^{-1}=\\frac{1}{2}\\)"
  },
  {
    "objectID": "slides/04-slr-diagnostics.html#review-of-logarithms-1",
    "href": "slides/04-slr-diagnostics.html#review-of-logarithms-1",
    "title": "Model Diagnostics",
    "section": "Review of logarithms",
    "text": "Review of logarithms\n\nTakes in only positive numbers, i.e.¬†\\(x&gt;0\\)\nThe log of products is the sum of the logs\n\\[\\log_b(mx) = \\log_b(m) + \\log_b(x)\\]\nThe log of quotients is the difference of the logs\n\\[\\log_b\\left(\\frac{m}{x}\\right) = \\log_b(m) - \\log_b(x)\\]\nThe log of powers is the exponent times the log\n\\[\\log_b(x^p) = p \\log_b(x)\\]"
  },
  {
    "objectID": "slides/12-mlr-selection.html#the-problem-with-scatterplots",
    "href": "slides/12-mlr-selection.html#the-problem-with-scatterplots",
    "title": "Model Selection",
    "section": "The problem with scatterplots",
    "text": "The problem with scatterplots\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe only see the marginal relationships between pairs of variables, not the relationships after accounting for other variables"
  },
  {
    "objectID": "slides/12-mlr-selection.html#partial-residual-plots",
    "href": "slides/12-mlr-selection.html#partial-residual-plots",
    "title": "Model Selection",
    "section": "Partial residual plots",
    "text": "Partial residual plots\nConsider two-predictor model: \\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_2\\)\n\nTo isolate the relationship between \\(Y\\) and \\(x_2\\) after accounting for \\(x_1\\), we can:\n\nFit the MLR model\nCalculate the residuals from the fitted model: \\(e_i = y_i - \\widehat{y}_i\\)\nAdd the ‚Äúcontribution‚Äù of \\(x_j\\) back into residuals: \\(\\text{pres}_{j,i} = e_i + \\widehat{\\beta}_jx_{j,i}\\)\nPlot \\(\\text{pres}_j\\) against \\(x_j\\)"
  },
  {
    "objectID": "slides/12-mlr-selection.html#example",
    "href": "slides/12-mlr-selection.html#example",
    "title": "Model Selection",
    "section": "Example",
    "text": "Example\nAfter accounting for climb, what is the relationship between time and distance?"
  },
  {
    "objectID": "slides/12-mlr-selection.html#example-1",
    "href": "slides/12-mlr-selection.html#example-1",
    "title": "Model Selection",
    "section": "Example",
    "text": "Example\nAfter accounting for distance, what is the relationship between time and climb?"
  },
  {
    "objectID": "slides/12-mlr-selection.html#why-is-this-useful",
    "href": "slides/12-mlr-selection.html#why-is-this-useful",
    "title": "Model Selection",
    "section": "Why is this useful?",
    "text": "Why is this useful?\n\nWe can see the ‚Äúeffect‚Äù of \\(x_j\\) after adjusting for other model terms\nWe can see the variation in \\(y\\) that remains after adjusting for other model terms\nWe can look for outliers that could be affecting the estimated effect of \\(x_j\\)\nWe can see if the effect of \\(x_j\\) is correctly modeled, non-linearity and/or non-constant variance suggest we need to correct our model form"
  },
  {
    "objectID": "slides/12-mlr-selection.html#partial-residual-plots-in-r",
    "href": "slides/12-mlr-selection.html#partial-residual-plots-in-r",
    "title": "Model Selection",
    "section": "Partial residual plots in R",
    "text": "Partial residual plots in R\nThe car package calls them component + residual plots\n\nlibrary(car)\nmod &lt;- lm(time ~ dist + climb, data = hills2000)\ncrPlots(mod, layout = c(1, 2))\n\n\n\nNote: The partial residuals in these plots are centered"
  },
  {
    "objectID": "slides/12-mlr-selection.html#example-1-1",
    "href": "slides/12-mlr-selection.html#example-1-1",
    "title": "Model Selection",
    "section": "Example 1",
    "text": "Example 1\n\n\nTrue model\nFitted model\n\n\\(y = x_1^2 + 0.5x_2 + \\epsilon\\)\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\)\n\nBoth \\(x_1\\) and \\(x_2\\) are numeric, roughly between -5 and 5"
  },
  {
    "objectID": "slides/12-mlr-selection.html#example-2",
    "href": "slides/12-mlr-selection.html#example-2",
    "title": "Model Selection",
    "section": "Example 2",
    "text": "Example 2\n\n\nTrue model\nFitted model\n\n\\(y = x_1^2 + 0.5x_2 + \\epsilon\\)\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\)\n\nBut now \\(x_1\\) is strictly positive \\(\\rightarrow\\) monotone relationship"
  },
  {
    "objectID": "slides/12-mlr-selection.html#example-1-2",
    "href": "slides/12-mlr-selection.html#example-1-2",
    "title": "Model Selection",
    "section": "Example 1",
    "text": "Example 1\n\n\nTrue model\nFitted model\n\n\\(y = x_1^2 + 0.5x_2 + \\epsilon\\)\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\)\n\nBoth \\(x_1\\) and \\(x_2\\) are numeric, roughly between -5 and 5"
  },
  {
    "objectID": "slides/12-mlr-selection.html#example-2-1",
    "href": "slides/12-mlr-selection.html#example-2-1",
    "title": "Model Selection",
    "section": "Example 2",
    "text": "Example 2\n\n\nTrue model\nFitted model\n\n\\(y = x_1^2 + 0.5x_2 + \\epsilon\\)\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\)\n\nBut now \\(x_1\\) is strictly positive \\(\\rightarrow\\) monotone relationship"
  },
  {
    "objectID": "slides/12-mlr-selection.html#your-turn",
    "href": "slides/12-mlr-selection.html#your-turn",
    "title": "Model Selection",
    "section": "Your turn",
    "text": "Your turn\n\nWork through example on the handout\nBe ready to share your thoughts (I‚Äôm going to cold call)"
  },
  {
    "objectID": "slides/12-mlr-selection.html#effects-plots",
    "href": "slides/12-mlr-selection.html#effects-plots",
    "title": "Model Selection",
    "section": "Effects plots",
    "text": "Effects plots\nConsider two-predictor model: \\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_1 + \\widehat{\\beta}_2 x_2\\)\n\n\nFix \\(x_1\\) at some value, say \\(x_1 = c\\)\nCalculate \\(\\widehat{y}\\) for a range of \\(x_2\\) values: \\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 c + \\widehat{\\beta}_2 x_2\\)\nPlot \\(\\widehat{y}\\) against \\(x_2\\)"
  },
  {
    "objectID": "slides/12-mlr-selection.html#example-3",
    "href": "slides/12-mlr-selection.html#example-3",
    "title": "Model Selection",
    "section": "Example",
    "text": "Example\nWhats the relationship between record time and distance, holding the total climb constant?\n\n\nClimb held constant at its mean value, 2077.32 ft"
  },
  {
    "objectID": "slides/12-mlr-selection.html#example-4",
    "href": "slides/12-mlr-selection.html#example-4",
    "title": "Model Selection",
    "section": "Example",
    "text": "Example\nWhats the relationship between record time and the total climb, holding the distance constant?\n\n\nDistance held constant at its mean value, 7.53 miles"
  },
  {
    "objectID": "slides/12-mlr-selection.html#effects-plots-in-r",
    "href": "slides/12-mlr-selection.html#effects-plots-in-r",
    "title": "Model Selection",
    "section": "Effects plots in R",
    "text": "Effects plots in R\nThe ggeffects package provides a nice way to visualize fitted models\n\nlibrary(ggeffects)\nmod &lt;- lm(time ~ dist + climb, data = hills2000)\npredict_response(mod, terms = \"dist\") |&gt; \n  plot() +\n  labs(\n    y = \"My y-axis label\",\n    x = \"My x-axis label\"\n  )\n\n\n\n\n\n\n\n\n\nNote\n\n\nIt holds the other predictors at their mean (for numeric) or mode (for categorical)"
  },
  {
    "objectID": "slides/12-mlr-selection.html#plotting-interaction-models",
    "href": "slides/12-mlr-selection.html#plotting-interaction-models",
    "title": "Model Selection",
    "section": "Plotting interaction models",
    "text": "Plotting interaction models\nRecall the FEV model with interaction between age and smoking status:\n\\(\\mu(y|x) = \\beta_0 + \\beta_1 \\mathtt{smoker} + \\beta_2 \\mathtt{age} + \\beta_3 \\mathtt{age} \\times \\mathtt{smoker}\\)"
  },
  {
    "objectID": "slides/12-mlr-selection.html#effects-plots-in-r-1",
    "href": "slides/12-mlr-selection.html#effects-plots-in-r-1",
    "title": "Model Selection",
    "section": "Effects plots in R",
    "text": "Effects plots in R\n\n\n\n\n\n\nNote\n\n\nTo get multiple fitted lines representing different groups, specify the variable you want to plot and the grouping variable in the terms argument\n\n\n\n\n\nfev_lm &lt;- lm(FEV ~ Age * Smoke, data = fev)\npredict_response(fev_lm, terms = c(\"Age\", \"Smoke\")) |&gt;\n  plot()"
  },
  {
    "objectID": "slides/12-mlr-selection.html#your-turn-1",
    "href": "slides/12-mlr-selection.html#your-turn-1",
    "title": "Model Selection",
    "section": "Your turn",
    "text": "Your turn\n\nWork through example on the handout\nBe ready to share your thoughts (I‚Äôm going to cold call)"
  },
  {
    "objectID": "slides/12-mlr-selection.html#define-the-goal",
    "href": "slides/12-mlr-selection.html#define-the-goal",
    "title": "Model Selection",
    "section": "1. Define the goal",
    "text": "1. Define the goal\nBefore you start building a model you need to identify why you are building the model.\n\nExploring associations\nTesting a theoretical relationship\nControlling for confounders\nPrediction\n\n\nReasons can be intertwined"
  },
  {
    "objectID": "slides/12-mlr-selection.html#choose-an-initial-pool-of-predictors",
    "href": "slides/12-mlr-selection.html#choose-an-initial-pool-of-predictors",
    "title": "Model Selection",
    "section": "2. Choose an initial pool of predictors",
    "text": "2. Choose an initial pool of predictors\n\nTheory might dictate some/all variables\nDesigned experiment might dictate some/all variables\nIn other situations\n\nExamine variables one at a time ‚Äì beware of skew, note outliers\nExamine pairwise correlations/scatterplot matrix ‚Äì note potential predictors, multicollinearity"
  },
  {
    "objectID": "slides/12-mlr-selection.html#fit-a-full-regression-model",
    "href": "slides/12-mlr-selection.html#fit-a-full-regression-model",
    "title": "Model Selection",
    "section": "3. Fit a full regression model",
    "text": "3. Fit a full regression model\nFit a ‚Äúfull‚Äù initial regression model where you include all of the potential variables"
  },
  {
    "objectID": "slides/12-mlr-selection.html#question-your-full-model",
    "href": "slides/12-mlr-selection.html#question-your-full-model",
    "title": "Model Selection",
    "section": "4. Question your full model",
    "text": "4. Question your full model\n\nCheck the full model for violations to the conditions, fix as needed.\nOrder I check/fix:\n\nLinearity\nHeteroscedasticity\nNormality\nOutliers and influential points"
  },
  {
    "objectID": "slides/12-mlr-selection.html#examine-if-any-variables-can-be-droppedadded",
    "href": "slides/12-mlr-selection.html#examine-if-any-variables-can-be-droppedadded",
    "title": "Model Selection",
    "section": "5. Examine if any variables can be dropped/added",
    "text": "5. Examine if any variables can be dropped/added\n\nThere may be ‚Äúinsignificant‚Äù predictor variables that you can consider dropping\nYou could use t-tests or extra-sums-of-squares F-tests to guide these decisions\nYou could use model selection criteria (AIC, BIC, adjusted R2) to guide these decisions\nSometimes you discover reasons to add variables (e.g., remedy model deficiencies, discovery of interactions)"
  },
  {
    "objectID": "slides/12-mlr-selection.html#section",
    "href": "slides/12-mlr-selection.html#section",
    "title": "Model Selection",
    "section": "",
    "text": "6. Iterate through steps 4 and 5\n\nModeling is an iterative process, unlikely to find the ‚Äúbest‚Äù model on the first try\nEach time you change the model, you need to re-check/fix the model conditions\n\n7. Do a final model check\n\nAre the conditions are satisfied?\nOutliers and influential points?\nMulticollinearity?"
  },
  {
    "objectID": "slides/12-mlr-selection.html#proceed-with-your-analysis",
    "href": "slides/12-mlr-selection.html#proceed-with-your-analysis",
    "title": "Model Selection",
    "section": "8. Proceed with your analysis",
    "text": "8. Proceed with your analysis\n\nInterpret coefficients\nTest hypotheses\nMake predictions\n\n\n\n\n\n\n\nConfirming a theory\n\n\nWhen you want to confirm a theory, only include ‚Äúextra‚Äù predictors in the model building process.\nAdd the variables that are ‚Äúpredetermined‚Äù by the theory back into the model at the end of the model building process."
  },
  {
    "objectID": "slides/12-mlr-selection.html#sat-data",
    "href": "slides/12-mlr-selection.html#sat-data",
    "title": "Model Selection",
    "section": "SAT data",
    "text": "SAT data\n\nData for the 50 states\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsat\naverage of combined verbal and math SAT\n\n\ntakers\npercentage of eligible seniors who took exam\n\n\nincome\nmedian income of families of test-takers\n\n\nyears\nmean number of years of schooling\n\n\npublic\npercentage of test-takers attending public school\n\n\nexpend\ntotal state expenditure on secondary schools (in hundreds of dollars per student)\n\n\nrank\nmedian percentile rank of test-takers in their high school classes\n\n\n\n\n\ncase0901 in the Sleuth"
  },
  {
    "objectID": "slides/12-mlr-selection.html#working-for-the-legislature",
    "href": "slides/12-mlr-selection.html#working-for-the-legislature",
    "title": "Model Selection",
    "section": "Working for the legislature",
    "text": "Working for the legislature\n\nWhat is the impact of state expenditures on SAT scores after accounting for other factors?\n\n\n\nStrategy: First, choose controls, then add expenditures\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n144.5300\n297.1628\n0.4864\n0.6291\n\n\nRank\n4.4498\n2.7466\n1.6201\n0.1124\n\n\nIncome\n0.2054\n0.1162\n1.7672\n0.0841\n\n\nYears\n24.8751\n6.4223\n3.8732\n0.0004\n\n\nlog(Takers)\n-26.0915\n17.0035\n-1.5345\n0.1321\n\n\nPublic\n0.6962\n0.5513\n1.2630\n0.2132\n\n\n\n\n\n\n\n\n\n\nRank, log(Takers), and Public may not be significant - but can we trust these results?"
  },
  {
    "objectID": "slides/12-mlr-selection.html#our-model-is-overspecified",
    "href": "slides/12-mlr-selection.html#our-model-is-overspecified",
    "title": "Model Selection",
    "section": "Our model is overspecified!",
    "text": "Our model is overspecified!\nRank and log(Takers) are highly correlated!\n\n\nvif(political_lm)\n\n       Rank      Income       Years log(Takers)      Public \n     21.068       1.692       1.326      22.175       1.928 \n\n\n\n\nLet‚Äôs try dropping Rank and refitting the model‚Ä¶"
  },
  {
    "objectID": "slides/12-mlr-selection.html#model-b",
    "href": "slides/12-mlr-selection.html#model-b",
    "title": "Model Selection",
    "section": "Model B",
    "text": "Model B\nIt looks like Public can also be removed as it doesn‚Äôt explain a substantial proportion of the variability in SAT scores\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-268.779\n127.4007\n-2.1097\n0.0405\n\n\nRank\n8.509\n0.7496\n11.3522\n0.0000\n\n\nIncome\n0.230\n0.1168\n1.9689\n0.0551\n\n\nYears\n27.564\n6.2710\n4.3954\n0.0001\n\n\nPublic\n0.267\n0.4821\n0.5539\n0.5824"
  },
  {
    "objectID": "slides/12-mlr-selection.html#now-add-expenditure",
    "href": "slides/12-mlr-selection.html#now-add-expenditure",
    "title": "Model Selection",
    "section": "Now, add expenditure",
    "text": "Now, add expenditure\nExamine the significance of expenditure after controlling for rank, income and years.\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-285.4613\n98.9617\n-2.885\n0.0060\n\n\nRank\n9.3411\n0.7393\n12.636\n0.0000\n\n\nIncome\n0.1199\n0.1078\n1.112\n0.2719\n\n\nYears\n25.5321\n5.3994\n4.729\n0.0000\n\n\nExpend\n1.6162\n0.6706\n2.410\n0.0201"
  },
  {
    "objectID": "slides/13-mlr-validation.html#metrics-that-avoid-testing",
    "href": "slides/13-mlr-validation.html#metrics-that-avoid-testing",
    "title": "Model Validation",
    "section": "Metrics that avoid testing",
    "text": "Metrics that avoid testing\n\nAdjusted \\(R^2\\)\nAkaike‚Äôs Information Criteria (AIC)\nBayesian Information Criteria (BIC)\nMallow‚Äôs \\(C_p\\)\n\nThese do not rely on p-values, and they balance both complexity and fit"
  },
  {
    "objectID": "slides/13-mlr-validation.html#mallows-c_p",
    "href": "slides/13-mlr-validation.html#mallows-c_p",
    "title": "Model Validation",
    "section": "Mallow‚Äôs \\(C_p\\)",
    "text": "Mallow‚Äôs \\(C_p\\)\nLet \\(p\\) be # of coefficients for the model in question\n\\[C_p = \\underbrace{\\frac{SSE_{p}}{MSE_{full}}}_{\\text{error part}} - \\underbrace{(n - 2p)}_{\\text{penalty for complexity}}\\]\n\nWant small \\(C_p\\), but also want \\(C_p \\approx p\\)\n\\(C_m = m\\) for the biggest model ( \\(m\\) = # of coefficients)\n\\(C_p\\) focuses on prediction\nOr, \\(C_p\\) minimizes \\(SSE + \\text{penalty}\\)"
  },
  {
    "objectID": "slides/13-mlr-validation.html#aic",
    "href": "slides/13-mlr-validation.html#aic",
    "title": "Model Validation",
    "section": "AIC",
    "text": "AIC\n\\[AIC = \\underbrace{n \\log(SSE/n)}_{\\text{error part}} + \\underbrace{2p}_{\\text{penalty for complexity}}\\]\n\nchoose model with smaller AIC\nmodels do not need to be nested to be compared\nbased on asymptotic theory\ngenerally favors models that are bigger than the ‚Äútrue‚Äù model"
  },
  {
    "objectID": "slides/13-mlr-validation.html#bic",
    "href": "slides/13-mlr-validation.html#bic",
    "title": "Model Validation",
    "section": "BIC",
    "text": "BIC\n\\[BIC = \\underbrace{n \\log(SSE/n)}_{\\text{error part}} + \\underbrace{p \\log(n)}_{\\text{penalty for complexity}}\\]\n\nchoose model with smaller BIC\nmodels do not need to be nested to be compared\nbased on asymptotic theory\nusually larger penalty than AIC \\(\\Longrightarrow\\) leads to smaller models"
  },
  {
    "objectID": "slides/13-mlr-validation.html#example-candy-rankings",
    "href": "slides/13-mlr-validation.html#example-candy-rankings",
    "title": "Model Validation",
    "section": "Example: Candy rankings",
    "text": "Example: Candy rankings\n\n‚Äúwe devised an experiment: Pit dozens of fun-sized candy varietals against one another‚Äù\nCrowd-sourced evaluations\nresponse = win percentage\npossible predictor variables:\n\n\n\nIndicator variables: chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus (one of many candies)\n\nQuantitative variables: sugarpercent,  pricepercent\n\n\nhttps://fivethirtyeight.com/videos/the-ultimate-halloween-candy-power-ranking/"
  },
  {
    "objectID": "slides/13-mlr-validation.html#selected-model",
    "href": "slides/13-mlr-validation.html#selected-model",
    "title": "Model Validation",
    "section": "Selected model",
    "text": "Selected model\nStarting with a model that contained all predictor variables, dropped variables that were unimportant:\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n51.804\n3.749\n13.817\n&lt;0.001\n\n\ncaramelTRUE\n‚àí14.510\n7.609\n‚àí1.907\n0.060\n\n\npeanutyalmondyTRUE\n‚àí18.192\n7.505\n‚àí2.424\n0.018\n\n\nhardTRUE\n16.201\n7.313\n2.215\n0.030\n\n\nbarTRUE\n20.275\n6.898\n2.939\n0.004"
  },
  {
    "objectID": "slides/13-mlr-validation.html#but-wait",
    "href": "slides/13-mlr-validation.html#but-wait",
    "title": "Model Validation",
    "section": "But wait!",
    "text": "But wait!\n\nI replaced the real win percentage with randomly generated values between 0 and 100!\nWe still got ‚Äúsignificant‚Äù results‚Ä¶"
  },
  {
    "objectID": "slides/13-mlr-validation.html#beware-of-inference-after-model-selection",
    "href": "slides/13-mlr-validation.html#beware-of-inference-after-model-selection",
    "title": "Model Validation",
    "section": "Beware of inference after model selection!",
    "text": "Beware of inference after model selection!\nIf we use the same data set to conduct inference as we used to select our model we run into (related) issues\n\noverfitting\nselection bias\nunder-estimation of MSE"
  },
  {
    "objectID": "slides/13-mlr-validation.html#validation",
    "href": "slides/13-mlr-validation.html#validation",
    "title": "Model Validation",
    "section": "Validation",
    "text": "Validation\nIf you want to run inference on variables used in model selection‚Ä¶\n\nCollect new data for inference -&gt; might be possible in controlled experiments\nCompare results with past results\nUse a holdout sample -&gt; most practical"
  },
  {
    "objectID": "slides/13-mlr-validation.html#data-splitting",
    "href": "slides/13-mlr-validation.html#data-splitting",
    "title": "Model Validation",
    "section": "Data splitting",
    "text": "Data splitting\n\nSplit your original data set into two pieces: a training set and a validation set\nTraining set is only used for model selection, should have at least 6 to 10 times as many rows as there are slope coefficients in the biggest model considered\nWhat split?\n\nRandom 50/50 split is a starting point\nIncrease size of training set until you get 6-10 x rows as slopes\n\nSometimes you don‚Äôt have enough data to do this!"
  },
  {
    "objectID": "slides/13-mlr-validation.html#interpreting-results",
    "href": "slides/13-mlr-validation.html#interpreting-results",
    "title": "Model Validation",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\nFit the model selected to the validation set and check\n\ncoefficients - are they similar?\nsignificance tests - similar results?\nMSE - similar prediction error?\n\nIf the results are similar between the sets, no big issues with bias from model selection\n\nCustomary to refit the model to the entire (training + validation) data set\n\n\n\n\n\n\nIf they are quite different, trust the results from the validation set"
  },
  {
    "objectID": "slides/03-slr-prediction.html#warm-up",
    "href": "slides/03-slr-prediction.html#warm-up",
    "title": "Inference for Prediction",
    "section": "Warm up",
    "text": "Warm up\n\nWork with a neighbor\nAnswer the questions associated with the warm up on the worksheet\nNote that the explanatory variable is standardized  (mean 0, SD 1)"
  },
  {
    "objectID": "slides/03-slr-prediction.html#prediction",
    "href": "slides/03-slr-prediction.html#prediction",
    "title": "Inference for Prediction",
    "section": "Prediction",
    "text": "Prediction\nThere are two types of predictions in regression\n\n\nPredicting the mean response at a specific value of \\(x\\)\ne.g., the average starting salary for some with a B.A. in statistics\n\n\n\n\n\nPredicting the response for a specific future observation\ne.g., predicting your starting salary (if you have a B.A. in statistics)\n\n\n\n\n Think of two additional examples of each type of prediction."
  },
  {
    "objectID": "slides/03-slr-prediction.html#inference-for-prediction",
    "href": "slides/03-slr-prediction.html#inference-for-prediction",
    "title": "Inference for Prediction",
    "section": "Inference for prediction",
    "text": "Inference for prediction\nBest estimate \\(\\widehat{y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_0\\)\n\n\nInterval formula: \\(\\text{estimate} \\pm q \\times \\text{SE}\\)\n\n\\(\\widehat{y}\\) is our estimate\nUse a \\(t\\)-distribution with \\(df=n-2\\) to find \\(q\\)\n\n\n\n\nWe‚Äôll need different SEs depending on if we are building a\n\nconfidence interval for \\(\\widehat{\\mu}(Y|X_0)\\)\nprediction interval for \\(\\widehat{y}\\) or \\(\\text{Pred}(Y|X_0)\\)"
  },
  {
    "objectID": "slides/03-slr-prediction.html#standard-errors",
    "href": "slides/03-slr-prediction.html#standard-errors",
    "title": "Inference for Prediction",
    "section": "Standard errors",
    "text": "Standard errors\n\\(\\text{SE}(\\widehat{\\mu}(Y|X)) = \\widehat{\\sigma} \\sqrt{\\dfrac{1}{n} + \\dfrac{(x_0-\\overline{x})^2}{\\sum_{i=1}^n (x_i - \\overline{x})^2}}\\)\n\n\\(\\text{SE}(\\widehat{y})= \\widehat{\\sigma} \\sqrt{1+\\dfrac{1}{n} + \\dfrac{(x_0-\\overline{x})^2}{\\sum_{i=1}^n (x_i - \\overline{x})^2}}\\)\n\n Looking at the standard errors for the two intervals, which interval will be wider? Why does this make sense?\n\n\n As \\(x_0\\) gets farther from \\(\\overline{x}\\) what happens to the standard errors?"
  },
  {
    "objectID": "slides/03-slr-prediction.html#r-point-estimate",
    "href": "slides/03-slr-prediction.html#r-point-estimate",
    "title": "Inference for Prediction",
    "section": "R: Point estimate",
    "text": "R: Point estimate\nWe‚Äôll let R do the computational work\npredict allows you to quickly calculate the value of \\(\\widehat{y}\\) for a given \\(x\\) (or vector of \\(x\\)s)\n\npredict(car_lm, newdata = data.frame(Mileage = 8221))\n\n       1 \n23346.27"
  },
  {
    "objectID": "slides/03-slr-prediction.html#r-intervals",
    "href": "slides/03-slr-prediction.html#r-intervals",
    "title": "Inference for Prediction",
    "section": "R: Intervals",
    "text": "R: Intervals\nThe interval argument allows you to specify the type of interval you want\n\npredict(car_lm, newdata = data.frame(Mileage = 8221), \n        interval = \"confidence\")\n\n       fit      lwr      upr\n1 23346.27 22170.67 24521.86\n\n\n\n\npredict(car_lm, newdata = data.frame(Mileage = 8221), \n        interval = \"prediction\")\n\n       fit      lwr      upr\n1 23346.27 4094.689 42597.85"
  },
  {
    "objectID": "slides/03-slr-prediction.html#r-ses",
    "href": "slides/03-slr-prediction.html#r-ses",
    "title": "Inference for Prediction",
    "section": "R: SEs",
    "text": "R: SEs\nAdding se.fit = TRUEreturns the necessary standard errors for ‚Äúby hand‚Äù calculations\n\n\n$fit\n       1 \n23346.27 \n\n$se.fit\n[1] 598.8985\n\n$df\n[1] 802\n\n$residual.scale\n[1] 9789.288"
  },
  {
    "objectID": "slides/03-slr-prediction.html#activity",
    "href": "slides/03-slr-prediction.html#activity",
    "title": "Inference for Prediction",
    "section": "Activity",
    "text": "Activity\n\nWork with a neighbor\nWork through the inference for prediction example on the worksheet\nThe R tutorial is linked on Moodle, also can follow the QR code"
  },
  {
    "objectID": "slides/03-slr-prediction.html#conditions-required-for-inference",
    "href": "slides/03-slr-prediction.html#conditions-required-for-inference",
    "title": "Inference for Prediction",
    "section": "Conditions required for inference",
    "text": "Conditions required for inference\nOur model must be valid for inference to be valid\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) where \\(\\varepsilon_i \\overset{\\rm iid}{\\sim} N (0, \\sigma^2)\\)\n\nConditions to check:\n\nLinear relationship is appropriate\nErrors are independent and identically distributed (iid)\nErrors are normally distributed\nVariance of the errors doesn‚Äôt depend on \\(x\\)"
  },
  {
    "objectID": "slides/03-slr-prediction.html#regression-conditions",
    "href": "slides/03-slr-prediction.html#regression-conditions",
    "title": "Inference for Prediction",
    "section": "Regression conditions",
    "text": "Regression conditions\nWhat happens if our assumptions aren‚Äôt valid?\n\nLinearity: if nonlinear, everything breaks!\nIndependence: estimates are still unbiased (i.e.¬†we fit the right line) but measures of the accuracy of those estimates (the SEs) are typically too small\nNormality: estimates are still unbiased (i.e.¬†we fit the right line), SEs are correct BUT confidence/prediction intervals are wrong (we can‚Äôt use t-distribution)\nConstant error variance: estimates are still unbiased but standard errors are wrong (and we don‚Äôt know how wrong)"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#example",
    "href": "slides/08-mlr-categorical.html#example",
    "title": "Adding Categorical Predictors",
    "section": "Example",
    "text": "Example\n\n\nGoal:\n\ninvestigate the association between smoking and lung capacity using data from 345 adolescents between the ages of 10 and 19\n\n\n\nWrinkle:\n\nLung function is expected to increase during adolescence, but smoking may slow it‚Äôs progression\n\n\n\n\nData:\n\n\n\n\nVariable\nDescription\n\n\n\n\nFEV\nforced expiratory volume (in liters per second)\n\n\nAge\nage in years\n\n\nSmoke\nSmoker or Nonsmoker\n\n\n\n\n\nSource: Rosner (2006)"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#eda",
    "href": "slides/08-mlr-categorical.html#eda",
    "title": "Adding Categorical Predictors",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#eda-1",
    "href": "slides/08-mlr-categorical.html#eda-1",
    "title": "Adding Categorical Predictors",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#eda-2",
    "href": "slides/08-mlr-categorical.html#eda-2",
    "title": "Adding Categorical Predictors",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#indicator-variable",
    "href": "slides/08-mlr-categorical.html#indicator-variable",
    "title": "Adding Categorical Predictors",
    "section": "Indicator variable",
    "text": "Indicator variable\nRegression requires a numeric representation of all variables\n\nCreate a Smoker indicator variable:\n\nSmoker = 1\nNonsmoker = 0"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#example-1",
    "href": "slides/08-mlr-categorical.html#example-1",
    "title": "Adding Categorical Predictors",
    "section": "Example 1",
    "text": "Example 1\n\nFor each regression model on the handout, sketch the fitted model on the whiteboard\nEach fitted model will have two lines: one for smokers, one for nonsmokers\nWork with your neighbors"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#model-1",
    "href": "slides/08-mlr-categorical.html#model-1",
    "title": "Adding Categorical Predictors",
    "section": "Model 1",
    "text": "Model 1\n\\(\\mu(y|x) = 10 +  1 \\mathtt{age} -2 \\mathtt{smoker}\\)\n\n\n\n\n\n\nWarning"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#model-2",
    "href": "slides/08-mlr-categorical.html#model-2",
    "title": "Adding Categorical Predictors",
    "section": "Model 2",
    "text": "Model 2\n\\(\\mu(y|x) = 5 +  1 \\mathtt{age} -0.5 \\mathtt{age \\times smoker}\\)"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#model-3",
    "href": "slides/08-mlr-categorical.html#model-3",
    "title": "Adding Categorical Predictors",
    "section": "Model 3",
    "text": "Model 3\n\\(\\begin{split}\\mu(y|x) = 4  &+  0.5 \\mathtt{age} + 3\\mathtt{smoker} -0.5 \\mathtt{age \\times smoker} \\end{split}\\)"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#parallel-lines-model",
    "href": "slides/08-mlr-categorical.html#parallel-lines-model",
    "title": "Adding Categorical Predictors",
    "section": "Parallel lines model",
    "text": "Parallel lines model\n\nLung function develops at the same pace, but always lower for smokers"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#different-slopes-model",
    "href": "slides/08-mlr-categorical.html#different-slopes-model",
    "title": "Adding Categorical Predictors",
    "section": "Different slopes model",
    "text": "Different slopes model\n\nAdolescents start with similar lung capacity, but smokers develop at a slower rate"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#separate-lines-model",
    "href": "slides/08-mlr-categorical.html#separate-lines-model",
    "title": "Adding Categorical Predictors",
    "section": "Separate lines model",
    "text": "Separate lines model\n\nThe smokers/non-smokers have different starting lung capacities and develop at different rates"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#parallel-lines-model-1",
    "href": "slides/08-mlr-categorical.html#parallel-lines-model-1",
    "title": "Adding Categorical Predictors",
    "section": "Parallel lines model",
    "text": "Parallel lines model\n\n\\(\\mu(y|x) = \\beta_0 + \\beta_1 \\mathtt{smoker} + \\beta_2 \\mathtt{age}\\)\n\n\n\n\n\n\n\n\nInterpretations\n\n\n\\(\\beta_0=\\) y-intercept for non-smokers\n\\(\\beta_0 + \\beta_1=\\) y-intercept for smokers\n\\(\\beta_2=\\) expected rate of change for both groups"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#different-slopes-model-1",
    "href": "slides/08-mlr-categorical.html#different-slopes-model-1",
    "title": "Adding Categorical Predictors",
    "section": "Different slopes model",
    "text": "Different slopes model\n\n\\(\\mu(y|x) = \\beta_0 + \\beta_1 \\mathtt{age} + \\beta_2 \\mathtt{age} \\times \\mathtt{smoker}\\)\n\n\n\n\n\n\n\n\nInterpretations\n\n\n\\(\\beta_0=\\) y-intercept for both groups\n\\(\\beta_1=\\) expected rate of change (slope) for non-smokers\n\\(\\beta_1+\\beta_2=\\) expected rate of change (slope) for smokers"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#separate-lines-model-1",
    "href": "slides/08-mlr-categorical.html#separate-lines-model-1",
    "title": "Adding Categorical Predictors",
    "section": "Separate lines model",
    "text": "Separate lines model\n\n\\(\\mu(y|x) = \\beta_0 + \\beta_1 \\mathtt{smoker} + \\beta_2 \\mathtt{age} + \\beta_3 \\mathtt{age} \\times \\mathtt{smoker}\\)\n\n\n\n\n\n\n\n\nInterpretations\n\n\n\\(\\beta_0=\\) y-intercept for non-smokers\n\\(\\beta_0 + \\beta_1=\\) y-intercept for smokers\n\\(\\beta_2=\\) expected rate of change (slope) for non-smokers\n\\(\\beta_2+\\beta_3=\\) expected rate of change (slope) for smokers"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#more-than-3-categories",
    "href": "slides/08-mlr-categorical.html#more-than-3-categories",
    "title": "Adding Categorical Predictors",
    "section": "More than 3 categories",
    "text": "More than 3 categories\nSuppose we have student survey data and one of the columns records the year in school:\n\nFirst year, Sophomore, Junior, and Senior.\n\nHow can we include this variable in a multiple regression model?"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#a-potentially-bad-idea",
    "href": "slides/08-mlr-categorical.html#a-potentially-bad-idea",
    "title": "Adding Categorical Predictors",
    "section": "A potentially bad idea",
    "text": "A potentially bad idea\nWe could convert the column to numeric\n\nFirst year \\(\\to\\) 1\nSophomore \\(\\to\\) 2\nJunior \\(\\to\\) 3\nSenior \\(\\to\\) 4\n\n\n\\[\n\\mu(Y|{\\tt classyear}) = \\beta_0 + \\beta_1 {\\tt classyear}\n\\]"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#a-good-idea",
    "href": "slides/08-mlr-categorical.html#a-good-idea",
    "title": "Adding Categorical Predictors",
    "section": "A good idea",
    "text": "A good idea\nWe can create a series of indicator (dummy) variables to represent the four categories\n\n\n\n\n\nOriginal\n\n\n\n\nFirst year\n\n\nSophomore\n\n\nSenior\n\n\nSenior\n\n\nJunior\n\n\nSophomore"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#a-good-idea-1",
    "href": "slides/08-mlr-categorical.html#a-good-idea-1",
    "title": "Adding Categorical Predictors",
    "section": "A good idea",
    "text": "A good idea\nWe can create a series of indicator (dummy) variables to represent the four categories\n\n\n\n\n\nOriginal\n\n\n\n\nFirst year\n\n\nSophomore\n\n\nSenior\n\n\nSenior\n\n\nJunior\n\n\nSophomore\n\n\n\n\n\n\n\nFY\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#a-good-idea-2",
    "href": "slides/08-mlr-categorical.html#a-good-idea-2",
    "title": "Adding Categorical Predictors",
    "section": "A good idea",
    "text": "A good idea\nWe can create a series of indicator (dummy) variables to represent the four categories\n\n\n\n\n\nOriginal\n\n\n\n\nFirst year\n\n\nSophomore\n\n\nSenior\n\n\nSenior\n\n\nJunior\n\n\nSophomore\n\n\n\n\n\n\n\nFY\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n\nSoph\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#a-good-idea-3",
    "href": "slides/08-mlr-categorical.html#a-good-idea-3",
    "title": "Adding Categorical Predictors",
    "section": "A good idea",
    "text": "A good idea\nWe can create a series of indicator (dummy) variables to represent the four categories\n\n\n\n\n\nOriginal\n\n\n\n\nFirst year\n\n\nSophomore\n\n\nSenior\n\n\nSenior\n\n\nJunior\n\n\nSophomore\n\n\n\n\n\n\n\nFY\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n\nSo\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n\n\n\nJu\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#example-2",
    "href": "slides/08-mlr-categorical.html#example-2",
    "title": "Adding Categorical Predictors",
    "section": "Example 2",
    "text": "Example 2\n\nFor each sketched regression model, write the mean function for a regression model that matches on the whiteboard\nNote that line type = education level\nWork with your neighbors"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#model-1-1",
    "href": "slides/08-mlr-categorical.html#model-1-1",
    "title": "Adding Categorical Predictors",
    "section": "Model 1",
    "text": "Model 1"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#model-2-1",
    "href": "slides/08-mlr-categorical.html#model-2-1",
    "title": "Adding Categorical Predictors",
    "section": "Model 2",
    "text": "Model 2"
  },
  {
    "objectID": "slides/08-mlr-categorical.html#model-3-1",
    "href": "slides/08-mlr-categorical.html#model-3-1",
    "title": "Adding Categorical Predictors",
    "section": "Model 3",
    "text": "Model 3"
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html",
    "href": "activity/09-mlr-linear-combos.html",
    "title": "Including Categorical Predictors",
    "section": "",
    "text": "In this activity you will learn how to include build multiple regression models in R, including models with categorical predictors. The cars data set loaded below contains information on used cars for sale, including their Price, Mileage, and Make (a categorical variable with six levels)."
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#overview",
    "href": "activity/09-mlr-linear-combos.html#overview",
    "title": "Including Categorical Predictors",
    "section": "",
    "text": "In this activity you will learn how to include build multiple regression models in R, including models with categorical predictors. The cars data set loaded below contains information on used cars for sale, including their Price, Mileage, and Make (a categorical variable with six levels)."
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#thinking-about-the-data",
    "href": "activity/09-mlr-linear-combos.html#thinking-about-the-data",
    "title": "Including Categorical Predictors",
    "section": "Thinking about the data",
    "text": "Thinking about the data\nTo begin, create a scatterplot of Price versus Mileage.\n\n\n\n\n\n\n\n\nQ1. What do you notice about the relationship between these two variables? Is a transformation necessary?\nQ2. The Make variable is categorical with six levels: Buick, Cadillac, Chevrolet, Pontiac, SAAB, and Saturn. To include this variable in a regression model, we need use indicator (i.e., dummy) variables. How many do we need to make?"
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#fitting-an-mlr-model",
    "href": "activity/09-mlr-linear-combos.html#fitting-an-mlr-model",
    "title": "Including Categorical Predictors",
    "section": "Fitting an MLR model",
    "text": "Fitting an MLR model\nBuild (fit) a multiple regression model using log(Price) as the response with Mileage and Make as the predictor variables. To do this, you use + to separate the predictor variable on the right side of the ~ in the model formula. R will automatically convert a categorical explanatory variable into a set of indicator variables.\n\n\n\n\n\n\n\n\nQ3. Report the fitted regression equation and the \\(R^2\\) value.\nQ4.Which level of Make is the baseline? The baseline level is represented by 0‚Äôs across all of the indicator variables and won‚Äôt have a coefficient in the output.\nQ5.What strategy did R use to create the indicator variables for Make? In other words, how did R decide which level of Make to use as the baseline?\nQ6. Which of the following models best describes the one just fit: parallel lines, different slopes, or separate lines?"
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#plotting-the-fitted-model",
    "href": "activity/09-mlr-linear-combos.html#plotting-the-fitted-model",
    "title": "Including Categorical Predictors",
    "section": "Plotting the fitted model",
    "text": "Plotting the fitted model\nTo plot a fitted regression model, we can use the ggpredict() and plot() functions in the {ggeffects} package. The ggpredict() function creates a data frame of predicted values from the model for each Make across a range of Mileage values. The plot() function then creates a plot of these predicted values.\n\n\n\n\n\n\n\n\nQ7. Does this plot confirm your answer to the previous question?\n\n\n\n\n\n\nNoteTips on plotting the model\n\n\n\n\nHolding other variables at ‚Äútypical‚Äù values\nIf you have more predictor variables in your model, then variables not specified in the terms argument the ggpredict() function are set to a specific value. Quantitative variables are set to their mean. Categorical variables are set to the mode (the level with the most observations).\n\n\nLabels\nIf you need to adjust your axis labels or title, then you will need to add a labs() layer to your plot. For example,\n\nplot(cars_pred) +\n  labs(x = \"My new x label\", y = \"My new y label\",\n       title = \"My new title\")"
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#holding-other-variables-at-typical-values",
    "href": "activity/09-mlr-linear-combos.html#holding-other-variables-at-typical-values",
    "title": "Including Categorical Predictors",
    "section": "Holding other variables at ‚Äútypical‚Äù values",
    "text": "Holding other variables at ‚Äútypical‚Äù values\nIf you have more predictor variables in your model, then variables not specified in the terms argument the ggpredict() function are set to a specific value. Quantitative variables are set to their mean. Categorical variables are set to the mode (the level with the most observations)."
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#labels",
    "href": "activity/09-mlr-linear-combos.html#labels",
    "title": "Including Categorical Predictors",
    "section": "Labels",
    "text": "Labels\nIf you need to adjust your axis labels or title, then you will need to add a labs() layer to your plot. For example,\n\nplot(cars_pred) +\n  labs(x = \"My new x label\", y = \"My new y label\",\n       title = \"My new title\")"
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#fitting-a-model-with-interactions",
    "href": "activity/09-mlr-linear-combos.html#fitting-a-model-with-interactions",
    "title": "Including Categorical Predictors",
    "section": "Fitting a model with interactions",
    "text": "Fitting a model with interactions\nIf we believe that the association between Price and Mileage differs by Make, then we can include an interaction term in our model. To do this, we use * instead of + to separate the predictor variables on the right side of the ~ in the model formula. This will include both main effects and the interaction term.\n\n\n\n\n\n\n\n\nQ8. Report the fitted regression equation for Buicks and Cadillacs.\nQ9. What is the \\(R^2\\) value for this model? Does this model appear to fit the data better than the previous model?"
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#plotting-the-fitted-model-with-interactions",
    "href": "activity/09-mlr-linear-combos.html#plotting-the-fitted-model-with-interactions",
    "title": "Including Categorical Predictors",
    "section": "Plotting the fitted model with interactions",
    "text": "Plotting the fitted model with interactions\nTo plot the interaction model, we again use the ggpredict() and plot() functions in the {ggeffects} package."
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#calculating-cis-for-linear-combinations",
    "href": "activity/09-mlr-linear-combos.html#calculating-cis-for-linear-combinations",
    "title": "Including Categorical Predictors",
    "section": "Calculating CIs for linear combinations",
    "text": "Calculating CIs for linear combinations\nTo calculate a confidence interval for a linear combination of coefficients, we must first calculate the estimate and standard error of the linear combination.\nLet‚Äôs calculate a 95% confidence interval for the slope of the Cadillac model. Refer to your notes/slides and determine the formulas for the estimate and standard error of this slope.\nNow that you know the formulas, you‚Äôll use R to implement them. First, extract the coefficients and covariance matrix from the model object. Try printing the results to see what they look like and make note of where the Cadillac coefficients are located.\n\n\n\n\n\n\n\n\n\nExtract the coefficients from the model object and store them in car_coefs.\nExtract the covariance matrix from the model object and store it in car_vcov.\n\nNext, calculate the estimate of the slope for Cadillacs.\n\n\n\n\n\n\n\n\n\ncar_coefs is a vector, so we pull off the necessary coefficients using their position in square brackets. Here, car_coefs[2] pulls off the coefficient for Mileage and car_coefs[8] pulls off the coefficient for the interaction term Mileage:MakeCadillac.\n\nNow, calculate the standard error of the slope for Cadillacs.\n\n\n\n\n\n\n\n\n\ncar_vcov is a matrix, so we pull off the necessary variances and covariances using their row and column positions in square brackets. Here, car_vcov[2,2] pulls off the variance for Mileage, car_vcov[8,8] pulls off the variance for the interaction term Mileage:MakeCadillac, and car_vcov[2,8] pulls off the covariance between these two coefficients.\n\nFinally, use the estimate and standard error to calculate a 95% confidence interval for the slope of the Cadillac model. You can use the qt() function to find the appropriate critical value.\nQ10. Calculate a 95% confidence interval for the slope of the Cadillac model."
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#calculating-extra-sums-of-squares-f-tests",
    "href": "activity/09-mlr-linear-combos.html#calculating-extra-sums-of-squares-f-tests",
    "title": "Including Categorical Predictors",
    "section": "Calculating extra sums of squares F-tests",
    "text": "Calculating extra sums of squares F-tests\nDo we really need the interaction terms? To answer this question, we can use an extra sums of squares F-test to compare the model with interaction terms to the model without interaction terms.\nWe have already seen how to use the anova() function to compare two nested models. Here, we will use it again to compare our two car models.\n\n\n\n\n\n\n\n\n\nUsing only the full model\nWe can also calculate the extra sums of squares F-test using only the full model. To do this, we need to extract the SSE and degrees of freedom for both models from the ANOVA table for the full model.\nQ11. Run the below code and verify that the df, sums of squares, mean squares, F value, and p-value match what you got from the previous anova() command."
  },
  {
    "objectID": "activity/09-mlr-linear-combos.html#function-quick-reference",
    "href": "activity/09-mlr-linear-combos.html#function-quick-reference",
    "title": "Including Categorical Predictors",
    "section": "Function quick reference",
    "text": "Function quick reference\nThe following table summarizes the functions we learned today:\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nlm()\nFit a linear model\n\n\nsummary()\nDisplay detailed results from a fitted model\n\n\ncoef()\nExtract model coefficients\n\n\nvcov()\nExtract the variance-covariance matrix of model coefficients\n\n\nggpredict()\nCreate a data frame of predicted values from a fitted model\n\n\nplot()\nCreate a plot of predicted values from a fitted model\n\n\nanova()\nCreate an ANOVA table for a fitted model or compare two nested models"
  },
  {
    "objectID": "activity/02H-sol-slr-inference.html",
    "href": "activity/02H-sol-slr-inference.html",
    "title": "Inference for SLR ‚Äì Stat 230",
    "section": "",
    "text": "Suppose that you wish to determine whether there is a difference in means between two groups. To do this you run a two-sample t-test and find the test statistic to be \\(t = 2.5\\).\n\nWhat is the null hypothesis for this test?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(H_0: \\mu_1 = \\mu_2\\) (or \\(H_0: \\mu_1 - \\mu_2 = 0\\))\n\n\n\n\nWhat is the alternative hypothesis for this test?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(H_A: \\mu_1 \\neq \\mu_2\\) (two-sided)\n\n\n\n\nBelow is a plot of the appropriate t-distribution for this test. Sketch how you would calculate the p-value for this situation.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe p-value is equal to the area in the two tails beyond \\(2.5\\) and \\(-2.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow did you calculate this p-value in your last statistics course?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswers will vary based on where you took intro stats. You may have used StatKey or a similar online app/calculator, you may have used R‚Äôs pt() command, or you may have used a t-distribution table (we won‚Äôt use tables in this class).\n\n\n\n\nIf the p-value you calculated was 0.02, what would you conclude about the null hypothesis?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nA p-value of 0.02 provides evidence against the null hypothesis. In other words, there is statistically discernible evidence of a difference in means between the two groups (\\(t=2.5\\), p-value = 0.02).\n\n\n\n\n\n\n\nWhat is the difference between standard deviation and standard error?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nStandard deviation is a summary statistic that we can calculate for any distribution that measures the spread of the distribution. Standard error is the standard deviation of the sampling distribution (i.e., the distribution of a sample statistic). It is a measure of how much variability we expect in a sample statistic (e.g., sample mean or slope) from sample to sample.\n\n\n\nA 95% confidence interval for the mean waiting time at an emergency room (ER) of (128 minutes, 147 minutes).\n\nConsider the following interpretation of this interval. For each, determine whether it is correct. If it is incorrect, explain why and provide a correct interpretation.\n\n‚ÄúThere is a 95% probability that the mean waiting time at this ER is between 128 and 147 minutes.‚Äù\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThis interpretation is incorrect. The population mean is a fixed but unknown value, so the probability that it falls between two specific values is either 0 or 1.\nThe correct interpretation is that we are 95% confident that true mean waiting time at this ER is between 128 and 147 minutes.\n\n\n\nb.  \"95% of patients wait between 128 and 147 minutes at this ER.\"\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThis interpretation is incorrect. The confidence interval is about the population mean, not individual observations.\n\n\n\n\nA local newspaper claims that the average waiting time at this ER exceeds 3 hours. Is this claim supported by the confidence interval? Explain your reasoning.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nConfidence intervals give ranges of plausible values for a parameter based on the data we have in hand; thus, the claim is not supported by the confidence interval. The entire interval (128, 147) is below 180 minutes (3 hours), so we do not have evidence that the mean waiting time exceeds 3 hours.\n\n\n\n\nWould a 90% confidence interval be wider or narrower than the 95% confidence interval? Why?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nA 90% confidence interval would be narrower than a 95% confidence interval. To see this you can find the new critical value, \\(t^*\\). Alternatively, you can think intuitively about what reducing the confidence level means: for a large number of confidence intervals constructed using the same process, only 90% would contain the true parameter value rather than 95%. This happens if the intervals are narrower."
  },
  {
    "objectID": "activity/02H-sol-slr-inference.html#warm-up-questions",
    "href": "activity/02H-sol-slr-inference.html#warm-up-questions",
    "title": "Inference for SLR ‚Äì Stat 230",
    "section": "",
    "text": "Suppose that you wish to determine whether there is a difference in means between two groups. To do this you run a two-sample t-test and find the test statistic to be \\(t = 2.5\\).\n\nWhat is the null hypothesis for this test?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(H_0: \\mu_1 = \\mu_2\\) (or \\(H_0: \\mu_1 - \\mu_2 = 0\\))\n\n\n\n\nWhat is the alternative hypothesis for this test?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(H_A: \\mu_1 \\neq \\mu_2\\) (two-sided)\n\n\n\n\nBelow is a plot of the appropriate t-distribution for this test. Sketch how you would calculate the p-value for this situation.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe p-value is equal to the area in the two tails beyond \\(2.5\\) and \\(-2.5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow did you calculate this p-value in your last statistics course?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAnswers will vary based on where you took intro stats. You may have used StatKey or a similar online app/calculator, you may have used R‚Äôs pt() command, or you may have used a t-distribution table (we won‚Äôt use tables in this class).\n\n\n\n\nIf the p-value you calculated was 0.02, what would you conclude about the null hypothesis?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nA p-value of 0.02 provides evidence against the null hypothesis. In other words, there is statistically discernible evidence of a difference in means between the two groups (\\(t=2.5\\), p-value = 0.02).\n\n\n\n\n\n\n\nWhat is the difference between standard deviation and standard error?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nStandard deviation is a summary statistic that we can calculate for any distribution that measures the spread of the distribution. Standard error is the standard deviation of the sampling distribution (i.e., the distribution of a sample statistic). It is a measure of how much variability we expect in a sample statistic (e.g., sample mean or slope) from sample to sample.\n\n\n\nA 95% confidence interval for the mean waiting time at an emergency room (ER) of (128 minutes, 147 minutes).\n\nConsider the following interpretation of this interval. For each, determine whether it is correct. If it is incorrect, explain why and provide a correct interpretation.\n\n‚ÄúThere is a 95% probability that the mean waiting time at this ER is between 128 and 147 minutes.‚Äù\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThis interpretation is incorrect. The population mean is a fixed but unknown value, so the probability that it falls between two specific values is either 0 or 1.\nThe correct interpretation is that we are 95% confident that true mean waiting time at this ER is between 128 and 147 minutes.\n\n\n\nb.  \"95% of patients wait between 128 and 147 minutes at this ER.\"\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThis interpretation is incorrect. The confidence interval is about the population mean, not individual observations.\n\n\n\n\nA local newspaper claims that the average waiting time at this ER exceeds 3 hours. Is this claim supported by the confidence interval? Explain your reasoning.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nConfidence intervals give ranges of plausible values for a parameter based on the data we have in hand; thus, the claim is not supported by the confidence interval. The entire interval (128, 147) is below 180 minutes (3 hours), so we do not have evidence that the mean waiting time exceeds 3 hours.\n\n\n\n\nWould a 90% confidence interval be wider or narrower than the 95% confidence interval? Why?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nA 90% confidence interval would be narrower than a 95% confidence interval. To see this you can find the new critical value, \\(t^*\\). Alternatively, you can think intuitively about what reducing the confidence level means: for a large number of confidence intervals constructed using the same process, only 90% would contain the true parameter value rather than 95%. This happens if the intervals are narrower."
  },
  {
    "objectID": "activity/02H-sol-slr-inference.html#your-turn1",
    "href": "activity/02H-sol-slr-inference.html#your-turn1",
    "title": "Inference for SLR ‚Äì Stat 230",
    "section": "Your turn1",
    "text": "Your turn1\nBiologists know that the leaves on plants tend to get smaller as temperatures rise. A sample of 252 leaves from the species Dodonaea viscosa subspecies angustissima (broadleaf hopbush) have been collected in a certain region of South Australia. Is there an association between leaf width and year? Below is the coefficient table from a simple linear regression model.\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n37.723\n8.575\n4.399\n&lt;0.001\n\n\nYear\n‚àí0.018\n0.004\n‚àí4.029\n&lt;0.001\n\n\n\n\n\n\n\nNote: The width is the average width, in mm, of leaves, taken at their widest points, that were collected in a given year.\n\nReport the fitted regression equation.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(\\hat{y} = 37.723 - 0.018x\\)\n\n\n\n\nInterpret the slope of the regression line in context of the data.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAs year increases by 1 (i.e., from one year to the next), we expect the average leaf width of this species to decrease by 0.018 mm.\n\n\n\n\nCalculate a 95% confidence interval for the slope of the regression line. Interpret this interval in context of the data. To find the critical value, you may use StatKey with df = 250. Alternatively, you may use the rounded critical value of 2.\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFirst, we need to find the critical value for the confidence interval. Using StatKey with df = 250, we find that \\(t^* \\approx 1.97\\).\nThe 95% confidence interval for the slope is given by:\n\\[\n-0.018 \\pm 1.97(0.004) = (-0.02588, -0.01012)\n\\]\n\n\n\n\nWhat hypotheses are being tested with the statistic and associated p-value in the (Intercept) row of the table? What conclusion can you draw?\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe summary output provided by R always conducts a two-tailed hypothesis test of whether a regression coefficient is equal to 0. In other words it tests\n\\(H_0: \\beta_0=0\\) vs.¬†\\(H_a: \\beta_0 \\ne 0\\)\nBased on a test statistic of \\(t=4.399\\), df = 250, and a p-value &lt; 0.001, we find very strong evidence that the intercept is not zero. (This is outside the range of the observed data, so I won‚Äôt interpret it in context.)"
  },
  {
    "objectID": "activity/02H-sol-slr-inference.html#footnotes",
    "href": "activity/02H-sol-slr-inference.html#footnotes",
    "title": "Inference for SLR ‚Äì Stat 230",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdapted from Stat 2: Modeling with Regression and ANOVA‚Ü©Ô∏é"
  },
  {
    "objectID": "activity/04-slr-prediction.html",
    "href": "activity/04-slr-prediction.html",
    "title": "Inference for Prediction in SLR",
    "section": "",
    "text": "For this activity you will consider predicting the price of a used car (it‚Äôs Kelly Blue Book value) based on its mileage. The data set is loaded by running the following code chunk:\n\n\n\n\n\n\n\n\nThe columns of interest are Price and Mileage."
  },
  {
    "objectID": "activity/04-slr-prediction.html#loading-data",
    "href": "activity/04-slr-prediction.html#loading-data",
    "title": "Inference for Prediction in SLR",
    "section": "",
    "text": "For this activity you will consider predicting the price of a used car (it‚Äôs Kelly Blue Book value) based on its mileage. The data set is loaded by running the following code chunk:\n\n\n\n\n\n\n\n\nThe columns of interest are Price and Mileage."
  },
  {
    "objectID": "activity/04-slr-prediction.html#fitting-a-simple-linear-regression-model",
    "href": "activity/04-slr-prediction.html#fitting-a-simple-linear-regression-model",
    "title": "Inference for Prediction in SLR",
    "section": "Fitting a simple linear regression model",
    "text": "Fitting a simple linear regression model\nUse the lm() command to fit the simple linear regression model where Mileage is used to predict Price.\n\n\n\n\n\n\n\n\nQ1. Report the fitted regression equation."
  },
  {
    "objectID": "activity/04-slr-prediction.html#making-predictions",
    "href": "activity/04-slr-prediction.html#making-predictions",
    "title": "Inference for Prediction in SLR",
    "section": "Making predictions",
    "text": "Making predictions\nThe first car in the data set is a Buick Century with 8221 miles.\nQ2. Calculate the expected price of this car using the fitted regression equation. (You may use R or do this by hand. There‚Äôs a code chunk below for your convenience)\n\n\n\n\n\n\n\n\nQ3. If we want to predict the price of this car, should we use a confidence interval or a prediction interval?\nQ4. Use R to construct the appropriate 89% interval for the price of this car.\n\n\n\n\n\n\n\n\nQ5. Interpret the interval in context."
  },
  {
    "objectID": "activity/04-slr-prediction.html#plotting-our-intervals",
    "href": "activity/04-slr-prediction.html#plotting-our-intervals",
    "title": "Inference for Prediction in SLR",
    "section": "Plotting our intervals",
    "text": "Plotting our intervals\nIt can be useful to plot your confidence/prediction intervals to communicate your findings to a broader audience. The code below creates a scatterplot of the data, adds the fitted regression line, and then adds 89% prediction intervals.\n\n\n\n\n\n\n\n\nQ6. Which is the prediction invterval and which is the confidence interval? How can you tell?"
  },
  {
    "objectID": "activity/04-slr-prediction.html#function-quick-reference",
    "href": "activity/04-slr-prediction.html#function-quick-reference",
    "title": "Inference for Prediction in SLR",
    "section": "Function quick reference",
    "text": "Function quick reference\nThe following table summarizes the functions we learned today:\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nlm(formula, data)\nFit a linear model\n\n\npredict(model, newdata, interval, level, se.fit)\nMake predictions and\n\n\ngf_point(formula, data) |&gt; gf_lm(interval = \"confidence\")\nPlot data and add confidence intervals for the mean response\n\n\ngf_point(formula, data) |&gt; gf_lm(interval = \"prediction\")\nPlot data and add prediction intervals for the mean response"
  },
  {
    "objectID": "activity/07-mlr-polynomial.html",
    "href": "activity/07-mlr-polynomial.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "To load the wildfires data set, run the following code chunk:"
  },
  {
    "objectID": "activity/07-mlr-polynomial.html#loading-data",
    "href": "activity/07-mlr-polynomial.html#loading-data",
    "title": "Polynomial Regression",
    "section": "",
    "text": "To load the wildfires data set, run the following code chunk:"
  },
  {
    "objectID": "activity/07-mlr-polynomial.html#fitting-a-polynomial-regression-model",
    "href": "activity/07-mlr-polynomial.html#fitting-a-polynomial-regression-model",
    "title": "Polynomial Regression",
    "section": "Fitting a polynomial regression model",
    "text": "Fitting a polynomial regression model\nTo fit a polynomial regression model we still use the lm() command, but we expand our formula to include polynomial terms. To include polynomial terms in a regression model, we need to use the I() function to indicate that we want to calculate a polynomial term. For example, to fit the quadratic model we have already discussed in class, we use the following code:\n\n\n\n\n\n\n\n\nOnce you have your fitted model, we can explore it like we have with simple linear regression models."
  },
  {
    "objectID": "activity/07-mlr-polynomial.html#exploring-a-cubic-model",
    "href": "activity/07-mlr-polynomial.html#exploring-a-cubic-model",
    "title": "Polynomial Regression",
    "section": "Exploring a cubic model",
    "text": "Exploring a cubic model\nLet‚Äôs fit a cubic model to the wildfires data set. The cubic model has the form \\[\\mu \\lbrace y | x \\rbrace = \\beta_0 + \\beta_1 x + \\beta_2x^2 + \\beta_3x^3.\\] Use the lm() command to fit the cubic model where Year is used to predict Acres.\n\n\n\n\n\n\n\n\nTo plot the fitted cubic model, you can use the gf_point() and gf_lm() functions from the ggformula package. The following code will create a scatter plot of the data and add the fitted cubic regression line:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the gf_lm() layer we use the poly(x, 3) function to specify that we want to fit a cubic polynomial. You can use poly() to fit polynomials of any degree by changing the second argument, and you can also use this function within the lm() function to fit polynomial regression models if you‚Äôd like.\n\n\nDoes the cubic model appear to be necessary? Use the summary() function to explore the fitted model adn run a hypothesis test for the cubic term. What do you conclude?\nWhat degrees of freedom did R for the t-distribution used to calculate the p-value for the test of the cubic term?\nDo you notice anything curious about the inferential results for the linear and quadratic terms?\nThe issue here is that the polynomial terms for year are highly correlated with each other (i.e., year, year\\(^2\\), and year\\(^3\\) are correlated). This can lead to numerical instability and make it difficult to interpret the coefficients. This is a situation called multicollinearity. We‚Äôll talk more about this later. One way to remedy this issue in polynomial regression is to use orthogonal polynomials, which are uncorrelated with each other."
  },
  {
    "objectID": "activity/07-mlr-polynomial.html#an-alternative-way-to-fit-polynomials",
    "href": "activity/07-mlr-polynomial.html#an-alternative-way-to-fit-polynomials",
    "title": "Polynomial Regression",
    "section": "An alternative way to fit polynomials",
    "text": "An alternative way to fit polynomials\nTo fit polynomial model with uncorrelated polynomial terms use the poly() function. For example, to fit a cubic model using orthogonal polynomials, we can run the following code:\n\n\n\n\n\n\n\n\nUse the summary() function to explore the fitted model. What do you notice about the inferential results for the linear, quadratic, and cubic terms? How does this compare to the previous cubic model we fit? How does it compare to the quadratic model?\n\n\n\n\n\n\n\n\n\nThe method of constructing the polynomial terms in our regression model does not change our predictions, but it can change the inferential results for the polynomial terms."
  },
  {
    "objectID": "activity/07-mlr-polynomial.html#function-quick-reference",
    "href": "activity/07-mlr-polynomial.html#function-quick-reference",
    "title": "Polynomial Regression",
    "section": "Function quick reference",
    "text": "Function quick reference\nThe following table summarizes the functions we learned today:\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\nlm(formula, data)\nFit a linear model. For polynomial regression the formula should include polynomial terms or use poly().\n\n\nI()\nUsed to create polynomial terms in a regression model\n\n\npoly(x, degree)\nCreate orthogonal polynomial terms"
  },
  {
    "objectID": "activity/11H-mlr-multicollinearity.html",
    "href": "activity/11H-mlr-multicollinearity.html",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "",
    "text": "In this activity you will learn how to calculate diagnostic measures and create diagnostic plots for multiple linear regression models in R."
  },
  {
    "objectID": "activity/11H-mlr-multicollinearity.html#overview",
    "href": "activity/11H-mlr-multicollinearity.html#overview",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "",
    "text": "In this activity you will learn how to calculate diagnostic measures and create diagnostic plots for multiple linear regression models in R."
  },
  {
    "objectID": "activity/11H-mlr-multicollinearity.html#example-1-uncorrelated-predictors",
    "href": "activity/11H-mlr-multicollinearity.html#example-1-uncorrelated-predictors",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "Example 1: Uncorrelated Predictors",
    "text": "Example 1: Uncorrelated Predictors\nAs a first example, let‚Äôs consider a multiple linear regression model where the two predictor variables are uncorrelated.\nTask 1. Inspect the below correlation matrix and verify that the correlation between the two predictor variables (x1 and x2) is 0.\n\n\n          x1        x2         y\nx1 1.0000000 0.0000000 0.7419309\nx2 0.0000000 1.0000000 0.6384057\ny  0.7419309 0.6384057 1.0000000\n\n\nTask 2. The below code chunk fits a simple linear regression model with y as the response variable and x1 as the predictor variables. The coefficients table was extract using the broom::tidy(). (Feel free to use broom::tidy() it in your work!)\n\nmod1 &lt;- lm(y ~ x1, data = ex1)\nbroom::tidy(mod1)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    23.5      10.1       2.32  0.0591\n2 x1              5.37      1.98      2.71  0.0351\n\n\nSimilarly, here is the output for the simple linear regression model with y as the response variable and x2 as the predictor variable.\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    27.3      11.6       2.35  0.0572\n2 x2              9.25      4.55      2.03  0.0885\n\n\nAnd finally, here is the output for the multiple linear regression model with y as the response variable and both x1 and x2 as the predictor variables.\n\n\n# A tibble: 3 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.375     4.74     0.0791 0.940   \n2 x1             5.37      0.664    8.10   0.000466\n3 x2             9.25      1.33     6.97   0.000937\n\n\nCompare the model summaries from the three models. What do you observe about the coefficients when both predictors are included in the model? What do you observe about the standard errors? The results of the hypothesis tests?"
  },
  {
    "objectID": "activity/11H-mlr-multicollinearity.html#example-2-prefectly-correlated-predictors",
    "href": "activity/11H-mlr-multicollinearity.html#example-2-prefectly-correlated-predictors",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "Example 2: Prefectly Correlated Predictors",
    "text": "Example 2: Prefectly Correlated Predictors\nIn many examples the predictor variables will be correlated. As an extreme example, let‚Äôs consider a situation where the predictors are perfectly correlated. The data set for this small example is printed below.\n\n\n  x1 x2   y\n1  2  6  23\n2  8  9  83\n3  6  8  63\n4 10 10 103\n\n\nTask 3. Inspect the below correlation matrix and verify that the correlation between the two predictor variables (x1 and x2) is 1.\n\n\n   x1 x2 y\nx1  1  1 1\nx2  1  1 1\ny   1  1 1\n\n\nTask 4. From the previous task, you should have seen that the correlation between x1 and x2 is 1, as is the correlation between both predictors and y. This sounds like a great situation, right? We‚Äôll explore two models to unpack this.\n\nConsider the fitted regression equation: \\(\\widehat{y} = -87 + x_1 + 18x_2\\). Verify that the predicted values are exactly equal to the observed values.\nConsider the fitted regression equation: \\(\\widehat{y} = -7 + 9x_1 + 2x_2\\). Verify that the predicted values are exactly equal to the observed values.\nDiscuss the following questions with your group:\n\nWould you be willing to use either model to make predictions for new observations? Why or why not?\nWould you be willing to interpret the coefficients in either model? Why or why not?\nDo you think the standard error of the slopes is reasonably small or large? Why?"
  },
  {
    "objectID": "activity/11H-mlr-multicollinearity.html#example-3-a-more-realistic-example",
    "href": "activity/11H-mlr-multicollinearity.html#example-3-a-more-realistic-example",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "Example 3: A more realistic example",
    "text": "Example 3: A more realistic example\nLet‚Äôs consider a more realistic example where the predictor variables are correlated, but not perfectly correlated. The data set bodyfat contains measurements on 20 healthy adults between 25 and 34 years of age. We‚Äôre interested in predicting body fat percentage using three body measurements: triceps skinfold thickness, thigh circumference, and midarm circumference.\nTask 5. Below is the correlation matrix for the data set. What do you observe about the correlations between the predictor variables?\n\n\n                     triceps_skinfold thigh_circumference midarm_circumference\ntriceps_skinfold            1.0000000           0.9238425            0.4577772\nthigh_circumference         0.9238425           1.0000000            0.0846675\nmidarm_circumference        0.4577772           0.0846675            1.0000000\nbody_fat                    0.8432654           0.8780896            0.1424440\n                      body_fat\ntriceps_skinfold     0.8432654\nthigh_circumference  0.8780896\nmidarm_circumference 0.1424440\nbody_fat             1.0000000\n\n\nTask 6. There are numerous models that we could consider. Below are summary tables from several models. Compare the model summaries from the fitted models. What do you observe about the coefficients when different predictors are included in the model? What do you observe about the standard errors? The results of the hypothesis tests?\nOnly triceps skinfold as a predictor:\n\n\n# A tibble: 2 √ó 5\n  term             estimate std.error statistic    p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)        -1.50      3.32     -0.451 0.658     \n2 triceps_skinfold    0.857     0.129     6.66  0.00000302\n\n\nOnly thigh circumference as a predictor:\n\n\n# A tibble: 2 √ó 5\n  term                estimate std.error statistic     p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)          -23.6       5.66      -4.18 0.000566   \n2 thigh_circumference    0.857     0.110      7.79 0.000000360\n\n\nOnly midarm circumference as a predictor:\n\n\n# A tibble: 2 √ó 5\n  term                 estimate std.error statistic p.value\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)            14.7       9.10      1.61    0.124\n2 midarm_circumference    0.199     0.327     0.611   0.549\n\n\nTriceps skinfold and thigh circumference as predictors:\n\n\n# A tibble: 3 √ó 5\n  term                estimate std.error statistic p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)          -19.2       8.36     -2.29   0.0348\n2 triceps_skinfold       0.222     0.303     0.733  0.474 \n3 thigh_circumference    0.659     0.291     2.26   0.0369\n\n\nTriceps skinfold and midarm circumference as predictors:\n\n\n# A tibble: 3 √ó 5\n  term                 estimate std.error statistic     p.value\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)          -26.0        7.00     -3.72  0.00172    \n2 midarm_circumference   0.0960     0.161     0.595 0.560      \n3 thigh_circumference    0.851      0.112     7.57  0.000000772\n\n\nThigh circumference and midarm circumference as predictors:\n\n\n# A tibble: 4 √ó 5\n  term                 estimate std.error statistic p.value\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)            117.       99.8       1.17   0.258\n2 triceps_skinfold         4.33      3.02      1.44   0.170\n3 thigh_circumference     -2.86      2.58     -1.11   0.285\n4 midarm_circumference    -2.19      1.60     -1.37   0.190\n\n\n Let me know when you have reached this spot. We‚Äôll regroup as a class to discuss your observations."
  },
  {
    "objectID": "activity/11H-mlr-multicollinearity.html#what-about-predictions",
    "href": "activity/11H-mlr-multicollinearity.html#what-about-predictions",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "What about predictions?",
    "text": "What about predictions?\nNow that we‚Äôve discussed the issues revolving around interpreting coefficients and drawing inference on them in the presence of multicollinearity, let‚Äôs consider how multicollinearity impacts predictions.\nRecall that the mean squared error (MSE) of a model measures the variability in the error terms (residuals). In other words, it is measuring the variability unexplained by the model.\nTask 7. Below are the MSE values for three of the models we considered in Task 6. What do you observe about the MSE values when different predictors are included in the model?\n\n\n\nPredictors\nMSE\n\n\n\n\ntriceps_skinfold\n7.95\n\n\ntriceps_skinfold, thigh_circumference\n6.47\n\n\ntriceps_skinfold, thigh_circumference, midarm_circumference\n6.15\n\n\n\nTask 8. Based on your observations, do you think we can use any of the models to make predictions for new observations? Why or why not?\n Let me know when you have reached this spot. We‚Äôll regroup as a class to discuss your observations."
  },
  {
    "objectID": "activity/11H-mlr-multicollinearity.html#be-wary-of-r2-when-comparing-models",
    "href": "activity/11H-mlr-multicollinearity.html#be-wary-of-r2-when-comparing-models",
    "title": "Regression Cautions: Multicollinearity and R2",
    "section": "Be Wary of R2 when comparing models",
    "text": "Be Wary of R2 when comparing models\nWe all love R2 right? It is ‚Äúeasy‚Äù to interpret and is a popular metric of model fit to report in many fields. While I agree that R2 is a nice metric for describing a single model, it has an issue when it‚Äôs used to compare models. In this example, you‚Äôll explore this issue.\nCase Study 10.1.1 in The Statistical Sleuth describes Galileo‚Äôs experiment that led him to discover that the trajectory of a body falling with horizontal velocity is a parabola (see page 272). The case1011 data set in the Sleuth3 R package contains the results of Galileo‚Äôs experiment. The data set contains seven observations on the following two variables: the horizontal distance traveled from the edge of the table to its landing spot, and the initial height of the object. Both measurements are in punti (1 punti = or 169/180 mm or about 0.037 in).\nTask 9. Fit each of the following models and report the R2 value for each. Fill in the table below.\n\n\n\n\n\n\n\nModel\nR2\n\n\n\n\n\\(\\text{distance} \\sim \\text{height}\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3 + \\text{height}^4\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3 + \\text{height}^4 + \\text{height}^5\\)\n\n\n\n\\(\\text{distance} \\sim \\text{height} + \\text{height}^2 + \\text{height}^3 + \\text{height}^4 + \\text{height}^5+ \\text{height}^6\\)\n\n\n\n\nWhat do you observe about the R2 values as you add more polynomial terms to the model?"
  },
  {
    "objectID": "activity/06H-slr-transform.html",
    "href": "activity/06H-slr-transform.html",
    "title": "Transformations",
    "section": "",
    "text": "Researchers wish to understand the relationship between the brain weights (y, in grams) and body weights (x, in kilograms) of mammals. They have data on a sample of 30 species of mammals, which can be loaded using the code shown below:\n\n\n\n\n\n\n\n\n\nFit a simple linear regression model using body weight to predict brain weight.\n\n\n\n\n\n\n\n\n\n\nCreate a scatterplot of \\(y\\) versus \\(x\\) with a regression line, a plot of the residuals vs.¬†predicted (or ‚Äúfitted‚Äù) values (\\(\\widehat{y}\\)), and either a normal Q-Q plot or a histogram of the residuals. (Remember that you can use the resid_panel() command in the ggResidpanel R package for residual plots.) Summarize what issues you see with the regression conditions.\n\n\n\n\n\n\n\n\n\n\nTry various transformations of the explanatory and response variables to create a better linear regression model. (Hint: Notice that both the \\(x\\) and \\(y\\) variables are right skewed and have outliers, both may need a transformation.) What transformation(s) seem to remedy the issues you saw in part (b)?"
  },
  {
    "objectID": "activity/06H-slr-transform.html#example-mammal-brain-weights",
    "href": "activity/06H-slr-transform.html#example-mammal-brain-weights",
    "title": "Transformations",
    "section": "",
    "text": "Researchers wish to understand the relationship between the brain weights (y, in grams) and body weights (x, in kilograms) of mammals. They have data on a sample of 30 species of mammals, which can be loaded using the code shown below:\n\n\n\n\n\n\n\n\n\nFit a simple linear regression model using body weight to predict brain weight.\n\n\n\n\n\n\n\n\n\n\nCreate a scatterplot of \\(y\\) versus \\(x\\) with a regression line, a plot of the residuals vs.¬†predicted (or ‚Äúfitted‚Äù) values (\\(\\widehat{y}\\)), and either a normal Q-Q plot or a histogram of the residuals. (Remember that you can use the resid_panel() command in the ggResidpanel R package for residual plots.) Summarize what issues you see with the regression conditions.\n\n\n\n\n\n\n\n\n\n\nTry various transformations of the explanatory and response variables to create a better linear regression model. (Hint: Notice that both the \\(x\\) and \\(y\\) variables are right skewed and have outliers, both may need a transformation.) What transformation(s) seem to remedy the issues you saw in part (b)?"
  },
  {
    "objectID": "activity/06H-slr-transform.html#back-transforming-your-model",
    "href": "activity/06H-slr-transform.html#back-transforming-your-model",
    "title": "Transformations",
    "section": "Back-transforming your model",
    "text": "Back-transforming your model\nIn Example 1 you selected transformation(s) to help ‚Äúlinearize‚Äù the relationship between brain weights and body weights. While this is helpful from a statistical perspective, it‚Äôs often preferred to plot the fitted model on the original scale of the data. To do this, we need to back-transform the model using the following steps:\n\nStart by creating a scatterplot on the original scale.\nAdd a gf_lm() layer, specifying the transformations in the formula and if \\(y\\) is transformed how to back-transform \\(y\\) via the backtrans argument.\n\nFor example, if we log-transformed both x and y, then we pass log(y) ~ log(x) in as the formula to gf_lm() and backtrans = exp to back-transform y. Below is the full call where df is the data set with columns xvar and yvar.\n\n# change variable names and data set name here\ngf_point(yvar ~ xvar, data = df) |&gt; \n  gf_lm(formula = log(y) ~ log(x), backtrans = exp, interval = \"confidence\")\n\nUse this idea to plot the fitted model relating brain weights and body weights on the original scale."
  },
  {
    "objectID": "activity/04H-sol-slr-prediction.html",
    "href": "activity/04H-sol-slr-prediction.html",
    "title": "Inference for Prediction in SLR",
    "section": "",
    "text": "Is education level associated with income? Researchers collected education level (in years) and income (in thousands of dollars) for a random sample of 32 employees working for the city of Riverview.\nThe researchers fit a simple linear regression of income on education and obtained the following output:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n11.321\n6.123\n1.849\n0.074\n\n\neducation\n2.651\n0.370\n7.173\n&lt;0.001\n\n\n\n\n\nQ1. Write the fitted regression equation.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(\\widehat{y} = 11.3 + 2.65x\\)\n\n\n\nQ2. Interpret the slope coefficient in the context of the problem. (Don‚Äôt forget to specify units.)\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFor a one-year increase in education level we expect average income to increase by $2,650.\n\n\n\nQ3. Interpret the intercept in the context of the problem. (Don‚Äôt forget to specify units.)\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFor an employee with 0 years of education, we expect their average income to be $11,300. (Note: This interpretation may not make sense in context since it‚Äôs unlikely that an employee would have 0 years of education.)\n\n\n\nA new researcher joined the team and decided that education should be standardized (to have mean 0 and SD 1) before fitting the regression model. The output from this regression is shown below:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n53.742\n1.587\n33.861\n&lt;0.001\n\n\nscale(education)\n11.567\n1.613\n7.173\n&lt;0.001\n\n\n\n\n\nQ4. Write the fitted regression equation for this new model.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(\\widehat{y} = 53.7 + 11.6x\\)\n\n\n\nQ5. Interpret the slope coefficient in the context of the problem. (Don‚Äôt forget to specify units.)\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFor a one standard deviation increase in education level we expect average income to increase by $11,600.\n\n\n\nQ6. Interpret the intercept in the context of the problem. (Don‚Äôt forget to specify units.)\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAn employee with average education level (since the standardized value is 0) is expected to have an income of $53,700.\n\n\n\nQ7. Compare the two models. What‚Äôs the same? What‚Äôs different?\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe two models have different regression coefficients and standard errors, but they have the same overall association between the explanatory and predictor variable."
  },
  {
    "objectID": "activity/04H-sol-slr-prediction.html#warm-up-questions",
    "href": "activity/04H-sol-slr-prediction.html#warm-up-questions",
    "title": "Inference for Prediction in SLR",
    "section": "",
    "text": "Is education level associated with income? Researchers collected education level (in years) and income (in thousands of dollars) for a random sample of 32 employees working for the city of Riverview.\nThe researchers fit a simple linear regression of income on education and obtained the following output:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n11.321\n6.123\n1.849\n0.074\n\n\neducation\n2.651\n0.370\n7.173\n&lt;0.001\n\n\n\n\n\nQ1. Write the fitted regression equation.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(\\widehat{y} = 11.3 + 2.65x\\)\n\n\n\nQ2. Interpret the slope coefficient in the context of the problem. (Don‚Äôt forget to specify units.)\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFor a one-year increase in education level we expect average income to increase by $2,650.\n\n\n\nQ3. Interpret the intercept in the context of the problem. (Don‚Äôt forget to specify units.)\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFor an employee with 0 years of education, we expect their average income to be $11,300. (Note: This interpretation may not make sense in context since it‚Äôs unlikely that an employee would have 0 years of education.)\n\n\n\nA new researcher joined the team and decided that education should be standardized (to have mean 0 and SD 1) before fitting the regression model. The output from this regression is shown below:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n53.742\n1.587\n33.861\n&lt;0.001\n\n\nscale(education)\n11.567\n1.613\n7.173\n&lt;0.001\n\n\n\n\n\nQ4. Write the fitted regression equation for this new model.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\\(\\widehat{y} = 53.7 + 11.6x\\)\n\n\n\nQ5. Interpret the slope coefficient in the context of the problem. (Don‚Äôt forget to specify units.)\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nFor a one standard deviation increase in education level we expect average income to increase by $11,600.\n\n\n\nQ6. Interpret the intercept in the context of the problem. (Don‚Äôt forget to specify units.)\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nAn employee with average education level (since the standardized value is 0) is expected to have an income of $53,700.\n\n\n\nQ7. Compare the two models. What‚Äôs the same? What‚Äôs different?\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe two models have different regression coefficients and standard errors, but they have the same overall association between the explanatory and predictor variable."
  },
  {
    "objectID": "activity/04H-sol-slr-prediction.html#prediction-and-confidence-intervals",
    "href": "activity/04H-sol-slr-prediction.html#prediction-and-confidence-intervals",
    "title": "Inference for Prediction in SLR",
    "section": "Prediction and confidence intervals",
    "text": "Prediction and confidence intervals\n\n\n\n\n\n\nTip\n\n\n\nThe R code for the following questions is found at \nhttps://aloy.github.io/stat230-materials/activity/04-slr-prediction.\nThe URL is also posted on Moodle.\n\n\nFor this activity you will consider predicting the price of a used car (it‚Äôs Kelly Blue Book value) based on its mileage. The columns of interest in the Cars data set are Price and Mileage.\nQ1. Use the lm() command to fit the simple linear regression model where Mileage is used to predict Price. Report the fitted regression equation.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\ncars &lt;- read.csv(\"https://aloy.github.io/stat230-materials/data/Cars.csv\")\ncar_mod &lt;- lm(Price ~ Mileage, data = cars)\ncar_mod\n\n\nCall:\nlm(formula = Price ~ Mileage, data = cars)\n\nCoefficients:\n(Intercept)      Mileage  \n 24764.5590      -0.1725  \n\n\n\\(\\widehat{y} = 24764.56 - 0.17x\\)\n\n\n\nQ2. The first car in the data set is a Buick Century with 8221 miles. Calculate the expected price of this car using the fitted regression equation.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nUsing the regression equation, we have: \\[\\widehat{y} = 24764.56 - 0.17(8221) = 23366.99\\] So, the expected price of this car is $23,366.99.\nIn R, we use predict() to find the expected price:\n\npredict(car_mod, newdata = data.frame(Mileage = 8221))\n\n       1 \n23346.27 \n\n\nThe slight difference here is due to rounding in the coefficients for the ‚Äúby hand‚Äù calculation.\n\n\n\nQ3. If we want to predict the price of this car, should we use a confidence interval or a prediction interval?\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe should use a prediction interval because we are predicting the price of an individual car, not the average price of cars with that mileage.\n\n\n\nQ4. Use R to construct the appropriate 89% interval for the price of this car. Record this interval below.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\npredict(car_mod, newdata = data.frame(Mileage = 8221), \n        interval = \"prediction\", level = 0.89)\n\n       fit      lwr      upr\n1 23346.27 7654.458 39038.08\n\n\n\n\n\nQ5. Interpret the interval in context.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nWe are 89% confident that the price of a Buick Century with 8221 miles is between $7,654.46 and $39,038.08.\n\n\n\nQ6. Run the code to produce a scatterplot, regression line, and both types of intervals. Which is the prediction interval and which is the confidence interval? How can you tell?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\nThe wider interval is the prediction interval, and the narrower interval is the confidence interval. This is because prediction intervals account for both the uncertainty in estimating the mean response and the variability of individual responses around that mean, while confidence intervals only account for the uncertainty in estimating the mean response."
  }
]